# TODO:

- [ ] segmentation from motion，从而建立约束得到loss

  通过相同刚体的T矩阵理论上也是相同的，来把动态物体分割出来

- [ ] 对segmentation的uncertainty进行建模

# BA结合神经网络

## BA-NET

- [论文](https://arxiv.org/abs/1806.04807)
- [git](https://github.com/frobelbest/BANet)

### 摘要

利用feature-metric(结合了特征点和度量信息的)BA创建的一种网络结构来处理SFM问题，

它们用feature-metric error的方式来实现multi-view geometry多视图几何约束。

### 主要工作

![image-20230708165631026](https://raw.githubusercontent.com/Fernweh-yang/ImageHosting/main/img/202307081656176.png)

<center style="color:#C125C0C0">BA-NET整体结构</center>

- 基本流程：
  1. 首先，DRN-54提取feature maps
  2. 然后，同时两件事：
     - Basis Depth Maps Generator对第一张图$I_1$生成多个基准深度图basis depth maps
     - Feature Pyramid Constructor对1中的feature maps提取出特征F 
  3. 最后，BA层用下面的1式同时优化相机位姿，深度

1. **特征金字塔结构**：

   ![image-20230708170036977](https://raw.githubusercontent.com/Fernweh-yang/ImageHosting/main/img/202307081700007.png)

   - 将骨干网络backbone network(蓝色)DRN-54(Dilated Residual Network-54)的最后几个残差快标记为C1,C2,C3,C4
     - 残差块（residual block）是深度残差网络（Residual Network）的基本组成单元。它包含一系列卷积层、激活函数和跳跃连接（skip connection）。
     - 跳跃连接将输入特征与残差块的输出特征进行直接相加，以便在信息传递过程中保留更多的低层细节和梯度信号。
     - 这些残差块通常是网络中的最后几个阶段，用于提取更高级别的特征表示，以便进行更复杂的任务，如语义理解或图像分割。
     - TIPS:本文用的DRN-54因为效率问题用ordinary convolution替代了dilation convolution。
   - 用Bilinear interpolation（双线性插值）对特征图$C^{k+1}$进行上采样，并和$C^k$结合后，用3X3的卷积层来将低维度到128维，最终得到图片$I_i$的特征$F_i$

2. **基准深度图**

  - Encoder: 用DRN-54来提取有用的特征
  - Decoder: 用卷积特征图作为基准深度图来优化

3. **BA层**：

   ![image-20230708183312273](https://raw.githubusercontent.com/Fernweh-yang/ImageHosting/main/img/202307081833302.png)

   - BA优化的error：
     $$
     e^f_{i,j}(\mathcal{X})=F_i(\pi(\mathbf{T}_i,d_j\cdot\mathbf{q}_j))-F_1(\mathbf{q_j})\tag{1}
     $$
     - $F_i$：是照片$I_i$的的特征金字塔
       - 特征金字塔：通过构建多尺度的特征表示，以捕捉图像中不同尺度的物体信息
     - $\mathcal{X}$：参数
       - $T_i$：i图下的相机位姿
       - $d_j$：j观测点(特征点)的深度
       - $q_j$：j观测点(特征点)的坐标
     
   - Figure4:

     - $E(\mathcal{X})=[e_{1,1}^f(\mathcal{X}),e_{1,2}^f(\mathcal{X})\cdots e_{N_i,N_j}^f(\mathcal{X})]$

       计算所有的$N_i$照片和$N_j$像素点的误差

     - 用global average pooling全局平均池化来聚合每个特征通道上所有像素的E(X)绝对值。

       然后用MLP网络预测damping factor$\lambda$。

     - 最后更新$\mathcal{X}$
       $$
       \begin{align}
       \Delta\mathcal{X}&=(J(\mathcal{X})^TJ(\mathcal{X})+\lambda D(\mathcal{X}))^{-1}J(\mathcal{X})^{T}E(\mathcal{X})
       \\
       \Delta\mathcal{X}&=g(\mathcal{X};\mathbb{F})
       \\
       \mathcal{X}_k&=g(\mathcal{X}_{k-1};\mathbb{F})\circ \mathcal{X}_{k-1}
       \end{align}
       $$

### 总结

网络模型 BA-NET利用特征度量误差(feature-metric error)来强制执行多视角几何约束。然后由feature-metric BA同时优化深度和位姿。

整个BA-Net的流程是可微分的，所以模型可以从数据中学习特征表示F，并且通过训练过程中的反向传播来更新模型参数$\lambda$

## DeepSFM

- [论文](https://arxiv.org/abs/1912.09697)
- [git](https://github.com/weixk2015/DeepSFM)

### 摘要

提出了一个名为DeepSFM的物理驱动架构，将深度学习和**显式的结构约束**(3D cost volume)相结合，来解决三维重建问题。

DeepSFM由两个基于成本体素的架构组成，分别用于深度估计和姿态估计，通过迭代运行来改进两者。

### 主要工作

![image-20230710000609230](https://raw.githubusercontent.com/Fernweh-yang/ImageHosting/main/img/202307100006281.png)

- 基本流程
  1. 使用2D CNN对输入图像进行处理，提取图像中的**光度特征**(photometric feature)
     - 光度特征可以是灰度图像中的像素值，也可以是彩色图像中的颜色通道值。
  2. 利用光度特征构建**成本体素**cost volume。
     - 成本体素是一种表示源图像和目标图像之间匹配程度的数据结构。
     - 它通过比较光度特征的相似性来计算匹配代价
  3. 初始源深度图(source depth maps)和相机位姿被用来引入**光度和几何一致性**(consistency)
     - 这些一致性约束有助于提高深度估计和姿态估计的准确性。
  4. 应用一系列的3D CNN层来处理**深度成本体素**（D-CV）和**位姿成本体素**（P-CV）。
     - 这些层用于进一步提取和学习特征，以优化深度和姿态的估计。
  5. 对于深度图，使用**上下文网络**(context network)和**深度回归操作**(depth regression operation)来生成目标图像的预测深度图。
     - 上下文网络可以考虑图像中的上下文信息
     - 深度回归操作用于预测每个像素点的深度值。

- **深度成本体素**(D-CV)

  区别于以前的成本体素，D-CV进一步利用由深度图带来的局部几何一致性约束(local geometric consistency constraints)。

  假设采样Hypothesis Sampling：

  - 为了将源视点的特征和深度图从源视角(source viewpoint)反投影(back-project)到目标视点的三维空间中，在逆深度空间(inverse-depth space)中均匀采样了一组L个**虚拟深度图平面**($d_l$)，
  - 这些平面与目标视点的前向方向（z轴）垂直。
  - 这些平面被用作输出深度图的假设，并且可以在它们之上构建成本体素。

  D-CV由三部分组成：

  - 目标图像特征target image features

  - 变换的源图特征warped source image features

    - 利用内参K和位姿T，将原图特征 $F$ 变换(warp)到每一个假设的深度图平面(hypothetical depth map planes $d_l$)
    - 再结合目标图像特征，得到一个$2Channel\times L\times Width \times Height$的特征量feature volume
      - 特征F的size: $Channel\times Width \times Height$
      - L：虚拟平面个数

  - 齐次深度一致性图homogeneous depth consistency maps

    为了利用几何一致性并提升深度预测的质量，我们在每个虚拟平面上增加了另外两个通道：

    1. 源视角的变换初始深度图(warped initial depth maps from the source views)

       和上面的变换的源图特征相同

    2. 以源视角为参考的投影虚拟深度平面(the projected virtual depth plane from the perspective of the source view)

       需要从目标视角到源视角进行坐标转换

- **位姿成本体素**(P-CV)

  ![image-20230710021541970](https://raw.githubusercontent.com/Fernweh-yang/ImageHosting/main/img/202307100215004.png)

  <center style="color:#C125C0C0">假设位姿采样</center>

  区别于D-CV是在假设深度图平面上创建，P-CV是在假设位姿上建立的，假设位姿采样如上图所示。a是对平移采样，b是对旋转采样。采样后就得到一组虚拟的位姿。

  P-CV也由三部分组成：

  - 目标图像特征
  - 变换的源图像特征
    - 通过双线性插值(bilinear sampling)对源视角的特征进行变换，得到变换后的源特征图
    - 结合目标图像特征和深度图，得到一个$(2Channel+2)\times P\times Width \times Height$的4D cost volume
      - W,H：特征图的宽和高
      - channel：通道数
      - P：采样的位姿个数
  - 齐次深度一致性图
    - 将初始目标视角深度(initial target view depth)和源视角(source view depth)深度转换为一个齐次坐标系
      - 这样可以在成本体素构建过程中增强 相机姿态 和 多视角深度图 之间的几何一致性


### 总结

DeepSFM深度学习框架通过两个关键组件D-CV和P-CV实现在深度网络中显式地强制执行光度一致性、几何一致性和相机运动约束。

可以看作是一种增强的基于学习的BA算法，它充分发挥了可学习的先验知识和几何规则的优势。



## Salient BA for VSLAM

- [论文](https://arxiv.org/pdf/2012.11863.pdf)

### 摘要

旨在模仿人类视觉系统在不同任务下从自然场景中选择最显著和感兴趣的区域或点进行进一步处理。

用显著性预测模型(saliency prediction model)预测显著性地图(saliency map)，这个地图可以捕捉场景的语义的几何信息(scene semantic and geometric information)，然后这个地图的值作为传统BA中特征点的权重。

### 主要工作

![image-20230711005442714](https://raw.githubusercontent.com/Fernweh-yang/ImageHosting/main/img/202307110054860.png)

<center style="color:#C125C0C0">红色为本文贡献</center>

- 主要贡献：

  1. 提出了一个适用于室内和室外环境的 **Salient SLAM框架**，可以应用于各种应用领域。

     ![image-20230711010108808](https://raw.githubusercontent.com/Fernweh-yang/ImageHosting/main/img/202307110112364.png)

     <center style="color:#C125C0C0">Pipeline of visual saliency map prediction</center>

  2. 提出了一种新方法，基于KITTI生成**显著性数据集**Salient-KITTI(网上没找到)

     - 这个salient dataset包含几何和语义信息，用语义凝视(semantic gaze)来代替人眼凝视真值(human gaze ground-truth)
     - 之所以用这个数据集来训练，是因为之前依靠人眼追踪的gaze data无法捕捉到所有重要的信息。

  3. 用Salient-KITTI来做**显著性预测**

     1. 首先，对每个图像提取几何信息，如特征点、线条和平面
     2. 然后，使用SDC-Net在感兴趣物体周围生成分割掩码(segmented mask)
        - 作者选了13个类别（红绿灯、交通标志、道路、建筑物、人行道、停车场、铁轨、栅栏、桥梁、电线杆、电线杆组、植被、地形）来筛选几何信息
        - 动态区域中的特征不会出现在我们的显著性数据集中，如移动车辆、行人等
     3. 最后，基于我们提出的显著性数据集，使用DI-Net 获得一个显著性模型，并将其用于预测初始的显著性地图

  4. 提出了一种显著性**Salient Bundle Adjustment（SBA）**方法，以模拟人类视觉系统

     参考上面的总体流程图，SBA有两种：

     1. Motion-only BA: 用于优化位姿(R,t)，并最小化重映射误差
        $$
        \{R,t\}=\mathop{arg\ min}_{R,t}\sum_{i\in\mathcal{X}}\rho\big(w_i||x^i_{(\cdot)}-\pi_{(\cdot)}(\mathbf{RX})^i+t||^2_{\Sigma}\big)
        $$

        - $\rho$：robust Huber cost function

        - $w_i$:显著性权重salient weight
          $$
          w_i=aS^2(x_i,y_i)+b
          $$

          - S是显著性图Saliency map的像素值

     2. Local BA：用于优化共可见关键帧co-visible keyframe $\mathcal{K}_L$和这些关键帧中所有的地图点map-points $\mathcal{P}_L$
        $$
        \begin{align}
        &\{\mathbf{X}^i,\mathbf{R}_l,\mathbf{t}_l|i\in\mathcal{P_L},l\in\mathcal{K}_L \}=\mathop{arg\ min}_{\mathbf{X}^i,\mathbf{R}_l,\mathbf{t}_l}\sum_{k\in\mathcal{K}_L\cup\mathcal{K}_F}\sum_{j\in\mathcal{X}_k}\rho(E_{k,j})\\
        &E_{k,j}=w_i||x^j_{(\cdot)}-\pi_{(\cdot)}(\mathbf{R}_k\mathbf{X}^j+t_k)||^2_{\Sigma}
        \end{align}
        $$

        - X：地图点map-points $\mathcal{P}_L$和关键帧$\mathcal{K}_L$中key points相匹配的集合



### 总结

基于显著性信息，可以提高准确性和稳健性

## Semantic Scene Labeling-improve BA

- [论文](https://link.springer.com/chapter/10.1007/978-3-319-49409-8_13)

### 摘要

基于 CNN 的场景标记来几何约束捆绑调整，达到使用深度学习来改进城市规模的 SLAM的目的。

### 主要工作

主要思想是充分利用3d building建筑 models

用L-M优化的目标函数：
$$
\mathop{arg\ min}_\mathbf{X}\ \frac{1}{t-B(\mathbf{X})}+\sum_{q\in Q}W_qd(q,N_q) \tag{1}
$$

- $\mathbf{X}$:相机位姿和3d点的坐标

- Q：是被标记为建筑的3d点

  - 由CNN网络确定

- $d(\cdot)$：表示squared Euclidian distance

- $N_q$：离$q\in Q$最近的建筑平面

- $W_q=\mathcal{D}(P_z)$：权重
  $$
  P_Z=\frac{1}{N_c}\sum_{i=1}^nP_i\tag{2}
  $$

  - 3d点Z可有由n个观测值

  - $\mathcal{D}$:Dirichlet density function

    用于过滤较差的分割结果

  - $P_i$：3d点Z的一个可能分布probability distributions

  - $P_Z$：是所有可能分布的均值

- B：the sum of squared reprojection errors

### 总结

被证明比单纯的arg max更有效。但计算复杂度高，很难应用于实时SLAM

# 语义BA无DL

## Semantic Photometric BA

- [论文](https://arxiv.org/pdf/1712.00110.pdf)

### 摘要

语义光度BA，用深度学习获得3D object prior结合PBA，实现在自然图像序列(natural sequence of images)中实现优秀的对象重建效果

- 自然图像序列(natural sequence of images)：指在现实世界中以连续的时间间隔拍摄或捕获的图像序列
- 3D对象先验（3D object prior）：指对三维对象的先验知识或假设。根据以往的经验、统计数据或领域知识来建模对象的一般性质和特征。

### 主要工作

![image-20230712094913344](https://raw.githubusercontent.com/Fernweh-yang/ImageHosting/main/img/202307120949602.png)

<center style="color:#C125C0C0">Pipeline of the optimization</center>

- 基本流程

  物体的分类假设已知

  1. 用offline PBA pipeline对自然图像序列处理后，提供粗糙的深度图估计。并初始化相机位姿和style vector
  2. 优化目标：光度一致性photometric consistency$\mathcal{L}_{ph}$，轮廓匹配误差silhouette matching error$\mathcal{L}_{cd}$，和逆深度误差inverse depth error$\mathcal{L}_{invd}$

- 优化目标

  - 光度一致性photometric consistency$\mathcal{L}_{ph}$
    $$
    \begin{align}
    &\mathcal{L}_{ph}(\mathbf{p}_0,\{\Delta\mathbf{p}_l\}^{L-1}_{l=1},s)=\\
    &\sum_{l=1}^{L-1}\Big[\sum_{j=1}^{M_{\mathbf{p}_0}}\mathcal{L}_{\delta_1}(\mathcal{I}_0(\pi(\mathbf{x}_j;\mathbf{p}_0))-\mathcal{I}_l(\pi(\mathbf{x}_j;\mathbf{p}_0\circ\Delta\mathbf{p}_l)))\\
    &+\sum_{k=1}^{M_{\mathbf{p}_l}}\mathcal{L}_{\delta_1}(\mathcal{I}_l(\pi(\mathbf{x}_k;\mathbf{p}_0\circ\Delta\mathbf{p}_l))-\mathcal{I}_0(\pi(\mathbf{x}_k;\mathbf{p}_0))\Big]
    \end{align} \tag{1}
    $$

    - $\mathcal{L}_\delta(\cdot)$: Huber loss
    - $\mathbf{p}_0$: the global camera pose of the target frame
    - $\Delta\mathbf{p}_l$:the relative camera pose between each source frame and the corresponding target frame
    - $\mathcal{I}_l(\pi(\mathbf{x}_j;\mathbf{p}_l))$: 像素点x在图像l上的重映射像素坐标
    - 共有L张图片，第一张作为target frame,剩下的L-1张是source frames
    - $M_P$: 对某点x做mask function后返回的$M_p$个visible points

  - 轮廓匹配误差silhouette matching error$\mathcal{L}_{cd}$
    $$
    \begin{align}
    &\mathcal{L}_{cd}(\mathbf{p}_0,\{\Delta\mathbf{p}_l\}^{L-1}_{l=1},s)=\\
    &\frac{1}{L}\sum^{L-1}_{l=0}(\sum_{\mathbf{u}_k\in U_{l\ 1}}\mathop{min}_{\mathbf{u}_j\in U_{l2}}||\mathbf{u}_k-\mathbf{u}_j||_2^2\\
    &+\sum_{\mathbf{u}_j\in U_{l\ 2}}\mathop{min}_{\mathbf{u}_k\in U_{l1}}||\mathbf{u}_j-\mathbf{u}_k||_2^2)
    \end{align} \tag{2}
    $$
    
  - 逆深度误差inverse depth error$\mathcal{L}_{invd}$
    $$
    \mathcal{L}_{invd}(\mathbf{p}_0,\{\Delta\mathbf{p}_l\}^{L-1}_{l=1},s)=\frac{1}{L}\sum_{l=0}^{L-1}\mathcal{L}_{\delta_2}(\mathbf{d}_l'-\alpha\mathbf{d}_l)\tag{3}
    $$
  
- 初始化

  style $\mathbf{s}$和pose $\mathbf{p}_0$初始化：

  - style：用cheap silhouette轮廓来得到style vector
  - pose：先用blender生成templates定一个相机位姿，然后找一个可以最大化IoU的模板

  Camera Motion参数初始化：
  $$
  \begin{cases}
  \Delta \mathcal{R}_0(w_0)\mathbf{x}+\mathbf{t}_0=\alpha(\mathbf{R'_0x+t'_0}) \\
  \Delta\mathcal{R}_l(\Delta w_l) \mathcal{R}_0(w_0)\mathbf{x}+\Delta\mathbf{t}_l+\mathbf{t}_0=\alpha(\mathbf{R'_l\mathbf{x}+\mathbf{t'_l}})
  \end{cases} \tag{4}
  $$
  
- 优化

  ![image-20230712105234987](https://raw.githubusercontent.com/Fernweh-yang/ImageHosting/main/img/202307121052041.png)

  - equ13：上面4式
  - equ8：上面3式的$\alpha$
  - [openMVS](https://github.com/cdcseacave/openMVS):open Multi-View Stereo reconstruction library

### 总结

SPBA对位姿和3D形状(由learned semantic shape prior而得)施加几何约束。

在世界坐标系中可以生成dense full 3D shape和深度图



## Object-Aware BA

- [论文](https://ora.ox.ac.uk/objects/uuid:580d2b5d-f8a0-44bb-bb8b-cd7518b97b16/download_file?safe_filename=icra.pdf.pdf&file_format=application%2Fpdf&type_of_work=Conference+item)

### 摘要

作者提出了一种单目方法，在点测量point measurements之外还进行对象检测object detection，**来消除尺度模糊性scale ambiguity和飘逸drift。**

通过对目标size的先验prior，可以将尺度估计scale estimation集成到ba中去。

本方法没有过去前端方法中constant camera height or planar roadways的限制，因此实用性更广泛

### 主要工作

1. 观察和世界模型

   ![image-20230712115729011](https://raw.githubusercontent.com/Fernweh-yang/ImageHosting/main/img/202307121157081.png)

   <center style="color:#C125C0C0">左：Parametrizations of points, objects and their respective projections. 右：Object detections and their labels.</center>

   - $Q_{kw}=[\mathbf{X},R]^T$：世界坐标系w下的物体k的位姿
     - $\mathbf{X}=[X,Y,Z,1]^T$
   - $\hat{q}_{ki}=[u,v,w,h]$：物体k在相机i坐标系下预测的映射
     - w,h是宽和高
   - $P_{kw}$：map point，看作是volume less的物体
   - $\hat{p}_{ki}=[u,v,0,0]$：因为无体积所以0，0

2. 物体测量和数据关联

   任何能够检测detect和关联associate输入序列input sequence的方法都可以。

   用来提供相机速度和尺度scale信息

3.  Object BA
   $$
   \mathop{arg\ min}_{\{\mathcal{T,P,Q}\}}(\sum_{i\in\mathcal{T}}\sum_{j\in\mathcal{P}}\mathbf{r}_{ij}^TW_{ij}^{-1}\mathbf{r}_{ij}+\sum_{i\in\mathcal{T}}\sum_{k\in\mathcal{Q}}\mathbf{r}_{ik}^TV_{ik}^{-1}\mathbf{r}_{ik}) \tag{1}
   $$

   - $\mathcal{T,P,Q}$: 位姿，路标点，物体
   - $r_{ij}=(\mathbf{p}_{ij}-\hat{\mathbf{p}}(T_i,\mathbf{P_{jw}}))$：路标和它的测量值之间的误差
     - 假设正态分布中心为0，协方差为$W$
   - $r_{ik}=(\mathbf{q}_{ik}-\hat{\mathbf{q}}(T_i,\mathbf{Q}_{kw}))$：物体和物体测量值之间的误差
     - 假设正态分布中心为0，协方差为$V$

4. 尺寸修正

   应用一个先验分布prior distribution，来确保地图的尺度和物体实际尺寸的一致。此时，物体的尺寸作为超参被使用，仅优化物体在三维空间中的位置。

   另外可以把路标点看作size为0的物体，这样1式可以化简为：
   $$
   \{\mathcal{Q,T}\}=\mathop{arg\ min}_{\{\mathcal{T,Q}\}}\sum_{i\in\mathcal{T}}\sum_{k\in\mathcal{Q}}\mathbf{r}_{ik}^TV_{ik}^{-1}\mathbf{r}_{ik} \tag{2}
   $$

5. 追踪和局部BA

   如果上面的全局BA计算量太大，所以考虑用10个关键帧算一个local BA：
   $$
   \{\mathcal{Q}_{local},\mathcal{T}_{local},T_{cam}\}=\mathop{arg\ min}_{\{\mathcal{Q}_{local},\mathcal{T}_{local},T_{cam}\}}\sum_{i}\sum_{k}\mathbf{r}_{ik}^TV_{ik}^{-1}\mathbf{r}_{ik} \tag{3}
   $$

   - $i\in{T_{cam},\mathcal{T}_{fixed},\mathcal{T}_{local}}$
     - $T_{cam}$：现在相机位姿
     - $\mathcal{T}_{local}$：最近的n=10个关键帧
     - $\mathcal{T}_{fixed}$：所有可以看到$\mathcal{Q}_{local}$的关键帧
   - $k\in\mathcal{Q}_{local}$
     - $\mathcal{Q}_{local}$：在这些关键帧可以看到的物体和点

6. 检测异常值

   使用robust error function on residuals来排除异常值，2式变成：
   $$
   \{\mathcal{Q,T}\}=\mathop{arg\ min}_{\{\mathcal{T,Q}\}}\sum_{i\in\mathcal{T}}\sum_{k\in\mathcal{Q}}Obj(|\mathbf{r}_{ik}|/V_{ik},\sigma_T) \tag{4}
   $$

   - $Obj(\cdot,\sigma_T)$：是Tukey biweight objective function
   - $\sigma_T$：在这里被设置为点误差分布(distribution of point errors)的估计标准偏差(estimated standard deviation)

### 总结

本方法把目标类object class的先验尺度信息整合到了单目slam中，却不增加计算复杂性。

本方法只假设存在一些已知的物体并且具有相对统一的尺寸，不需要其他假设。

系统能够在整个轨迹中保持较低的尺度漂移，并在长时间没有检测到物体的情况下纠正尺度漂移





# 语义分割

## PanopticDepth

- [论文](chrome-extension://efaidnbmnnnibpcajpcglclefindmkaj/https://arxiv.org/pdf/2206.00468.pdf)
- [git](https://github.com/NaiyuGao/PanopticDepth)

### 摘要

本文提出了一种基于深度感知的全景分割（DPS，Depth-aware panoptic segmentation）的统一框架，旨在从一幅图像中重建具有实例级语义的三维场景。该框架将动态卷积技术应用于全景分割（PS）和深度预测任务中，以生成特定于实例的内核来预测每个实例的深度和分割掩码。此外，利用实例级深度估计方案，添加了额外的实例级深度线索，以通过新的深度损失来帮助监督深度学习。

> 全景分割（Panoptic Segmentation）是一种计算机视觉任务，旨在对图像中的每个像素进行语义分割，并将其分为具有语义类别标签的物体实例和无语义类别标签的背景区域。它的目标是同时获得像素级的语义信息和实例级的分割结果，将语义分割和实例分割相结合。

### 主要工作

- 主要流程

![image-20230715022419932](https://raw.githubusercontent.com/Fernweh-yang/ImageHosting/main/img/202307150224063.png)

<center style="color:#C125C0C0">图1，解决方案图示：
    通过使用实例特定的卷积核和实例掩码生成方法，实现了对单个图像中每个像素的深度、语义类别和实例ID的预测。</center>

> 实例掩码（Instance Mask）是指对图像进行像素级别标记的一种技术。语义分割只会将所有的人标记为同一类别，而无法区分不同的人。实例掩码在此基础上，将每个人的像素分配不同的标记，从而区分不同的个体。

- 贡献：

  - 提出了一种特定于实例的动态卷积核技术将深度估计和全景分割方法统一起来，从而提高了这两种任务的性能。
  - 为了简化深度估计，受批量归一化的启发，提出将每个实例深度图表示为三元组，即归一化深度图、深度范围和深度偏移，将原始实例深度映射的值规范化为[0，1]，以提高了学习效率。
  - 基于新的深度图表示（如深度偏移）添加了实例级深度统计，以加强深度监控。为适应这种新的监督，提出了相应的深度损失，以改进深度预测。

- 方法：

  ![image-20230715031358502](https://raw.githubusercontent.com/Fernweh-yang/ImageHosting/main/img/202307150313554.png)

  <center style="color:#C125C0C0">图2，PanopticDepth框架：H和W是是输入图像的宽和高。N是实例数目。c是分类数。3个e分别是卷积核、深度核和深度嵌入的维度。圈叉是卷积的意思</center>

  > Depth Embedding深度嵌入：指将深度信息编码为向量表示的技术，通过将深度信息映射到低维向量空间，将连续的深度值转换为可用于计算的固定维度向量表示
  >
  > Mask embeddings掩码嵌入：掩码信息编码为向量表示的技术，通过将掩码信息映射到低维向量空间，将二进制的掩码转换为具有固定维度的向量表示。掩码是用于标记或表示图像中特定区域的二进制图像，其中每个像素指示该像素是否属于感兴趣的区域。
  >
  > Single stage feature单阶段特征：指的是在单阶段目标检测器或分割器中使用的特征表示。单阶段特征通常是从输入图像中经过一系列卷积和池化操作得到的特征图，具有较低的分辨率和较大的感受野。这些特征通常用于全局感知和目标检测/分割任务的粗略定位。
  >
  > High resolution feature高分辨率特征：指的是具有更高分辨率的特征表示，通常是在多阶段或级联模型中使用。高分辨率特征对于需要准确定位和细粒度信息的任务非常重要，如密集目标分割、边缘检测等。它们通常通过上采样或特定的卷积操作来获得

  1. Kernel Producer内核生成器

     - 用于生成instance classification，mask convolution kernels 和 depth estimation kernels

     - 内核生成器基于PanopticFCN（Panoptic Fully Convolutional Network）开发，PFCN是一种用于全景分割任务的神经网络模型。它是基于全卷积网络（Fully Convolutional Network）的架构，旨在同时进行像素级的语义分割和实例分割。
     - 内核生成器分为内核生成器和内核融合两个阶段
       - 在内核生成器阶段，将特征金字塔FPN中第i阶段的一个单阶段特征$X_i$作为输入，生成器生成一个内核权重映射，以及分别为对象和对象生成的两个位置映射，给定每个FPN阶段的位置图和核权重图
       - 在核融合阶段，合并多个FPN阶段的重复核权重，通过提出的自适应核融合（AKF，adaptive kernel fusion）操作实现的。

  2. Panoptic Segmentation全景分割

     即图2的最下面部分，thing和stuff掩码Mask M的计算公式为：
     $$
     M=Sigmoid(K^m\otimes E^m)\tag{1}
     $$

     > "Thing" 实例：在语义分割中，"thing" 实例通常指的是具有明确边界和独立形状的物体类别，如人、车、动物等。这些物体实例在图像中通常是有限的、可数的，并且可以通过边界框或掩码精确地定位和分割。
     >
     > "Stuff" 实例：相比之下，"stuff" 实例指的是没有明确边界或独立形状的类别，例如天空、道路、草地、水等。这些类别通常具有连续的分布和不规则的形状，难以通过简单的边界框或掩码进行精确分割。"Stuff" 实例通常出现在图像的背景或更广阔的环境中，它们与"thing" 实例形成了对比。

     首先丢弃冗余实例掩码。然后，将所有剩余的实例掩码与argmax合并，以生成不重叠的全景分割结果non-overlapped panoptic segmentation result，这样每个像素都被分配到一个thing or stuff segment.

  3. Instance-wise Depth Estimation基于实例的深度估计

     即图2中间部分，首先对深度嵌入施加深度核depth kernels来得到实例深度图instance depth maps。然后根据全景分割结果合并这些单独的图像以生成最终的整体深度图。

     - Depth Map Generator深度图生成器

       首先为了简化深度估计的学习过程，生成normalized instance depth map D‘:
       $$
       D'=Sigmoid(K^d \otimes E^d) \tag{2}
       $$
       然后将其转为unnormalized instance depth map D:
       $$
       \begin{align}
       &\mathcal{T}_1(D|D',d^\mathcal{T},d^s)=d_{max}\times(d^{r}\times D'+d^s)\tag{3}\\
       &\mathcal{T}_2(D|D',d^\mathcal{T},d^s)=d_{max}\times[d^{r}\times (D'-0.5)+d^s] \tag{4}
       \end{align}
       $$

       - $d^r\in R^{N\times1}$：深度范围depth range。指的是深度图像中表示的深度值的范围
       - $d^s\in R^{N\times1}$：深度偏移depth shift。深度偏移可以通过将深度图像中的每个像素值加上或减去一个常数值来实现
       - $d_{max}$：控制深度尺寸depth scale

       最后得到所有instance depth map后，根据不重叠的全景分割掩码nonoverlapped panoptic segmentation masks M，将它们聚合成一张完整的图像深度图，以生成精确的实例边界处的深度值。

  4. Depth Loss深度损失

     基于scale-invariant logarithmic error和relative squared error来设计深度损失函数：
     $$
     \tilde{L}_{dep}(d,\hat{d})=\frac{1}{n}\sum_j(logd_j-log\hat{d}_j)^2-\frac{1}{n^2}(\sum_jlogd_j-log\hat{d}_j)^2+[\frac{1}{n}\sum_j(\frac{d_j-\hat{d}_j}{\hat{d}_j})^2]^{0.5}\tag{5}
     $$

     - $d$：gt 深度，$\hat{d}$: predicted depth

     由于上面第三部分的实例深度估计，我们可以在传统的像素级别的监督上增加实例级别的监督，这样可以增加深度的准确度。最终的深度损失函数由2部分组成
     $$
     L_{dep}=L_{dep}^P+\lambda_{dep}^IL_{dep}^I\tag{6}
     $$

     - $L_{dep}^P=\tilde{L}_{dep}(D_{all},\hat{D}_{all})$

       像素级别的损失函数，计算了每个实例instance中每个像素的深度估计和深度真值之间的区别。

     - $L_{dep}^I=\tilde{L}_{dep}(d^s,\hat{d}^s)$

       实例级别的损失函数，计算了实例深度偏移instance depth shift$d^s$和它真值之间的区别。

     - $\lambda_{dep}^I$是权重，默认为1

### 总结

本文提出了一个统一的深度感知全景分割框架，生成特定于实例的内核来预测每个实例的深度和分割掩码。

采用动态核技术将高层目标信息引入深度估计，使用深度偏移和深度范围对每个实例深度图进行归一化，以简化共享深度嵌入的学习。

此外，本文还提出了一种新的深度损失方法来监督实例级深度线索的深度学习。在城市景观DPS和SemKITTI DPS基准上的实验证明了该方法的有效性。

## RAFT-3D

- [论文](https://arxiv.org/abs/2012.00726)
- [git](https://github.com/princeton-vl/RAFT-3D)

### 摘要

解决的问题：给定一组stereo或者rgbd视觉帧，估计每个像素的3D运动。

RAFT-3D基于RAFT模型开发，但在更新dense field时是更新像素级的SE3运动，而不是2D运动。

RAFT-3D的关键创新是一个rigid-motion刚体运动 embeddings：Dense-SE3，它会施加一个geometric consistency约束

> embeddings嵌入，常用于将高维数据转换为具有较低维度的向量表示，从而提取有用的特征或实现更高效的计算
>
> "rigid-motion embeddings"（刚体运动嵌入）是指将刚体运动信息编码为向量表示的技术。通过将像素分配到相应的嵌入向量中，可以表示它们属于同一刚体对象，并捕捉刚体运动的几何关系。

### 主要工作

### 总结

我们引入了 RAFT-3D，一种用于场景流的端到端网络。 RAFT-3D 使用刚性运动嵌入，将像素软分组到刚性移动的对象中。 我们证明这些嵌入可用于求解密集且准确的 3D 运动场。



## Joint Semantic and Motion Segmentation for dynamic scenes using CNN

- Joint Semantic and Motion Segmentation for dynamic scenes using Deep Convolutional Networks

### 摘要

本文提出了一种使用 CNN 融合语义特征和运动线索的方法，以解决单目语义运动分割问题。

通过将光流作为具有语义特征的约束集成到扩张卷积网络中来推导语义和运动标签。

主要有3部分：1. feature extraction 2. feature amplification 3. Multi Scale Context Aggregation

>光流（Optical Flow）是计算机视觉领域的一种技术，用于描述图像中像素在时间序列上的运动模式。它通过分析连续帧之间的像素亮度变化来估计每个像素点的运动速度和方向。

在视频序列中

### 主要工作

- 主要贡献：

  1. 提出了一种端到端的卷积神经网络架构，可以从单目图像中执行**运动和语义标签**的**联合学习joint learning**。

     > "end-to-end"（端到端）：是指整个系统或模型可以直接从原始输入（在本文中是单目图像）到最终的输出（运动和语义标签）进行端到端的学习，而不需要中间的手动特征工程或多个阶段的处理。

  2. 提供了一种新颖的方法，用于将运动线索motion cues与训练过的用来预测语义标签semantic labels的网络无缝集成

- 主题流程：

  ![image-20230719115039270](https://raw.githubusercontent.com/Fernweh-yang/ImageHosting/main/img/202307191150422.png)
  
  <center style="color:#C125C0C0">图1</center>
  
  - 神经网络(A)接收t和t+1处的图像
  
  - 扩张网络dilated network通过添加运动标签和最后的Conv进行微调。 提取特征（B）
  
  - 将两帧之间的光流Optical flow（C）缩放scaled并调整为特征图的大小（D）
  
    ！！！？通过什么方式normalize 和 resize
  
    > "Flow magnitude"（光流幅值）是计算机视觉中用于表示光流场的一个属性。。在光流场中，光流幅值表示了每个像素点的运动强度或运动速度的大小。较大的光流幅值表示像素点在图像序列中移动较远或运动速度较快，而较小的光流幅值表示像素点在图像序列中移动较少或运动速度较慢。
  
  - 使用光流幅值按元素乘积 (E) 放大扩张的特征。
  
  - convolution layers are freezed and fully connected layers are fine tuned.
  
  - **通过上下文模块Context Module的端到端训练，学习对象类和运动标签之间的依赖关系，进一步增强了增强的特征图。**
  
    > Context module（上下文模块）是指在计算机视觉和深度学习中，用于捕获图像上下文信息的一种模块或组件。上下文信息是指图像中某个像素点周围的像素值或区域的信息。它包含了与目标像素点相关的上下文内容，可以帮助理解该像素点所在的语义和结构。
  
  - 对从 softmax 层获得的预测进行上采样，为每个像素提供联合标签 (F)。

### 相关处

- **如何捕捉动作信息?**

  ![image-20230723000622694](https://raw.githubusercontent.com/Fernweh-yang/ImageHosting/main/img/%E5%85%89%E6%B5%81%E5%B9%85%E5%BA%A6%E5%80%BC%E4%B9%98%E5%8D%B7%E7%A7%AF%E7%89%B9%E5%BE%81.png)

  <center style="color:#C125C0C0">图2：光流幅度值逐元素相乘卷积特征</center>
  
  1. 用CNN网络得到卷积特征
  
  2. 计算光流，并将其归一化成值在1-2范围内的光流幅值(Flow magnitude)
  
  3. **将光流幅值(Flow magnitude)与卷积特征逐像素相乘，得到包含运动和语义信息的Amplified feature**
  
     !!!!为什么：逐像素相乘就可以得到包含运动和语义信息的特征。
  
  4. 最后Context module（上下文模块）将Amplified feature转换为语义分割图

### 总结

提出了一种使用单目相机预测语义和运动标签的联合方法。 

结合空间和时间信息来共同学习对象类别和运动标签。



## ！！！MoA-Net

- MoA-Net: Self-Supervised Motion Segmentation
- 参考：Geo-Net

### 摘要

最新的运动分割方法使用光流将图像分割成静态环境和独立移动的物体。 基于神经网络的方法通常需要大量标记的训练数据才能实现最先进的性能。

本文提出了一种以自我监督的方式训练运动分割网络的新方法。将运动分割问题分解为两个较小的子问题：

1. 修改流场以消除观察者的旋转和位移
2. 分割(segment)旋转 -补偿(compensated)流为静态环境和独立移动物体的流(flow)。

### 主要工作

![image-20230719120502103](https://raw.githubusercontent.com/Fernweh-yang/ImageHosting/main/img/202307191205144.png)

<center style="color:#C125C0C0">图1：Self-Supervised Motion Segmentation</center>

- 神经网络模型很难直接从光流中分割运动，所以分为2步：
  1. 调整相机旋转的光流
  2. 将补偿光流(compensated flow)的角度分割为静态环境和移动物体

### 相关处

**思考：**既然静态的场景和动态的物体可以分别被检测出来，那图像中的每个元素不就相当于判断了2次，它是不是属于这个动态的Object。

- **3种行为产生光流**(光流计算公式见论文)

  - 由相机旋转产生的光流$v_r$：和场景深度(scene depth)无关，所以要从光流中消除它
  - 由相机平移产生的光流$v_t$：要考虑
  - 由物体运动产生的光流$v_o$：要考虑

  最后使用的光流：$v=v_t+v_o$。

- **如何利用这些光流**

  - 光流由：场景深度(scene depth)，相机平移(camera translation)和物体运动(obejct motion)三者造成。所以依靠光流幅值(flow’s magnitude)无法分辨具体是什么动了。

  - 但光流方向flow direction很容易判断

    - 因为光流方向只由1.相机运动方向(observer’s motion direction) 和2.目标运动方( object’s motion direction)两者决定

    - **所以只要存在光流的产生不能完全由相机的平移运动解释，那么场景中就肯定有物体动了**

      作责用此信息来建立数据集，训练网络来做运动分割。

- **如何做Motion segmentation（运动分割）**

  - 调整相机旋转的光流，消除相机旋转带来的光流

  - 将调整后光流(compensated flow)的分割为静态环境和移动物体
    - 计算每个像素的光流方向$\theta$，得到angle image
    
    - 给MoA-Net (Motion Angle - Network)输入angle image，学习得到每个像素的运动标签
    
      *文章没有介绍MOA-NET具体结构*

- **如何生成自监督学习用的traing data**：

  ![image-20230723011630175](https://raw.githubusercontent.com/Fernweh-yang/ImageHosting/main/img/Moa-Net%E6%95%B0%E6%8D%AE%E9%9B%86%E7%94%9F%E6%88%90%E7%A4%BA%E6%84%8F%E5%9B%BE.png)

  <center style="color:#C125C0C0">图2：生成的数据集</center>

  1. Generating connected object regions：

     使用binary segmentations masks of FlyingThings3D，生成得到图2a

     >  "connected object regions" 指的是图像中的连通物体区域，也称为连通分量或连通域。连通物体区域是指图像中由相邻像素组成的一组像素，它们在图像中连接在一起，形成一个整体的物体。

  2. Modeling articulated object motion：

     像人这种多关节物体运动，可能有的部分在动，有的没在动。所以用superpixels将每一个object region划分成多个子区域。如图2b

     > Superpixels（超像素）是图像处理中的一种技术，用于将图像中的像素点分组成更大的、具有一定连续性和相似性的像素块。

   3. Assigning a translational 3D direction to each motion region:

      使用一个正二十面体（icosahedron）的顶点作为近似，在球体上生成一组平移运动方向。每个正二十面体的顶点代表一个平移运动方向。如图2c

    4. Smoothing motion boundaries:
  
       使用高斯滤波来平滑运动边界。如图2d
  
    5. Add random Gaussian noise
  

### 总结

我们将运动分割问题分解为两个较小的子任务：（1）补偿相机旋转的流；（2）将剩余的流角场分割为静态环境和移动对象。 这导致了运动物体的“抽象”定义，使我们能够以全自动方式合成训练数据，并可以使用 CNN 来完成运动分割任务，同时确保对场景的正确解释 几何学。

## GeoNet

- [GeoNet: Unsupervised Learning of Dense Depth, Optical Flow and Camera Pose](https://arxiv.org/abs/1803.02276)
- [git](https://github.com/yzcjtr/GeoNet)
- [git](https://github.com/yijie0710/GeoNet_pytorch)

### 摘要

提出了一个联合无监督学习框架，可以端到端的学习得到：单目深度、光流和自我运动估计

### 主要工作

![image-20230723015256829](https://raw.githubusercontent.com/Fernweh-yang/ImageHosting/main/img/GeoNet%20overview.png)

<center style="color:#C125C0C0">图1：GeO-Net的结构</center>

分3个阶段：

1. Rigid Structure Reconstructor

   重建刚性场景，针对静态场景

   - depthnet：取单一视图作为输入，并利用累积的场景先验进行深度预测
   - posenet：所有的视图作为输入，一口气返回所有的从target frame到resource frame的$T_{t\rightarrow s}$

2. Non-rigid Motion Localizer

   使用ResFlowNet定位非刚性目标，作为动态物体的补偿

3. Geometric Consistency Enforcement

   减轻occlusions and non-Lambertian surfaces的影响

   > 非兰伯特表面（non-Lambertian surfaces）用于描述不遵循兰伯特定律的表面.
   >
   > 兰伯特定律（Lambert's law），光线在物体表面上的反射是均匀分布的，即入射光线与法线的夹角越大，反射光强度越小，与表面颜色无关。这种表面在所有方向上均匀地反射光线，是一种理想的漫反射表面。

### 相关处：

- 在Rigid Structure Reconstructor部分提供了刚性场景布局rigid scene layout的立体感知stereoscopic perception，忽略了动态物体。用到的Loss如下：

  - rigid warping loss loss：用于过滤错误的预测和保护sharp details。

    ![image-20230723020336646](https://raw.githubusercontent.com/Fernweh-yang/ImageHosting/main/img/geonet_photometric_loss.png)

    - $SSIM$：表示结构相似指数structural similarity index
    - $I_t$：target frame，一系列帧中的reference view
    - $I_s$：source frame，一系列帧中target frame之外的所有frame
    - $\widetilde{I}_s^{rig}$：inverse warped image from$I_s$ to to target image plane
    - $\alpha$：0.85

  - edge-aware depth smoothness loss
    $$
    \mathcal{L}_{ds}=\sum_{pt}|\nabla D(p_t)|\cdot(e^{-|\nabla I(p_t)|})^T \tag{2}
    $$

    - $D$：深度图
    - $p_t$：像素在帧$I_m$中的齐次坐标
    - $|\cdot|$：逐元素相乘
    - $\nabla$：vector differential operator
    - T：transpose

- 在Geometric Consistency Enforcement方面

  - 施加geometric consistency enforcement几何一致性强化
    $$
    \mathcal{L}_{gc}=\sum_{p_t}[\delta(p_t)]\cdot||\Delta f_{t\rightarrow s}^{full}(p_t) ||_1 \tag{3}
    $$
    参考：需要参考这些loss

- 最后的损失函数是3部分之和：
  $$
  \mathcal{L}=\sum_l\sum_{<t,s>}\{\mathcal{L}_{rw}+\lambda_{ds}\mathcal{L}_{ds}+\mathcal{L}_{fw}+\lambda_{fs}\mathcal{L}_{fs}+\lambda_{gc}\mathcal{L}_{gc}\} \tag{4}
  $$

  - fw,fs是Non-rigid Motion Localizer部分的1，2式
  - $\lambda$：各个loss的权重



## Motion Detection and Segmentation Using Optical Flow

### 摘要

本文讨论了使用光流算法对视频中的运动进行检测、分析和分割。

讨论光流背后的理论，然后描述我们的方法和在 MATLAB 中的实现。

**本文主要介绍optical的原理，无特别需要参考**

为什么：如何多检测，分割

## Motion Segmentation a Review

### 摘要

本文研究了运动分割问题，分析和回顾了最重要和最新的技术

### 主要工作

最重要的运动分割类别：

- **图像差异**是检测变化的最简单且最常用的技术之一

  - 缺点：它对噪声非常敏感
  - 缺点：当相机移动时，整个图像正在发生变化，如果帧速率不够高，结果将无法提供任何有用的信息。

- **统计理论**：运动分割可以看作是一个分类问题，其中每个像素都必须被分类为背景或前景

  常见框架有：

  - 最大后验概率 (MAP)
  - 粒子滤波器 (PF) ，不看
  - 期望最大化 (EM)和softmax有关联

- **基于小波wavelets**

  可以结合光流

  - 这些小波充当匹配滤波器，并对速度、方向、尺度和时空位置执行最小均方误差估计

- **光流**

- **基于层layer**的技术

  - 这种方法通常用于立体视觉，因为它更容易计算深度距离。 然而，无需计算深度，就可以估计哪些物体在相似平面上移动。

- **分解技术**

  使用通过图像序列跟踪的特征来恢复结构和运动

### 总结

## Motion-based Object Segmentation based on Dense RGB-D Scene Flow

- [论文和git](https://paperswithcode.com/paper/motion-based-object-segmentation-based-on)

### 摘要

- 给定两个连续的 RGB-D 图像，我们提出了一个模型来估计密集的 3D 运动场，也称为场景流。 
- 利用了这样一个事实：在机器人操作场景中，场景通常由一组刚性移动的物体组成。
- 我们的模型联合估计，将场景分割为：
  - 未知但有限数量的对象
  - 这些对象的运动轨迹
  - 这些对象场景流。

### 主要工作

![image-20230719143511658](https://raw.githubusercontent.com/Fernweh-yang/ImageHosting/main/img/202307191435699.png)

<center style="color:#C125C0C0">图1：A neural network which learns to estimate object segmentation and scene flow given a pair of RGB-D images. The data undergoes spatial compression, correlation, and refinement to propose object segmentations and transformations.</center>

![image-20230719143723956](https://raw.githubusercontent.com/Fernweh-yang/ImageHosting/main/img/202307191437997.png)

<center style="color:#C125C0C0">图2：Network architecture utilized in this paper. The RGB-D input is split into two components, RGB and XYZ, before being passed into Siamese neural networks. A correlation is performed on the output of the RGB Siamese network and applied to the XYZ features from time t − 1. After a max pooling layer, the newly combined features undergo upconvolutions. The output of the upconvolutions is fed into 3 different layers that predict the center of the object, translation, and rotation. Thereafter, the segmentation ID is determined using the center of the object and its predicted translation. For predicting scene flow, the translation, rotation, and input XYZ data is utilized. The final output is presented as a segmentation mask and scene flow predictions. Note that the blue, red, and green arrows do not have gradient flow.</center>

### 相关处：

- **如何判断一个像素是否属于object**：

  ![image-20230723034328439](https://raw.githubusercontent.com/Fernweh-yang/ImageHosting/main/img/Object%20segmentation%20procee.png)

  <center style="color:#C125C0C0">图3：Object segmentation process. Left: Points represent points in a point cloud. Stars represent ground-truth object centers. Same color indicates same object. Middle: Each square represents the trajectory features ˆ ξ in trajectory feature space each associated with a point on the left. The size of the squares represents the corresponding point’s probability ˆ η of being an object centroid. Right: The segmentation process cycles through the squares starting with those having the highest probability to be an object centroid. A sphere centered at one of those squares with radius ˆ B then segments trajectories and corresponding points.</center>

  - 让$\xi_k=[X_k,X_k+T_k]$代表物体$O_k$的object trajectory的起始和终点。

    - $X_k,X_k+T_k$：物体在t和t-1时刻的中心

    - 任何属于该物体的像素都应该有相同的$\xi_k$值

    - 在模型中会逐个像素预测一个$\hat{\xi}_{uv}=\xi_{uv}+\epsilon_{uv}$值

      - 在这个$\epsilon$范围内的都被认为是一个Object。

      - 在这个范围外的都被认为是背景

  - 模型还会学习一个球半径$B_{uv}$值

    - 它是一个球的半径，球心是$\hat{\xi}_{uv}$

    - 位于这个球内的，被认为是一个Object

    - 位于这个球外的，被认为是背景

    - 这个B值的gt计算公式：
      $$
      B_{uv}^{gt}=\frac{1}{2}\mathop{min}_{k\neq l}||\xi_k-\xi_l||_2
      $$

      - $\xi_l$：图像中所有其他物体的轨迹

  - **还会学习一个mask layer，用于把错认为object的像素转为背景。**

  - 还会学习一个$\eta$图

    每一个像素都一个概率$\eta_{uv}$表示这个像素是否是物体的质心。

- **Loss Function**:
  $$
  L=\lambda_mL_m+\lambda_{center}L_{center}+L_p+\lambda_{var}L_{var}+\lambda_{vio}L_{vio} 
  $$

  - $L_m$mask loss：cross entropy loss between the ground truth and estimated foreground/background segmentation.

  - $L_{center}$cluster center loss：learn the probability ηuv of a pixel (u, v) to be the object center。

  - $L_p$Pixel-wise Loss：基于真值和预测值使用L-2norm来最小化目标旋转、目标位移、场景流、球半径和轨迹。

  - **$L_{var}$Variance Loss：优化属于同一个物体$O_k$的像素，有更小的偏差**
    $$
    L_{var}=\sum_k\frac{1}{N_k}\sum_{(u,v)\in O_k}||\hat{\xi}_{uv}-\overline{\hat{\xi}}_{uv}||^2
    $$

    - $\overline{\hat{\xi}}_{uv}$：是所有$N_k$个属于物体$O_k$的轨迹$\hat{\xi}_{uv}$均值

  - **$L_{vio}$Violation Loss：惩罚那些没有被正确分类的像素点**
    $$
    L_{vio}=\sum_K\sum_{(u,v)\in O_k}\{||\hat{\xi}_{uv}-\xi_{uv}||_2>\frac{1}{5} B_{uv}\}||\hat{\xi}_{uv}-\xi_{uv}||_2
    $$
    

### 总结

提出了一种深度神经网络架构，给定两个连续的 RGB-D 图像，可以准确地估计对象场景流和基于运动的对象分割。

## ！！！Unsupervised Online Video Object Segmentation with Motion Property Understanding

- [论文和代码](https://paperswithcode.com/paper/unsupervised-online-video-object-segmentation)

### 摘要

我们提出了一种新颖的无监督在线视频对象分割（UOVOS）框架，

- 通过将运动属性解释为与分割区域的通用对象同时移动。
- 通过结合显着运动检测salient motion detection和目标提议object proposals，开发了逐像素融合策略，以有效去除动态背景和静止目标等检测噪声
- 通过利用从紧邻的前一帧获得的分割，采用前向传播算法来处理不可靠的运动检测和对象建议

> Unsupervised Learning无监督学习在没有标签的情况下从数据中学习结构，而Self-Supervised Learning自监督学习则是通过为数据创建虚拟的监督信号来进行学习。
>
> 无监督学习通常用于聚类、降维等任务，而自监督学习更多地关注学习有用的特征表示，以用于其他任务的迁移学习。

### 主要工作

- 总体流程：

  1. 对每一帧，使用salient motion detection来检测动的区域(salient motion mask$S^t$)

     只检测哪些地方动了，不检测这是物体(船)还是动的背景(浪)。
     $$
     \widetilde{S}_i^t(\mathbf{F}^t)=\sum_{F_j^t\in \mathbf{F}^t}d(F_i^t,F_j^t) \tag{1}
     $$

     - $\mathbf{F}^t={F_1^t\cdots F_N^t}$：optical flow field

       - $F_N^t=[u_N^t,v_N^t]$：在帧$I_t$中每个像素的光流

     - $\widetilde{S}_i^t \in [0,1]$：salient motion map on $\mathbf{F}^t$

     - $d(\cdot)$：distance metric

     $$
     S^t=\phi(\widetilde{S}_i^t) \tag{2}
     $$

     - $S^t$：salient motion mask
     - $\phi$：binary splitting function
  
  2. 对每一帧，使用object proposal method来检测generic objects（objectness mask$O^t$）
  
     从动的背景中donddddd物体检测出来。
  
     本文使用了Mask R-CNN model pretrained on MS-COCO dataset来得到这个mask.
  
  3. 使用fusion method将上面2个结果融合起来,得到segmentation mask$P^t$
     $$
     p^t=D(S^t,R)\cap O^t \tag{3}
     $$

     - D：image dilation function
     - r：dilated radius
  
  4. 为了使分割结果更准确
  
     - 使用了一个forward propagation refinement方法
     - 还额外用了一个CRF模型

### 相关处：

- **Forward Propagation Refinement**

  idea：将之前帧的segmentation mask也考虑进去，得到更准确的动作分割结果。

  原理：the video content in neighboring frames often share consistent motion dynamic：

  - refined salient motion mask$\overline{S}^t$
    $$
    \overline{S}^t = \theta \widetilde{S}^t+(1-\theta)\sum_{\tau=1}^n\overline{M}^{t-\tau} \tag{4}
    $$

    - $\overline{M}^{t-\tau}$：之前的segmentation masks
    - $\theta=[0,1]$：权重
    - $\widetilde{S}_i^t \in [0,1]$：salient motion map。即1式

  - refined objectness mask$\overline{O}^t$
    $$
    \overline{O}^t=\theta O^t+(1-\theta)\sum_{\tau=1}^n\overline{M}^{t-\tau} \tag{5}
    $$

  - refined segmentation mask$\overline{M}^{t}$
    $$
    M^t=D(\phi(\overline{S}^t),r)\cap \phi(\overline{O}^t) \tag{6}
    $$

- **CRF Refinement**

  > CRF Refinement（条件随机场优化）是一种在计算机视觉和图像处理中常用的图像后处理技术，用于改善图像分割或图像标注的结果。

  解决的问题：无法分辨object boundary非常准确

  idea：将动态物体的segmentation optimization构建成一个binary classification problem二元分类问题。

  - 构建图graph$\mathcal{G}$：
    $$
    \mathcal{G}=<\mathcal{V,E}> \tag{7}
    $$

    - $\mathcal{V}$：代表图片像素的顶点(vertices)集合
    - $\mathcal{E}$：代表图像中每个像素与其四个相邻像素之间的关系或连接关系(edge)。

  - 优化的目标函数：
    $$
    \mathcal{L}^t=arg\ \mathop{min}_{L^t}\ E(L^t) \tag{8}
    $$

    - $L^t \in\{0,1\}$：每个像素的标签

    - $\mathcal{L}^t=\{L_1^t\cdots L_N^t\}$：foreground/background label

    - $E$：energy function
      $$
      E(L^t)=\sum_{i\in\mathcal{V}}\mathcal{U}_i^t(L_i^t)+\lambda\sum_{(i,j)\in\mathcal{E}}\mathcal{W}_{ij}^t(L_i^t,L_j^t)\tag{9}
      $$

    - $\mathcal{U}_i^t(L_i^t)$：appearance based unary term.

      用于对 RGB 颜色空间中和最初估计的前景/背景外观的偏差进行建模。
      $$
      \mathcal{U}_i^t(L_i^t)=(1-L_i^t)\mathcal{C}_f^t+L_i^t\mathcal{C_b^t} \tag{10}
      $$

      - $\mathcal{C}_f^t$：total cost of assigning background to foreground
      - $\mathcal{C_b^t}$：total cost of assigning foreground to background

    - $\mathcal{W}_{ij}^t(L_i^t,L_j^t)$：pairwise term for spatial smoothness purpose.

      用于确保相邻像素分配有相同的标签
      $$
      \mathcal{W}_{ij}^t(L_i^t,L_j^t)=(L_i^t-L_j^t)^2exp(-\beta||I_i^t-I_j^t||^2) \tag{11}
      $$

      - $\beta>0$：常数
      - $I_i^t,I_j^t$：帧$I^t$中4 个相邻像素的强度值

### UOVOS算法框架

![image-20230723134638645](https://raw.githubusercontent.com/Fernweh-yang/ImageHosting/main/img/UOVOS%E7%AE%97%E6%B3%95%E6%A1%86%E6%9E%B6.png)

### 总结

- 提出了一个解决无监督在线 VOS 问题的新框架
- 受移动物体的两个关键属性（即“移动”和“通用”）的启发，我们建议应用显着运动检测和对象提议技术来解决这个具有挑战性的问题。
- 设计了像素级融合方法和前向传播细化策略来提高分割性能

## ！！！SMSnet

- SMSnet: Semantic Motion Segmentation using Deep Convolutional Neural Networks

### 摘要

- 解释物体的语义和运动是自主机器人能够在动态现实环境中推理和操作的先决条件。
- 在本文中，我们提出了一种新颖的卷积神经网络架构，可以学习预测图像中每个像素的对象标签和运动状态。 给定一对连续图像，网络学习融合自生成的光流图和语义分割内核的特征，以产生逐像素语义运动标签。 
- 还介绍了 Cityscapes-Motion 数据集，其中包含超过 2,900 个手动注释的语义运动标签，这是迄今为止同类数据集中最大的。
-  证明网络优于现有方法，在 KITTI 数据集以及更具挑战性的 Cityscapes-Motion 数据集上实现了最先进的性能，同时比现有技术快得多。

> Ego-motion（自我运动）是指相机或机器人相对于其周围环境的运动。具体来说，ego-motion表示的是相机或机器人在三维空间中的运动，通常包括平移运动和旋转运动.
>
> ego-motion估计是一项重要的任务。通过从图像序列中计算相邻帧之间的运动，可以实现视觉里程计，从而估计相机在场景中的运动轨迹。

### 主要工作：

这个网络的功能是通过最小化cross-entropy(softmax) loss来学习emantic motion features

这个loss可以由1式计算：
$$
\mathcal{L}(k)=-\frac{exp(a_k)}{\sum_{l=1}^C exp(a_l)} \tag{1}
$$
优化方法，用随机梯度下降最小化1式：
$$
\theta^*=\mathop{argmin}_\theta\sum_{i=1}^{N\times|X_n|}\mathcal{L}(f(x^i;\theta),y^i)\tag{2}
$$
网络整体结构：

![image-20230723135606416](https://raw.githubusercontent.com/Fernweh-yang/ImageHosting/main/img/SMSnet%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84.png)

<center style="color:#C125C0C0">图1：SMS-NET结构.</center>

### 相关处：

- **如何融合Motion feature和semantic feature**

  见图1橙色部分的网络

- **消除相机自己运动导致的光流**

  额外利用IMU 和 odometry的数据计算ego-motion flow：
  $$
  \hat{X}'=KRK^{-1}X+K\frac{T}{Z} \tag{3}
  $$

  - $X=(u,v,1)^T$：每个像素的齐次坐标
  - $T,R$：相机自身的位姿变换
  - Z：是像素的深度

  最后后SMS-NET算出的流$\hat{X}$减去相机产生的流$\hat{X}'$就是单纯的物体运动了

### 总结

- 提出了一种卷积神经网络，它将两个输入图像作为输入，并学习预测图像中每个像素的语义类标签和运动状态。

- 引入了两个具有真实注释的大型数据集，可以训练用于语义运动分割的深度神经网络

## ！！！Optical Flow with Semantic Segmentation and Localized Layers

- [论文](https://paperswithcode.com/paper/optical-flow-with-semantic-segmentation-and)

### 摘要

- 图像中的光流会根据对象类别而变化。 简而言之，不同的物体运动方式不同。
- 本文利用静态语义场景分割的最新进展将图像分割成不同类型的对象。然后根据物体的类型在这些区域定义不同的图像运动模型。
- 使用局部层的新颖公式提出流量估计问题，它解决了传统分层模型处理复杂场景运动的局限性。

### 主要工作

- 本文定义了三类物体object：

  - things：飞机、自行车、鸟、船、公共汽车、汽车、猫、牛、狗、马、摩托车、羊、火车和人
  - planes：像道路，天空和水这样的区域，具有广阔的空间范围，大致呈平面，并且通常位于背景中
  - stuff：对应于表现出纹理运动的类或像“建筑物”和“植被”这样的物体，它们可能具有复杂的 3D 形状，表现出复杂的视差，并且我们没有紧凑的运动表示。

  > "Thing" 实例：在语义分割中，"thing" 实例通常指的是具有明确边界和独立形状的物体类别，如人、车、动物等。这些物体实例在图像中通常是有限的、可数的，并且可以通过边界框或掩码精确地定位和分割。
  >
  > "Stuff" 实例：相比之下，"stuff" 实例指的是没有明确边界或独立形状的类别，例如天空、道路、草地、水等。这些类别通常具有连续的分布和不规则的形状，难以通过简单的边界框或掩码进行精确分割。"Stuff" 实例通常出现在图像的背景或更广阔的环境中，它们与"thing" 实例形成了对比。

- 使用了Layered optical flow（分层光流）

  > Layered optical flow 通过将图像分解成多个图层，每个图层代表一个运动物体或运动区域，从而可以针对不同运动的物体进行光流估计。每个图层都有自己的光流场，用于描述该图层中像素的运动。这样一来，Layered optical flow 能够更好地处理多个运动物体之间的相互遮挡、速度变化等情况，提供更精确的运动估计结果。

### 相关处：

- **构建的local layered energy term:**

  ![image-20230723142922766](https://raw.githubusercontent.com/Fernweh-yang/ImageHosting/main/img/local%20layered%20energy%20term.png)

  - $E_{data}$：data term，当相应像素在同一层可见时，数据项会施加外观恒定性，否则会施加恒定的惩罚。 它通过比较相应像素的层分配来推断遮挡。
  - $E_{motion}$：motion term，预设了2个假设：1. 如果相邻像素属于同一层，则它们应该具有相似的运动。2.每一层k的像素应共享一个全局运动模型
  - $E_{time}$：time term，随着时间推移，让相应的像素具有相同的图层标签
  - $E_{space}$：space term，保证层分割的空间连续性
  - $E_{layer}$：coupling term,保证前景层分割和语义分割之间的相似性

- **如何优化上面这项**

  使用[A fully-connected layered model of foreground and background flow](https://vcg.seas.harvard.edu/publications/a-fully-connected-layered-model-of-foreground-and-background-flow/paper)中的算法。

  区别于原算法使用heuristics来推断深度排序depth ordering，本文使用class category 来推断深度排序depth ordering并假设分类为things的物体一直是foreground的

### 总结

- 我们定义了一种使用语义分割来改进光流估计的方法。
-  我们的语义光流方法使用对象类标签来确定要应用于每个区域的适当运动模型。 我们将场景分为物体（独立移动）、平面（大的、大致平坦的区域）和物体（其他一切）。
-  我们专注于使用局部层模型来估计事物，其中我们仅在感兴趣对象周围的受限区域中应用分层光流。 
- 我们引入了一种新的约束来选择类似于我们的语义分割的分层分割。 一个关键的见解是，检测到的对象区域可能最多包含两个运动，并且该对象可能位于前面。

## ！！！DytanVO

- [DytanVO: Joint Refinement of Visual Odometry and Motion Segmentation in Dynamic Environments](https://paperswithcode.com/paper/dytanvo-joint-refinement-of-visual-odometry)
- [git](https://github.com/castacks/DytanVO)

### 摘要

- 在本文中，我们利用**相机自我运动**和**运动分割**之间的相互依赖，并表明两者可以在一个基于学习的框架中共同完善
- 我们提出了 DytanVO，这是第一个处理动态环境的基于监督学习的 Visual Odometry 方法。
  - 它实时获取两个连续的单目帧，并以迭代方式预测相机的自我运动

### 主要工作：

- 数据集：

  - 本文模型基于TartanVO构建，保留了它的泛化能力的同时还增加了更大量多样的数据来训练
  - 模型在TartanAir和SceneFlow2个数据集上进行训练

- 网络结构：

  ![image-20230723145222387](https://raw.githubusercontent.com/Fernweh-yang/ImageHosting/main/img/DytanVo%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84.png)

  <center style="color:#C125C0C0">图1：DytanVO结构.It consists of a matching network which estimates optical flow from two consecutive images, a pose network that estimates pose based on optical flow without dynamic movements, and a motion segmentation network that outputs a probability mask of the dynamicness. The matching network is forwarded only once while the pose network and the segmentation network are iterated to jointly refine pose estimate and motion segmentation. In the first iteration, we randomly initialize the segmentation mask. In each iteration, optical flow is set to zero inside masked regions.</center>

- 运动分割

  参照论文[Learning to segment rigid motions from two frames](https://arxiv.org/abs/2101.03694)来做这部分

  - 通过[optical expansion](https://openaccess.thecvf.com/content_CVPR_2020/papers/Yang_Upgrading_Optical_Flow_to_3D_Scene_Flow_Through_Optical_Expansion_CVPR_2020_paper.pdf)将2D光流拓展为3D光流。

    根据重叠图像块(overlapping image patches)的尺度变化(scale change)来估计相对深度

  - 将cost map明确构建为分割网络输入。

    成本图基于1. coplanar motion ambiguity共面运动歧义性 2. colinear motion ambiguity共线运动歧义性来构建。

    这些歧义性会导致geometry-based motion segmentation的分割错误

- 迭代优化相机运动预测

  ![image-20230723150417417](https://raw.githubusercontent.com/Fernweh-yang/ImageHosting/main/img/DytanVo%E8%BF%AD%E4%BB%A3%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95.png)
  
  - $S_t^{t+1}$：binary segmentation mask,
  - $F_t^{t+1}$：optical flow
  - $z_t^{t+1}$：a probability map of every pixel belonging to a dynamic object or not
  - $\delta_t^{t+1}=(R|T)$：relative camera motion
  - $\widetilde{F}_t^{t+1}$：optical flow with mask

- 监督学习网络

  给定真值R,T。基于camera motion loss $L_p$：
  $$
  L_p=||\frac{\hat{\mathbf{T}}}{max(||\hat{\mathbf{T}}||,\epsilon)}-\frac{\mathbf{T}}{max(||\mathbf{T}||,\epsilon)}||+||\hat{\mathbf{R}}-\mathbf{R}|| \tag{1}
  $$



### 相关处：

- **目标函数**

  除了向上面那样只训练相机pose network，还可以**端到端的训练**：

  此时的目标函数为：
  $$
  L=\lambda_1L_M+\lambda_2L_U+L_P \tag{2}
  $$
  

  - $L_M$: optical flow loss
    - L1 norm between the predicted flow and the ground truth flow

  - $L_P$：camera  motion loss

  - $L_U$：motion segmentation loss
    - **the binary cross entropy loss between predicted probability and the segmentation label**

- **相机位姿和Motion segmentation是如何联合优化的**

  见上面图1和算法1
