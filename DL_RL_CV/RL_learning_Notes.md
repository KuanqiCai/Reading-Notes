# é›¶ã€å­¦ä¹ èµ„æºæ•´ç†

1. [OpenAI Spinning Up](https://spinningup.openai.com/en/latest/index.html)
2. [æœ¬ç¬”è®°æ‰€å­¦ä¹ çš„è¯¾](https://github.com/huggingface/deep-rl-class)
3. [Reinforcement Learning: An Introduction](http://incompleteideas.net/book/RLbook2020.pdf)(å·²ä¿å­˜åœ¨onedrive)

# ä¸€ã€æ·±åº¦å¼ºåŒ–å­¦ä¹ 

ç½‘è¯¾ï¼šhttps://github.com/huggingface/deep-rl-class#the-hugging-face-deep-reinforcement-learning-class-

## 1. FoundationåŸºç¡€

- Formal Definition

  Reinforcement learning is a framework for solving control tasks (also called decision problems) by building agents that learn from the environment by interacting with it through trial and error and receiving rewards (positive or negative) as unique feedback

### 1.1 å¼ºåŒ–å­¦ä¹ ä¸­å¿ƒæ€æƒ³

å¼ºåŒ–å­¦ä¹ çš„ä¸­å¿ƒæ€æƒ³ï¼Œå°±æ˜¯**è®©æ™ºèƒ½ä½“åœ¨ç¯å¢ƒé‡Œå­¦ä¹ **ã€‚æ¯ä¸ªè¡ŒåŠ¨ä¼šå¯¹åº”å„è‡ªçš„å¥–åŠ±ï¼Œæ™ºèƒ½ä½“é€šè¿‡åˆ†ææ•°æ®æ¥å­¦ä¹ ï¼Œæ€æ ·çš„æƒ…å†µä¸‹åº”è¯¥åšæ€æ ·çš„äº‹æƒ…ã€‚

æ¯”å¦‚æ™ºèƒ½ä½“è¦å­¦ç€ç©è¶…çº§é©¬é‡Œå¥¥ï¼š

![](https://picd.zhimg.com/80/v2-e058a9d05a30d09edbbd0b70da685133_720w.webp?source=1940ef5c)

- æ™ºèƒ½ä½“åœ¨ç¯å¢ƒ (è¶…çº§é©¬é‡Œå¥¥) é‡Œè·å¾—åˆå§‹çŠ¶æ€**S0** (æ¸¸æˆçš„ç¬¬ä¸€å¸§) ï¼›

- åœ¨S0çš„åŸºç¡€ä¸Šï¼Œagentä¼šåšå‡ºç¬¬ä¸€ä¸ªè¡ŒåŠ¨**A0** (å¦‚å‘å³èµ°) ï¼›

- ç¯å¢ƒå˜åŒ–ï¼Œè·å¾—æ–°çš„çŠ¶æ€**S1** (A0å‘ç”Ÿåçš„æŸä¸€å¸§) ï¼›

- ç¯å¢ƒç»™å‡ºäº†ç¬¬ä¸€ä¸ªå¥–åŠ±**R1** (æ²¡æ­»ï¼š+1) ï¼›

  äºæ˜¯ï¼Œè¿™ä¸ª[loop](https://www.zhihu.com/search?q=loop&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A602826860})è¾“å‡ºçš„å°±æ˜¯ä¸€ä¸ª**ç”±çŠ¶æ€ã€å¥–åŠ±å’Œè¡ŒåŠ¨ç»„æˆçš„åºåˆ—**ã€‚

  è€Œæ™ºèƒ½ä½“çš„**ç›®æ ‡**å°±æ˜¯è®©**é¢„æœŸç´¯ç§¯å¥–åŠ±æœ€å¤§åŒ–**ã€‚





### 1.2 å¥–åŠ±å‡è¯´reward hypothesis

ç”±ä¸Šå¯çŸ¥ç›®æ ‡æ˜¯è¦è®©ç´¯ç§¯å¥–åŠ±æœ€å¤§åŒ–

- æ¯ä¸€ä¸ªæ—¶é—´æ­¥çš„ç´¯ç§¯å¥–åŠ±å¯ä»¥è¡¨ç¤ºä¸ºï¼š
  $$
  G_t = R_{t+1}+R_{t+2}+...
  $$
  ä½†ç°å®ä¸­**ä¸èƒ½æŠŠå¥–åŠ±ç›´æ¥ç›¸åŠ **ã€‚å› ä¸ºæ¸¸æˆé‡Œï¼Œè¶Šæ¥è¿‘æ¸¸æˆå¼€å§‹å¤„çš„å¥–åŠ±ï¼Œå°±è¶Šå®¹æ˜“è·å¾—ï¼›è€Œéšç€æ¸¸æˆçš„è¿›è¡Œï¼Œåé¢çš„å¥–åŠ±å°±æ²¡æœ‰é‚£ä¹ˆå®¹æ˜“æ‹¿åˆ°äº†ã€‚

- ç»™å¥–åŠ±æ·»åŠ æŠ˜æ‰£

  ç”¨$\gamma$è¡¨ç¤ºæŠ˜æ‰£ç‡ï¼Œ0-1ä¹‹é—´

  - $\gamma$è¶Šå¤§ï¼ŒæŠ˜æ‰£è¶Šå°ã€‚è¡¨ç¤ºæ™ºèƒ½ä½“è¶Šåœ¨æ„é•¿æœŸçš„å¥–åŠ±ã€‚
  - $\gamma$è¶Šå°ï¼ŒæŠ˜æ‰£è¶Šå¤§ã€‚è¡¨ç¤ºæ™ºèƒ½ä½“è¶Šåœ¨æ„çŸ­æœŸçš„å¥–åŠ±ã€‚

  $$
  G_t = R_{t+1} + \gamma R_{t+2}+\gamma^2 R_{t+3}+...
  $$

  ç®€å•æ¥è¯´ï¼Œ**ç¦»å›°éš¾çš„åœ°æ–¹è¿‘ä¸€æ­¥**ï¼Œ**å°±ä¹˜ä¸Šä¸€ä¸ªÎ³**ï¼Œè¡¨ç¤ºå¥–åŠ±è¶Šéš¾è·å¾—ã€‚





### 1.3 å¼ºåŒ–å­¦ä¹ çš„ä¸¤ç§ä»»åŠ¡

#### 1.3.1 ç‰‡æ®µæ€§ä»»åŠ¡Episodic Task

è¿™ç±»ä»»åŠ¡ï¼Œæœ‰ä¸ª**èµ·ç‚¹**ï¼Œæœ‰ä¸ª**ç»ˆç‚¹**ã€‚ä¸¤è€…ä¹‹é—´æœ‰ä¸€å †çŠ¶æ€ï¼Œä¸€å †è¡ŒåŠ¨ï¼Œä¸€å †å¥–åŠ±ï¼Œå’Œä¸€å †æ–°çš„çŠ¶æ€ï¼Œå®ƒä»¬å…±åŒæ„æˆäº†ä¸€â€œé›†â€ã€‚å½“ä¸€é›†ç»“æŸï¼Œä¹Ÿå°±æ˜¯åˆ°è¾¾ç»ˆæ­¢çŠ¶æ€çš„æ—¶å€™ï¼Œæ™ºèƒ½ä½“ä¼šçœ‹ä¸€ä¸‹å¥–åŠ±ç´¯ç§¯äº†å¤šå°‘ï¼Œä»¥æ­¤**è¯„ä¼°è‡ªå·±çš„è¡¨ç°**ã€‚

ç„¶åï¼Œå®ƒå°±å¸¦ç€ä¹‹å‰çš„ç»éªŒå¼€å§‹ä¸€å±€æ–°æ¸¸æˆã€‚è¿™ä¸€æ¬¡ï¼Œæ™ºèƒ½ä½“åšå†³å®šçš„ä¾æ®ä¼šå……åˆ†ä¸€äº›ã€‚

æ¯”å¦‚é©¬é‡Œå¥¥æ¸¸æˆï¼š

- æ°¸è¿œä»åŒä¸€ä¸ªèµ·ç‚¹å¼€å§‹
- å¦‚æœé©¬é‡Œå¥¥æ­»äº†æˆ–è€…åˆ°è¾¾ç»ˆç‚¹ï¼Œåˆ™æ¸¸æˆç»“æŸ
- ç»“æŸæ—¶å¾—åˆ°ä¸€ç³»åˆ—çŠ¶æ€ã€è¡ŒåŠ¨ã€å¥–åŠ±å’Œæ–°çŠ¶æ€
- ç®—å‡ºå¥–åŠ±çš„æ€»å’Œ (çœ‹çœ‹è¡¨ç°å¦‚ä½•)
- æ›´æœ‰ç»éªŒåœ°å¼€å§‹æ–°æ¸¸æˆ

**é›†æ•°è¶Šå¤š**ï¼Œ**æ™ºèƒ½ä½“çš„è¡¨ç°ä¼šè¶Šå¥½**



#### 1.3.2 è¿ç»­æ€§ä»»åŠ¡Continuing Task

**æ°¸è¿œä¸ä¼šæœ‰æ¸¸æˆç»“æŸçš„æ—¶å€™**ã€‚æ™ºèƒ½ä½“è¦å­¦ä¹ å¦‚ä½•é€‰æ‹©æœ€ä½³çš„è¡ŒåŠ¨ï¼Œå’Œç¯å¢ƒè¿›è¡Œå®æ—¶äº¤äº’ã€‚å°±åƒè‡ªåŠ¨é©¾é©¶æ±½è½¦ï¼Œå¹¶æ²¡æœ‰è¿‡å…³æ‹”æ——å­çš„äº‹ã€‚

è¿™æ ·çš„ä»»åŠ¡æ˜¯é€šè¿‡æ—¶é—´**å·®åˆ†å­¦ä¹ ** (Temporal Difference Learning) æ¥è®­ç»ƒçš„ã€‚æ¯ä¸€ä¸ªæ—¶é—´æ­¥ï¼Œéƒ½ä¼šæœ‰æ€»ç»“å­¦ä¹ ï¼Œç­‰ä¸åˆ°ä¸€é›†ç»“æŸå†åˆ†æç»“æœã€‚



### 1.4 æ¢ç´¢å’Œå¼€å‘ä¹‹é—´çš„æƒè¡¡

- **æ¢ç´¢** (Exploration) æ˜¯æ‰¾åˆ°å…³äºç¯å¢ƒçš„æ›´å¤šä¿¡æ¯ï¼š

  You go every day to the same one that you know is good and **take the risk to miss another better restaurant.**

- **å¼€å‘** (Exploitation) æ˜¯åˆ©ç”¨å·²çŸ¥ä¿¡æ¯æ¥å¾—åˆ°æœ€å¤šçš„å¥–åŠ±ï¼š

  Try restaurants you never went to before, with the risk of having a bad experience **but the probable opportunity of a fantastic experience.**

æˆ‘ä»¬éœ€è¦è®¾å®šä¸€ç§è§„åˆ™ï¼Œè®©æ™ºèƒ½ä½“èƒ½å¤Ÿ**æŠŠæ¡äºŒè€…ä¹‹é—´çš„å¹³è¡¡**ï¼Œæ¥å¤„ç†ä»¥ä¸‹å›°å¢ƒï¼š

- è€é¼ åœ¨è¿·å®«é‡Œåƒå¥¶é…ªï¼Œåœ¨å®ƒé™„è¿‘å¯ä»¥è¿Ÿåˆ°æ— æ•°å¿«åˆ†æ•£çš„å¥¶é…ª(+1)ï¼Œè¿·å®«æ·±å¤„æœ‰å·¨å‹å¥¶é…ª(+1000)ä½†ä¹Ÿå­˜åœ¨å±é™©ã€‚
  - å¦‚æœæˆ‘ä»¬åªå…³æ³¨åƒäº†å¤šå°‘ï¼Œè€é¼ åªä¼šåœ¨é™„è¿‘å®‰å…¨å¤„æ…¢æ…¢çš„åƒï¼Œæ°¸è¿œä¸ä¼šå»è¿·å®«æ·±å¤„ã€‚
  - å¦‚æœå®ƒé€‰æ‹©æ¢ç´¢è·‘å»è¿·å®«æ·±å¤„ï¼Œä¹Ÿè®¸èƒ½å‘ç°å¤§å¥–ä½†ä¹Ÿå¯èƒ½é‡åˆ°å±é™©ã€‚

### 1.5 è§‚æµ‹å’ŒçŠ¶æ€ç©ºé—´Observations/States Space

- Observations/States are the **information our agent gets from the environment.** 

  - In the case of a video game, it can be a frame (a screenshot). 

  - In the case of the trading agent, it can be the value of a certain stock, etc.

- åŒºåˆ«ï¼š

  - *State s*: is **a complete description of the state of the world** (there is no hidden information). In a fully observed environment.
  - *Observation o*: is a **partial description of the state.** In a partially observed environment.

### 1.6 åŠ¨ä½œç©ºé—´Action Space

- The Action space is the set of **all possible actions in an environment.**

- The actions can come from a *discrete* or *continuous space*:

  - *Discrete space*: the number of possible actions is **finite**.
    æ¯”å¦‚é©¬é‡Œå¥¥æ¸¸æˆåªæœ‰4ä¸ªæ–¹å‘çš„è·³è¿œ

  - *Continuous space*: the number of possible actions is **infinite**.

    æ¯”å¦‚è‡ªåŠ¨é©¾é©¶æ— æ•°æ–¹å‘çš„è¿åŠ¨

### 1.7 å¼ºåŒ–å­¦ä¹ çš„ä¸‰ç§æ–¹æ³•ï¼š

è®°ä½ï¼šRLçš„ç›®æ ‡æ˜¯æ‰¾åˆ°ä¸€ä¸ªoptimal policy Ï€.æœ‰ä¸‹é¢3ç§æ–¹æ³•ï¼Œä½†ç°å®ä¸­åªç”¨å‰2ç§

![](https://github.com/Fernweh-yang/Reading-Notes/blob/main/%E7%AC%94%E8%AE%B0%E9%85%8D%E5%A5%97%E5%9B%BE%E7%89%87/Reinforcement%20Learning/two-approaches%20to%20find%20policy.jpg?raw=true)

#### 1.7.1 åŸºäºä»·å€¼(value-based)

- ç›®æ ‡æ˜¯**ä¼˜åŒ–ä»·å€¼å‡½æ•°V(s)**: ä»·å€¼å‡½æ•°value functionä¼šå‘Šè¯‰æˆ‘ä»¬ï¼Œæ™ºèƒ½ä½“åœ¨æ¯ä¸ªçŠ¶æ€é‡Œå¾—å‡ºçš„æœªæ¥å¥–åŠ±æœ€å¤§é¢„æœŸ (maximum expected future reward) ã€‚In Value-based methods, instead of training a policy function, **we train a value function that maps a state to the expected value of being at that state**.

- æœ‰ä¸¤ç§åŸºäºä»·å€¼çš„æ–¹æ³•ï¼š

  1. The State-Value function
     $$
     V_\pi(s) = E_\pi[R_{t+1}+\gamma R_{t+2}+\gamma^2R_{t+3}+... | S_t=s]
     $$
     For each state, the state-value function outputs the expected return if the agent **starts at that state,** and then follow the policy forever after (for all future timesteps if you prefer).

     - E: Expected
     - S: State
     - Rï¼šreward

  2. The Action-Value function
     $$
     Q_\pi(s,a) = E_\pi[R_{t+1}+\gamma R_{t+2}+\gamma^2R_{t+3}+... | S_t=s,A_t=a]
     $$
     In the Action-value function, for each state and action pair, the action-value function **outputs the expected return** if the agent starts in that state and takes action, and then follows the policy forever after.

  3. Difference:

     - In state-value function, we calculate **the value of a state $S_t$**
     - In action-value function, we calculate **the value of the state-action pair $(S_t,A_t)$** hence the value of taking that action at that state.

- æ‰€ä»¥åŸºäºä»·å€¼çš„æ–¹æ³•ï¼š

  - don't train the policy
  - The policy is a function defined by hand.

- Value å’Œ Policyä¹‹é—´çš„è”ç³»
  $$
  \pi^*(s)=arg\ \underset{a}{max}Q^*(s,a)
  $$
  

#### 1.7.2 åŸºäºç­–ç•¥ (policy-based)

- The Policy Ï€ **is the brain of our Agent**, itâ€™s the function that tell us what action to take given the state we are. So it defines the agentâ€™s behavior at a given time.
- This policy function will **map from each state to the best corresponding action at that state**. Or a **probability distribution over the set of possible actions at that state**.

- ç›®æ ‡æ˜¯ **ä¼˜åŒ–ç­–ç•¥å‡½æ•°$\pi(s)$** , ç­–ç•¥å°±æ˜¯è¯„åˆ¤æ™ºèƒ½ä½“åœ¨ç‰¹å®šæ—¶é—´ç‚¹çš„è¡¨ç°ã€‚æŠŠæ¯ä¸€ä¸ªçŠ¶æ€å’Œå®ƒæ‰€å¯¹åº”çš„æœ€ä½³è¡ŒåŠ¨å»ºç«‹è”ç³»ã€‚

  ç­–ç•¥åˆ†ä¸ºä¸¤ç§

  - **ç¡®å®šæ€§**ç­–ç•¥ï¼šæŸä¸€ä¸ªç‰¹å®šçŠ¶æ€ä¸‹çš„ç­–ç•¥ï¼Œæ°¸è¿œéƒ½ä¼šç»™å‡ºåŒæ ·çš„è¡ŒåŠ¨ã€‚


  - **éšæœºæ€§**ç­–ç•¥ï¼šç­–ç•¥ç»™å‡ºçš„æ˜¯å¤šç§è¡ŒåŠ¨çš„å¯èƒ½æ€§åˆ†å¸ƒã€‚
    $$
    Stochastic\ Policy:\pi(a|s)=P[A_t=a|S_t=s]
    $$

    - S: State
    - A: Action


- æ‰€ä»¥åŸºäºç­–ç•¥çš„æ–¹æ³•
  - Train directly the policy
  - The policy is a Neural Network.

#### 1.7.3 åŸºäºæ¨¡å‹(model-based)

è¿™ç§æ–¹æ³•æ˜¯å¯¹ç¯å¢ƒå»ºæ¨¡ã€‚è¿™è¡¨ç¤ºï¼Œæˆ‘ä»¬è¦åˆ›å»ºä¸€ä¸ªæ¨¡å‹ï¼Œæ¥è¡¨ç¤ºç¯å¢ƒçš„è¡Œä¸ºã€‚

é—®é¢˜æ˜¯ï¼Œ**æ¯ä¸ªç¯å¢ƒ**éƒ½ä¼šéœ€è¦ä¸€ä¸ªä¸åŒçš„æ¨¡å‹ (é©¬é‡Œå¥¥æ¯èµ°ä¸€æ­¥ï¼Œéƒ½ä¼šæœ‰ä¸€ä¸ªæ–°ç¯å¢ƒ) ã€‚è¿™ä¹Ÿæ˜¯è¿™ä¸ªæ–¹æ³•åœ¨å¼ºåŒ–å­¦ä¹ ä¸­å¹¶ä¸å¤ªå¸¸ç”¨çš„åŸå› ã€‚

#### 1.7.4 æ€»ç»“å¯¹æ¯”

- Consequently, whatever method you use to solve your problem, **you will have a policy**, but in the case of value-based methods you don't train it, your policy **is just a simple function that you specify** (for instance greedy policy) and this policy **uses the values given by the value-function to select its actions.**
- So the difference is:
  - In policy-based, **the optimal policy is found by training the policy directly.**
  - In value-based, **finding an optimal value function leads to having an optimal policy.**

## 2. Q-Learning

Q-learningæ˜¯ä¸€ç§åŸºäºä»·å€¼çš„ç®—æ³•ã€‚

### 2.1 è´å°”æ›¼æ–¹ç¨‹

Bellman Equation: simplify our value estimation

1.7.1ä¸­æåˆ°çš„2ç§åŸºäºä»·å€¼çš„å‡½æ•°(state-value or action-value function)ï¼Œéƒ½éœ€è¦sum all the rewards an agent can get if it starts at that state.è¿™å°±éå¸¸çš„tediouså†—é•¿çš„ã€‚å› æ­¤ç”¨è´å°”æ›¼æ–¹ç¨‹æ¥ç®€åŒ–ä»·å€¼å‡½æ•°çš„è®¡ç®—ã€‚
$$
V_\pi(s)=E_\pi [R_{t+1}+\gamma V_\pi(S_{t+1})|S_t=s]
$$

- $R_{t+1}$: immediate reward
- $\gamma V_\pi(S_{t+1})$:  discounted value of the next state
-  the idea of the Bellman equation is that instead of calculating each value as the sum of the expected return, **which is a long process**
-  

### 2.2 ä¸¤ç§è®­ç»ƒä»·å€¼å‡½æ•°çš„ç­–ç•¥strategies

- RL agentéƒ½æ˜¯é€šè¿‡ä½¿ç”¨ä¹‹å‰çš„ç»éªŒæ¥å­¦ä¹ ï¼Œäºæ­¤ä»–ä»¬çš„åŒºåˆ«æ˜¯ï¼š
  - Monte Carlo uses **an entire episode of experience before learning**
  - Temporal Difference uses **only a step$(S_t,A_t,R_{t+1},S_{t+1})$ to learn**

#### 2.2.1è’™ç‰¹å¡æ´›(Monte Carlo)

Monte Carlo waits until the end of the episode, calculates $G_t$(expected return) and uses it as **a target for updating $V(S_t)$.**So it requires a **complete entire episode of interaction before updating our value function.**

åœ¨ç©å®Œä¸€å±€åæ‰æ€»ç»“å­¦ä¹ ã€‚

- å…¬å¼ï¼š
  $$
  V(S_t)\leftarrow V(S_t)+\alpha[G_t-V(S_t)]
  $$

  - $V(S_t)$: value of state t

    - å·¦ä¾§çš„é‚£ä¸ªæ˜¯ï¼šæ–°çš„çŠ¶æ€ä»·å€¼
    - å³ä¾§çš„2ä¸ªæ˜¯ï¼šä¹‹å‰ä¼°è®¡çš„estimateçš„çŠ¶æ€ä»·å€¼

  - $G_t$: sum of the total rewards at timestep

- æµç¨‹ï¼š

  1. åœ¨ä¸€ä¸ªepisodeå…³å¡ç»“æŸåï¼Œæˆ‘ä»¬å¾—åˆ°è¿™ä¸€å±€ä¸€ç³»åˆ—çš„state,action,rewardå’Œnew state
  2. agentå°†æ‰€æœ‰rewardç›¸åŠ ï¼Œå¾—åˆ°$G_t$
  3. åˆ©ç”¨ä¸Šé¢çš„å…¬å¼æ›´æ–°ä»·å€¼å‡½æ•°$V(S_t)$
  4. ç”¨æ–°å­¦åˆ°çš„çŸ¥è¯†å¼€å§‹æ–°ä¸€è½®çš„æ¸¸æˆã€‚

#### 2.2.2æ—¶é—´å·®åˆ†(Temporal Difference)

Temporal difference, on the other hand, waits for only one interaction (one step)$S_{t+1}$ to form a TD target and update $V(S_t)$ using $R_{t+1}$ and $\gamma V(S_{t+1})$ã€‚

æ¯èµ°ä¸€æ­¥å­¦ä¹ ä¸€æ¬¡ã€‚

- å…¬å¼ï¼š
  $$
  V(S_t) \leftarrow V(S_t) + \alpha[R_{t+1}+\gamma V(S_{t+1})-V(S_t)]
  $$

  - $V(S_t)$: value of state 
    - å·¦ä¾§çš„é‚£ä¸ªæ˜¯ï¼šæ–°çš„çŠ¶æ€ä»·å€¼
    - å³ä¾§çš„2ä¸ªæ˜¯ï¼šä¹‹å‰ä¼°è®¡çš„estimateçš„çŠ¶æ€ä»·å€¼
  - $V(S_{t+1})$: value of next state
  - $\gamma$: discount
  - $R_{t+1}$: reward

- æµç¨‹

  1. æ¯ä¸€æ­¥ç»“æŸåï¼Œæˆ‘ä»¬ä¼šå¾—åˆ°state,action,rewardå’Œnew state

  2. å°†$V(S_{t+1})$å’Œ$R_{t+1}$ç”¨äºestimate $G_t$,å¹¶åˆ©ç”¨ä¸Šå¼æ›´æ–°æ–°çš„ä»·å€¼å‡½æ•°$V(S_t)$

     

### 2.3 [Q-Learning](https://huggingface.co/blog/deep-rl-q-part2#what-is-q-learning)æ˜¯ä»€ä¹ˆ

- Q-Learningæ˜¯ä¸€ä¸ªoff-policy, value-basedå‡½æ•°å¹¶ä½¿ç”¨Temporal Differenceæ–¹æ³•æ¥è®­ç»ƒå®ƒçš„action-value function.

- Q-Learningæ˜¯ä¸€ä¸ªç”¨æ¥è®­ç»ƒQ-functionçš„ç®—æ³•ã€‚

  ![](https://github.com/Fernweh-yang/Reading-Notes/blob/main/%E7%AC%94%E8%AE%B0%E9%85%8D%E5%A5%97%E5%9B%BE%E7%89%87/Reinforcement%20Learning/Q-function.jpg?raw=true)

  - Q-functionæ˜¯ä¸€ä¸ªaction-value functionã€‚å®ƒdetermineäº†åœ¨ä¸€ä¸ªparticular stateå’Œé‡‡å–specific actionæ—¶çš„valueã€‚
  - Q-tableè®°å½•äº†æ‰€æœ‰çš„state-action pair values ,æ˜¯Q-functionçš„memoryã€‚
    - åœ¨ç»™äºˆäº†ä¸€ä¸ªactionå’ŒstateåQ-functionä¼šæœç´¢è¿™ä¸ªQ-tableå¹¶è¾“å‡ºä¸€ä¸ªå€¼ã€‚
    - ä¸€å¼€å§‹Q-tableé€šå¸¸ä¼šå…¨éƒ½åˆå§‹ä¸º0ï¼Œåœ¨exploreç¯å¢ƒæ—¶æ›´æ–°Q-table

### 2.4 Q-Learningç®—æ³•

### 2.5 å¯¹æ¯”ï¼šoff-policyå’Œon-policy



# äºŒã€ä¸€çš„ä»£ç 

## 1. Foundation:

### 1.1 gym çš„åŸºæœ¬ç”¨æ³•ï¼š

```python
import gym

# First, we create our environment called LunarLander-v2
env = gym.make("LunarLander-v2")

# Then we reset this environment
observation = env.reset()

for _ in range(20):
  # Take a random action
  action = env.action_space.sample()
  print("Action taken:", action)

  # Do this action in the environment and get
  # next_state, reward, done and info
  observation, reward, done, info = env.step(action)
  
  # If the game is done (in our case we land, crashed or timeout)
  if done:
      # Reset the environment
      print("Environment is reset")
      observation = env.reset()
```

### 1.2 Stable-Baselines3çš„åŸºæœ¬ä½¿ç”¨æ­¥éª¤

```python
# Create environment
env = gym.make('LunarLander-v2')

# Instantiate the agent
model = PPO('MlpPolicy', env, verbose=1)
# Train the agent
model.learn(total_timesteps=int(2e5))
```



### 1.3 ä¸€ä¸ªè‡ªåŠ¨é™è½çš„æ™ºèƒ½é£æœº

è¿è¡Œåœ¨colabï¼šhttps://colab.research.google.com/github/huggingface/deep-rl-class/blob/main/unit1/unit1.ipynb

#### 1.3.1 å®‰è£…ç”¨äºcolabè™šæ‹Ÿç•Œé¢æ˜¾ç¤º

  ```python
  !sudo apt-get update
  !apt install python-opengl
  !apt install ffmpeg
  !apt install xvfb
  !pip3 install pyvirtualdisplay
  
  # Virtual display
  from pyvirtualdisplay import Display
  
  virtual_display = Display(visible=0, size=(1400, 900))
  virtual_display.start()
  ```

#### 1.3.2 **å®‰è£…æ·±åº¦å­¦ä¹ åº“**

  ```python
  !pip install importlib-metadata==4.12.0 # To overcome an issue with importlib-metadata https://stackoverflow.com/questions/73929564/entrypoints-object-has-no-attribute-get-digital-ocean
  !pip install gym[box2d]
  !pip install stable-baselines3[extra]
  !pip install huggingface_sb3
  !pip install pyglet==1.5.1
  !pip install ale-py==0.7.4 # To overcome an issue with gym (https://github.com/DLR-RM/stable-baselines3/issues/875)
  
  !pip install pickle5
  ```

#### 1.3.3 **å¯¼å…¥åŒ…**

  ```python
  import gym
  
  from huggingface_sb3 import load_from_hub, package_to_hub, push_to_hub
  from huggingface_hub import notebook_login # To log to our Hugging Face account to be able to upload models to the Hub.
  
  from stable_baselines3 import PPO
  from stable_baselines3 import DQN	# æ‰©å±•éƒ¨åˆ†ï¼šå°è¯•ä½¿ç”¨å¦ä¸ªç®—æ³•DQN
  from stable_baselines3.common.evaluation import evaluate_policy
  from stable_baselines3.common.env_util import make_vec_env
  ```

#### 1.3.4 **å»ºç«‹ç¯å¢ƒ**

- ç¯å¢ƒæ–‡æ¡£ï¼šhttps://www.gymlibrary.dev/environments/box2d/lunar_lander/

  - Action Space:
    - do nothing
    - fire left orientation engine
    - fire main engine
    - fire right orientation engine.
  - Observation Space
    - the coordinates of the lander in `x` & `y`
    - its linear velocities in `x` & `y`
    - its angle
    - its angular velocity
    - two booleans that represent whether each leg is in contact with the ground or not.
  - Rewards
    - Moving from the top of the screen to the landing pad and zero speed is about 100~140 points.
    - Firing main engine is -0.3 each frame
    - Each leg ground contact is +10 points
    - Episode finishes if the lander crashes (additional - 100 points) or come to rest (+100 points)

-  å»ºç«‹å¹¶æŸ¥çœ‹ç¯å¢ƒ

```python
# ä½¿ç”¨gymåˆ›å»ºç¯å¢ƒ
env = gym.make("LunarLander-v2")
env.reset()	#å°†ç¯å¢ƒé‡ç½®
print("_____OBSERVATION SPACE_____ \n")
print("Observation Space Shape", env.observation_space.shape)
print("Sample observation", env.observation_space.sample()) # Get a random observation
print("\n _____ACTION SPACE_____ \n")
print("Action Space Shape", env.action_space.n)
print("Action Space Sample", env.action_space.sample()) # Take a random action


# ä½¿ç”¨stable baseline3å‘é‡åŒ–ç¯å¢ƒ
# We create a vectorized environment (method for stacking multiple independent environments into a single environment) of 16 environments, this way, we'll have more diverse experiences during the training.
env = make_vec_env('LunarLander-v2', n_envs=16)
```



#### 1.3.5 **å»ºç«‹RLç®—æ³•æ¨¡å‹**

- ä½¿ç”¨ [Stable Baselines3 (SB3)](https://stable-baselines3.readthedocs.io/en/master/)ï¼ŒSB3 is a set of **reliable implementations of reinforcement learning algorithms in PyTorch**.

##### 1.[PPOç®—æ³•](https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html#example%5D)

```python
# We use MultiLayerPerceptron (MLPPolicy) because the input is a vector,
# if we had frames as input we would use CnnPolicy
model = PPO(
  policy = 'MlpPolicy',
  env = env,
  n_steps = 1024,
  batch_size = 64,
  n_epochs = 4,
  gamma = 0.999,
  gae_lambda = 0.98,
  ent_coef = 0.01,
  verbose=1)
```

##### 2.DQNç®—æ³•

```python
model = DQN(
    policy='MlpPolicy',
    env = env,
    verbose=1,
    batch_size= 64  
)
```



#### 1.3.6 **è®­ç»ƒæ¨¡å‹**

1. PPO:

   ```python
   # Train it for 500,000 timesteps
   model.learn(total_timesteps=500000)
   # Save the model
   model_name = "ppo-LunarLander-v2"
   model.save(model_name)
   ```

   

2. DQN:

   ```python
   # Train it for 500,000 timesteps
   model.learn(total_timesteps=500000)
   # Save the model
   model_name = "dqn-LunarLander-v2"
   model.save(model_name)
   ```

   

#### 1.3.7 **è¯„ä¼°è®­ç»ƒå®Œçš„æ™ºèƒ½ä½“**

```python
eval_env = gym.make("LunarLander-v2")
mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=10, deterministic=True)
print(f"mean_reward={mean_reward:.2f} +/- {std_reward}")
```

#### 1.3.8 **å‘å¸ƒæ¨¡å‹åˆ°Huggingfaceä¸Š**

- [Huggingfaceåº“](https://github.com/huggingface/huggingface_sb3/tree/main#hugging-face--x-stable-baselines3-v20)åŠŸèƒ½ï¼š

  - You can **showcase our work** ğŸ”¥
  - You can **visualize your agent playing** ğŸ‘€
  - You can **share with the community an agent that others can use** ğŸ’¾
  - You can **access a leaderboard ğŸ† to see how well your agent is performing compared to your classmates** ğŸ‘‰

- è¿æ¥ç”µè„‘åˆ°Huggingface

  - Create a new token (https://huggingface.co/settings/tokens) **with write role**

    è¾“å…¥å¦‚ä¸‹å‘½ä»¤åä¼šè¦æ±‚è¾“å…¥token

  ```python
  # å¦‚æœç”¨çš„colabæˆ–è€…jupyter
  notebook_login()
  !git config --global credential.helper store
  # å¦‚æœç”¨çš„æœ¬åœ°pythonç¯å¢ƒ
  huggingface-cli login
  ```

- ä½¿ç”¨package_to_hub()å‡½æ•°æ¥ä¸Šä¼ ä»£ç 

  ```python
  import gym
  from stable_baselines3.common.vec_env import DummyVecEnv
  from stable_baselines3.common.env_util import make_vec_env
  
  from huggingface_sb3 import package_to_hub
  
  ## TODO: Define a repo_id
  ## repo_id is the id of the model repository from the Hugging Face Hub (repo_id = {organization}/{repo_name} for instance ThomasSimonini/ppo-LunarLander-v2
  repo_id = "TUMxudashuai/ppo-LunarLander-v2"
  
  # TODO: Define the name of the environment
  env_id =  "LunarLander-v2"
  
  # Create the evaluation env
  eval_env = DummyVecEnv([lambda: gym.make(env_id)])
  
  
  # TODO: Define the model architecture we used
  model_architecture = "PPO"
  
  ## TODO: Define the commit message
  commit_message = "Upload PPO LunarLander-v2 trained agent"
  
  # method save, evaluate, generate a model card and record a replay video of your agent before pushing the repo to the hub
  package_to_hub(model=model, # Our trained model
                 model_name=model_name, # The name of our trained model 
                 model_architecture=model_architecture, # The model architecture we used: in our case PPO
                 env_id=env_id, # Name of the environment
                 eval_env=eval_env, # Evaluation Environment
                 repo_id=repo_id, # id of the model repository from the Hugging Face Hub (repo_id = {organization}/{repo_name} for instance ThomasSimonini/ppo-LunarLander-v2
                 commit_message=commit_message)
  
  # Note: if after running the package_to_hub function and it gives an issue of rebasing, please run the following code
  # cd <path_to_repo> && git add . && git commit -m "Add message" && git pull 
  # And don't forget to do a "git push" at the end to push the change to the hub.
  ```

  - model here: https://huggingface.co/TUMxudashuai/ppo-LunarLander-v2
    - see a video preview of your agent at the right.
    - click "Files and versions" to see all the files in the repository.
    - click "Use in stable-baselines3" to get a code snippet that shows how to load the model.
    - a model card (`README.md` file) which gives a description of the model

#### æ‰©å±•ï¼š

1. Try different hyperparameters of `PPO`. You can see them at https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html#parameters.
2. Check the [Stable-Baselines3 documentation](https://stable-baselines3.readthedocs.io/en/master/modules/dqn.html) and try another models such as DQN.
3. Try to **change the environment**, why not using CartPole-v1, MountainCar-v0 or CarRacing-v0? Check how they works [using the gym documentation](https://www.gymlibrary.dev/) and have fun

### 1.4 å€’ç«‹æ‘†cartpole

#### 1.4.1 ç¯å¢ƒ

```python
!sudo apt-get update
!apt install python-opengl
!apt install ffmpeg
!apt install xvfb
!pip3 install pyvirtualdisplay

# Virtual display
from pyvirtualdisplay import Display

virtual_display = Display(visible=0, size=(1400, 900))
virtual_display.start()

!pip install importlib-metadata==4.13.0
!pip install gym[classic_control]
!pip install stable-baselines3[extra]
!pip install huggingface_sb3
!pip install pyglet==1.5.1
!pip install ale-py==0.7.4 # To overcome an issue with gym (https://github.com/DLR-RM/stable-baselines3/issues/875)

!pip install pickle5
```

#### 1.4.2 åŠ è½½åº“

```python
import gym

from huggingface_sb3 import load_from_hub, package_to_hub, push_to_hub
from huggingface_hub import notebook_login # To log to our Hugging Face account to be able to upload models to the Hub.

from stable_baselines3 import PPO
from stable_baselines3 import DQN	# æ‰©å±•éƒ¨åˆ†ï¼šå°è¯•ä½¿ç”¨å¦ä¸ªç®—æ³•DQN
from stable_baselines3.common.evaluation import evaluate_policy
from stable_baselines3.common.env_util import make_vec_env
```

#### 1.4.3 åˆ›å»ºç¯å¢ƒ

```python
env = gym.make('CartPole-v1')
env.reset()
print("_____OBSERVATION SPACE_____ \n")
print("Observation Space Shape", env.observation_space.shape)
print("Sample observation", env.observation_space.sample()) # Get a random observation
print("\n _____ACTION SPACE_____ \n")
print("Action Space Shape", env.action_space.n)
print("Action Space Sample", env.action_space.sample()) # Take a random action
env = make_vec_env('CartPole-v1', n_envs=16)
```

#### 1.4.4 åˆ›å»ºæ¨¡å‹

```python
model = PPO(
  policy = 'MlpPolicy',
  env = env,
  n_steps = 1024,
  batch_size = 64,
  n_epochs = 4,
  gamma = 0.999,
  gae_lambda = 0.98,
  ent_coef = 0.01,
  verbose=1)
```



#### 1.4.5 è®­ç»ƒæ¨¡å‹

```
# Train it for 500,000 timesteps
model.learn(total_timesteps=500000)
model_name = "ppo-CartPole-v1"
model.save(model_name)
```

