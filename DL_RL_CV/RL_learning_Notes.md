# é›¶ã€å­¦ä¹ èµ„æºæ•´ç†

1. [OpenAI Spinning Up](https://spinningup.openai.com/en/latest/index.html)
2. [æœ¬ç¬”è®°æ‰€å­¦ä¹ çš„è¯¾](https://github.com/huggingface/deep-rl-class)
3. [Reinforcement Learning: An Introduction](http://incompleteideas.net/book/RLbook2020.pdf)(å·²ä¿å­˜åœ¨onedrive)
4. æå®æ¯…æ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼š
   - [ç¬”è®°](https://github.com/changliang5811/leedeeprl-notes)
   - [è¯¾ç¨‹](http://speech.ee.ntu.edu.tw/~tlkagk/courses_MLDS18.html)ï¼Œ[bilibili](https://www.bilibili.com/video/BV1MW411w79n/)

# ä¸€ã€æ·±åº¦å¼ºåŒ–å­¦ä¹ 

ç½‘è¯¾ï¼šhttps://github.com/huggingface/deep-rl-class#the-hugging-face-deep-reinforcement-learning-class-

## 1. FoundationåŸºç¡€

- Formal Definition

  Reinforcement learning is a framework for solving control tasks (also called decision problems) by building agents that learn from the environment by interacting with it through trial and error and receiving rewards (positive or negative) as unique feedback

### 1.1 å¼ºåŒ–å­¦ä¹ ä¸­å¿ƒæ€æƒ³

å¼ºåŒ–å­¦ä¹ çš„ä¸­å¿ƒæ€æƒ³ï¼Œå°±æ˜¯**è®©æ™ºèƒ½ä½“åœ¨ç¯å¢ƒé‡Œå­¦ä¹ **ã€‚æ¯ä¸ªè¡ŒåŠ¨ä¼šå¯¹åº”å„è‡ªçš„å¥–åŠ±ï¼Œæ™ºèƒ½ä½“é€šè¿‡åˆ†ææ•°æ®æ¥å­¦ä¹ ï¼Œæ€æ ·çš„æƒ…å†µä¸‹åº”è¯¥åšæ€æ ·çš„äº‹æƒ…ã€‚

æ¯”å¦‚æ™ºèƒ½ä½“è¦å­¦ç€ç©è¶…çº§é©¬é‡Œå¥¥ï¼š

![](https://picd.zhimg.com/80/v2-e058a9d05a30d09edbbd0b70da685133_720w.webp?source=1940ef5c)

- æ™ºèƒ½ä½“åœ¨ç¯å¢ƒ (è¶…çº§é©¬é‡Œå¥¥) é‡Œè·å¾—åˆå§‹çŠ¶æ€**S0** (æ¸¸æˆçš„ç¬¬ä¸€å¸§) ï¼›

- åœ¨S0çš„åŸºç¡€ä¸Šï¼Œagentä¼šåšå‡ºç¬¬ä¸€ä¸ªè¡ŒåŠ¨**A0** (å¦‚å‘å³èµ°) ï¼›

- ç¯å¢ƒå˜åŒ–ï¼Œè·å¾—æ–°çš„çŠ¶æ€**S1** (A0å‘ç”Ÿåçš„æŸä¸€å¸§) ï¼›

- ç¯å¢ƒç»™å‡ºäº†ç¬¬ä¸€ä¸ªå¥–åŠ±**R1** (æ²¡æ­»ï¼š+1) ï¼›

  äºæ˜¯ï¼Œè¿™ä¸ª[loop](https://www.zhihu.com/search?q=loop&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A602826860})è¾“å‡ºçš„å°±æ˜¯ä¸€ä¸ª**ç”±çŠ¶æ€ã€å¥–åŠ±å’Œè¡ŒåŠ¨ç»„æˆçš„åºåˆ—**ã€‚

  è€Œæ™ºèƒ½ä½“çš„**ç›®æ ‡**å°±æ˜¯è®©**é¢„æœŸç´¯ç§¯å¥–åŠ±æœ€å¤§åŒ–**ã€‚





### 1.2 å¥–åŠ±å‡è¯´reward hypothesis

ç”±ä¸Šå¯çŸ¥ç›®æ ‡æ˜¯è¦è®©ç´¯ç§¯å¥–åŠ±æœ€å¤§åŒ–

- æ¯ä¸€ä¸ªæ—¶é—´æ­¥çš„ç´¯ç§¯å¥–åŠ±å¯ä»¥è¡¨ç¤ºä¸ºï¼š
  $$
  G_t = R_{t+1}+R_{t+2}+...
  $$
  ä½†ç°å®ä¸­**ä¸èƒ½æŠŠå¥–åŠ±ç›´æ¥ç›¸åŠ **ã€‚å› ä¸ºæ¸¸æˆé‡Œï¼Œè¶Šæ¥è¿‘æ¸¸æˆå¼€å§‹å¤„çš„å¥–åŠ±ï¼Œå°±è¶Šå®¹æ˜“è·å¾—ï¼›è€Œéšç€æ¸¸æˆçš„è¿›è¡Œï¼Œåé¢çš„å¥–åŠ±å°±æ²¡æœ‰é‚£ä¹ˆå®¹æ˜“æ‹¿åˆ°äº†ã€‚

- ç»™å¥–åŠ±æ·»åŠ æŠ˜æ‰£

  ç”¨$\gamma$è¡¨ç¤ºæŠ˜æ‰£ç‡ï¼Œ0-1ä¹‹é—´

  - $\gamma$è¶Šå¤§ï¼ŒæŠ˜æ‰£è¶Šå°ã€‚è¡¨ç¤ºæ™ºèƒ½ä½“è¶Šåœ¨æ„é•¿æœŸçš„å¥–åŠ±ã€‚
  - $\gamma$è¶Šå°ï¼ŒæŠ˜æ‰£è¶Šå¤§ã€‚è¡¨ç¤ºæ™ºèƒ½ä½“è¶Šåœ¨æ„çŸ­æœŸçš„å¥–åŠ±ã€‚

  $$
  G_t = R_{t+1} + \gamma R_{t+2}+\gamma^2 R_{t+3}+...
  $$

  ç®€å•æ¥è¯´ï¼Œ**ç¦»å›°éš¾çš„åœ°æ–¹è¿‘ä¸€æ­¥**ï¼Œ**å°±ä¹˜ä¸Šä¸€ä¸ªÎ³**ï¼Œè¡¨ç¤ºå¥–åŠ±è¶Šéš¾è·å¾—ã€‚





### 1.3 å¼ºåŒ–å­¦ä¹ çš„ä¸¤ç§ä»»åŠ¡

#### 1.3.1 ç‰‡æ®µæ€§ä»»åŠ¡Episodic Task

è¿™ç±»ä»»åŠ¡ï¼Œæœ‰ä¸ª**èµ·ç‚¹**ï¼Œæœ‰ä¸ª**ç»ˆç‚¹**ã€‚ä¸¤è€…ä¹‹é—´æœ‰ä¸€å †çŠ¶æ€ï¼Œä¸€å †è¡ŒåŠ¨ï¼Œä¸€å †å¥–åŠ±ï¼Œå’Œä¸€å †æ–°çš„çŠ¶æ€ï¼Œå®ƒä»¬å…±åŒæ„æˆäº†ä¸€â€œé›†â€ã€‚å½“ä¸€é›†ç»“æŸï¼Œä¹Ÿå°±æ˜¯åˆ°è¾¾ç»ˆæ­¢çŠ¶æ€çš„æ—¶å€™ï¼Œæ™ºèƒ½ä½“ä¼šçœ‹ä¸€ä¸‹å¥–åŠ±ç´¯ç§¯äº†å¤šå°‘ï¼Œä»¥æ­¤**è¯„ä¼°è‡ªå·±çš„è¡¨ç°**ã€‚

ç„¶åï¼Œå®ƒå°±å¸¦ç€ä¹‹å‰çš„ç»éªŒå¼€å§‹ä¸€å±€æ–°æ¸¸æˆã€‚è¿™ä¸€æ¬¡ï¼Œæ™ºèƒ½ä½“åšå†³å®šçš„ä¾æ®ä¼šå……åˆ†ä¸€äº›ã€‚

æ¯”å¦‚é©¬é‡Œå¥¥æ¸¸æˆï¼š

- æ°¸è¿œä»åŒä¸€ä¸ªèµ·ç‚¹å¼€å§‹
- å¦‚æœé©¬é‡Œå¥¥æ­»äº†æˆ–è€…åˆ°è¾¾ç»ˆç‚¹ï¼Œåˆ™æ¸¸æˆç»“æŸ
- ç»“æŸæ—¶å¾—åˆ°ä¸€ç³»åˆ—çŠ¶æ€ã€è¡ŒåŠ¨ã€å¥–åŠ±å’Œæ–°çŠ¶æ€
- ç®—å‡ºå¥–åŠ±çš„æ€»å’Œ (çœ‹çœ‹è¡¨ç°å¦‚ä½•)
- æ›´æœ‰ç»éªŒåœ°å¼€å§‹æ–°æ¸¸æˆ

**é›†æ•°è¶Šå¤š**ï¼Œ**æ™ºèƒ½ä½“çš„è¡¨ç°ä¼šè¶Šå¥½**



#### 1.3.2 è¿ç»­æ€§ä»»åŠ¡Continuing Task

**æ°¸è¿œä¸ä¼šæœ‰æ¸¸æˆç»“æŸçš„æ—¶å€™**ã€‚æ™ºèƒ½ä½“è¦å­¦ä¹ å¦‚ä½•é€‰æ‹©æœ€ä½³çš„è¡ŒåŠ¨ï¼Œå’Œç¯å¢ƒè¿›è¡Œå®æ—¶äº¤äº’ã€‚å°±åƒè‡ªåŠ¨é©¾é©¶æ±½è½¦ï¼Œå¹¶æ²¡æœ‰è¿‡å…³æ‹”æ——å­çš„äº‹ã€‚

è¿™æ ·çš„ä»»åŠ¡æ˜¯é€šè¿‡æ—¶é—´**å·®åˆ†å­¦ä¹ ** (Temporal Difference Learning) æ¥è®­ç»ƒçš„ã€‚æ¯ä¸€ä¸ªæ—¶é—´æ­¥ï¼Œéƒ½ä¼šæœ‰æ€»ç»“å­¦ä¹ ï¼Œç­‰ä¸åˆ°ä¸€é›†ç»“æŸå†åˆ†æç»“æœã€‚



### 1.4 æ¢ç´¢å’Œå¼€å‘ä¹‹é—´çš„æƒè¡¡

- **æ¢ç´¢** (Exploration) æ˜¯æ‰¾åˆ°å…³äºç¯å¢ƒçš„æ›´å¤šä¿¡æ¯ï¼š

  You go every day to the same one that you know is good and **take the risk to miss another better restaurant.**

- **å¼€å‘** (Exploitation) æ˜¯åˆ©ç”¨å·²çŸ¥ä¿¡æ¯æ¥å¾—åˆ°æœ€å¤šçš„å¥–åŠ±ï¼š

  Try restaurants you never went to before, with the risk of having a bad experience **but the probable opportunity of a fantastic experience.**

æˆ‘ä»¬éœ€è¦è®¾å®šä¸€ç§è§„åˆ™ï¼Œè®©æ™ºèƒ½ä½“èƒ½å¤Ÿ**æŠŠæ¡äºŒè€…ä¹‹é—´çš„å¹³è¡¡**ï¼Œæ¥å¤„ç†ä»¥ä¸‹å›°å¢ƒï¼š

- è€é¼ åœ¨è¿·å®«é‡Œåƒå¥¶é…ªï¼Œåœ¨å®ƒé™„è¿‘å¯ä»¥è¿Ÿåˆ°æ— æ•°å¿«åˆ†æ•£çš„å¥¶é…ª(+1)ï¼Œè¿·å®«æ·±å¤„æœ‰å·¨å‹å¥¶é…ª(+1000)ä½†ä¹Ÿå­˜åœ¨å±é™©ã€‚
  - å¦‚æœæˆ‘ä»¬åªå…³æ³¨åƒäº†å¤šå°‘ï¼Œè€é¼ åªä¼šåœ¨é™„è¿‘å®‰å…¨å¤„æ…¢æ…¢çš„åƒï¼Œæ°¸è¿œä¸ä¼šå»è¿·å®«æ·±å¤„ã€‚
  - å¦‚æœå®ƒé€‰æ‹©æ¢ç´¢è·‘å»è¿·å®«æ·±å¤„ï¼Œä¹Ÿè®¸èƒ½å‘ç°å¤§å¥–ä½†ä¹Ÿå¯èƒ½é‡åˆ°å±é™©ã€‚

### 1.5 è§‚æµ‹å’ŒçŠ¶æ€ç©ºé—´Observations/States Space

- Observations/States are the **information our agent gets from the environment.** 

  - In the case of a video game, it can be a frame (a screenshot). 

  - In the case of the trading agent, it can be the value of a certain stock, etc.

- åŒºåˆ«ï¼š

  - *State s*: is **a complete description of the state of the world** (there is no hidden information). In a fully observed environment.
  - *Observation o*: is a **partial description of the state.** In a partially observed environment.

### 1.6 åŠ¨ä½œç©ºé—´Action Space

- The Action space is the set of **all possible actions in an environment.**

- The actions can come from a *discrete* or *continuous space*:

  - *Discrete space*: the number of possible actions is **finite**.
    æ¯”å¦‚é©¬é‡Œå¥¥æ¸¸æˆåªæœ‰4ä¸ªæ–¹å‘çš„è·³è¿œ

  - *Continuous space*: the number of possible actions is **infinite**.

    æ¯”å¦‚è‡ªåŠ¨é©¾é©¶æ— æ•°æ–¹å‘çš„è¿åŠ¨

### 1.7 å¼ºåŒ–å­¦ä¹ çš„ä¸‰ç§æ–¹æ³•ï¼š

è®°ä½ï¼šRLçš„ç›®æ ‡æ˜¯æ‰¾åˆ°ä¸€ä¸ªoptimal policy Ï€.æœ‰ä¸‹é¢3ç§æ–¹æ³•ï¼Œä½†ç°å®ä¸­åªç”¨å‰2ç§

![](https://github.com/Fernweh-yang/Reading-Notes/blob/main/%E7%AC%94%E8%AE%B0%E9%85%8D%E5%A5%97%E5%9B%BE%E7%89%87/Reinforcement%20Learning/two-approaches%20to%20find%20policy.jpg?raw=true)

#### 1.7.1 åŸºäºä»·å€¼(value-based)

- ç›®æ ‡æ˜¯**ä¼˜åŒ–ä»·å€¼å‡½æ•°V(s)**: ä»·å€¼å‡½æ•°value functionä¼šå‘Šè¯‰æˆ‘ä»¬ï¼Œæ™ºèƒ½ä½“åœ¨æ¯ä¸ªçŠ¶æ€é‡Œå¾—å‡ºçš„æœªæ¥å¥–åŠ±æœ€å¤§é¢„æœŸ (maximum expected future reward) ã€‚In Value-based methods, instead of training a policy function, **we train a value function that maps a state to the expected value of being at that state**.

- æœ‰ä¸¤ç§åŸºäºä»·å€¼çš„æ–¹æ³•ï¼š

  1. The State-Value function
     $$
     V_\pi(s) = E_\pi[R_{t+1}+\gamma R_{t+2}+\gamma^2R_{t+3}+... | S_t=s]
     $$
     For each state, the state-value function outputs the expected return if the agent **starts at that state,** and then follow the policy forever after (for all future timesteps if you prefer).

     - E: Expected
     - S: State
     - Rï¼šreward

  2. The Action-Value function
     $$
     Q_\pi(s,a) = E_\pi[R_{t+1}+\gamma R_{t+2}+\gamma^2R_{t+3}+... | S_t=s,A_t=a]
     $$
     In the Action-value function, for each state and action pair, the action-value function **outputs the expected return** if the agent starts in that state and takes action, and then follows the policy forever after.

  3. Difference:

     - In state-value function, we calculate **the value of a state $S_t$**
     - In action-value function, we calculate **the value of the state-action pair $(S_t,A_t)$** hence the value of taking that action at that state.

- æ‰€ä»¥åŸºäºä»·å€¼çš„æ–¹æ³•ï¼š

  - don't train the policy
  - The policy is a function defined by hand.

- Value å’Œ Policyä¹‹é—´çš„è”ç³»
  $$
  \pi^*(s)=arg\ \underset{a}{max}Q^*(s,a)
  $$
  

#### 1.7.2 åŸºäºç­–ç•¥ (policy-based)

- The Policy Ï€ **is the brain of our Agent**, itâ€™s the function that tell us what action to take given the state we are. So it defines the agentâ€™s behavior at a given time.
- This policy function will **map from each state to the best corresponding action at that state**. Or a **probability distribution over the set of possible actions at that state**.

- ç›®æ ‡æ˜¯ **ä¼˜åŒ–ç­–ç•¥å‡½æ•°$\pi(s)$** , ç­–ç•¥å°±æ˜¯è¯„åˆ¤æ™ºèƒ½ä½“åœ¨ç‰¹å®šæ—¶é—´ç‚¹çš„è¡¨ç°ã€‚æŠŠæ¯ä¸€ä¸ªçŠ¶æ€å’Œå®ƒæ‰€å¯¹åº”çš„æœ€ä½³è¡ŒåŠ¨å»ºç«‹è”ç³»ã€‚

  ç­–ç•¥åˆ†ä¸ºä¸¤ç§

  - **ç¡®å®šæ€§**ç­–ç•¥ï¼šæŸä¸€ä¸ªç‰¹å®šçŠ¶æ€ä¸‹çš„ç­–ç•¥ï¼Œæ°¸è¿œéƒ½ä¼šç»™å‡ºåŒæ ·çš„è¡ŒåŠ¨ã€‚


  - **éšæœºæ€§**ç­–ç•¥ï¼šç­–ç•¥ç»™å‡ºçš„æ˜¯å¤šç§è¡ŒåŠ¨çš„å¯èƒ½æ€§åˆ†å¸ƒã€‚
    $$
    Stochastic\ Policy:\pi(a|s)=P[A_t=a|S_t=s]
    $$

    - S: State
    - A: Action


- æ‰€ä»¥åŸºäºç­–ç•¥çš„æ–¹æ³•
  - Train directly the policy
  - The policy is a Neural Network.

#### 1.7.3 åŸºäºæ¨¡å‹(model-based)

è¿™ç§æ–¹æ³•æ˜¯å¯¹ç¯å¢ƒå»ºæ¨¡ã€‚è¿™è¡¨ç¤ºï¼Œæˆ‘ä»¬è¦åˆ›å»ºä¸€ä¸ªæ¨¡å‹ï¼Œæ¥è¡¨ç¤ºç¯å¢ƒçš„è¡Œä¸ºã€‚

é—®é¢˜æ˜¯ï¼Œ**æ¯ä¸ªç¯å¢ƒ**éƒ½ä¼šéœ€è¦ä¸€ä¸ªä¸åŒçš„æ¨¡å‹ (é©¬é‡Œå¥¥æ¯èµ°ä¸€æ­¥ï¼Œéƒ½ä¼šæœ‰ä¸€ä¸ªæ–°ç¯å¢ƒ) ã€‚è¿™ä¹Ÿæ˜¯è¿™ä¸ªæ–¹æ³•åœ¨å¼ºåŒ–å­¦ä¹ ä¸­å¹¶ä¸å¤ªå¸¸ç”¨çš„åŸå› ã€‚

#### 1.7.4 æ€»ç»“å¯¹æ¯”

- Consequently, whatever method you use to solve your problem, **you will have a policy**, but in the case of value-based methods you don't train it, your policy **is just a simple function that you specify** (for instance greedy policy) and this policy **uses the values given by the value-function to select its actions.**
- So the difference is:
  - In policy-based, **the optimal policy is found by training the policy directly.**
  - In value-based, **finding an optimal value function leads to having an optimal policy.**

## 2. Q-Learning

Q-learningæ˜¯ä¸€ç§åŸºäºä»·å€¼çš„ç®—æ³•ã€‚The **Q comes from "the Quality" of that action at that state.**

### 2.1 è´å°”æ›¼æ–¹ç¨‹

Bellman Equation: simplify our value estimation

1.7.1ä¸­æåˆ°çš„2ç§åŸºäºä»·å€¼çš„å‡½æ•°(state-value or action-value function)ï¼Œéƒ½éœ€è¦sum all the rewards an agent can get if it starts at that state.è¿™å°±éå¸¸çš„tediouså†—é•¿çš„ã€‚å› æ­¤ç”¨è´å°”æ›¼æ–¹ç¨‹æ¥ç®€åŒ–ä»·å€¼å‡½æ•°çš„è®¡ç®—ã€‚
$$
V_\pi(s)=E_\pi [R_{t+1}+\gamma V_\pi(S_{t+1})|S_t=s]
$$

- $R_{t+1}$: immediate reward
- $\gamma V_\pi(S_{t+1})$:  discounted value of the next state
-  the idea of the Bellman equation is that instead of calculating each value as the sum of the expected return, **which is a long process**
-  

### 2.2 ä¸¤ç§è®­ç»ƒä»·å€¼å‡½æ•°çš„ç­–ç•¥strategies

- RL agentéƒ½æ˜¯é€šè¿‡ä½¿ç”¨ä¹‹å‰çš„ç»éªŒæ¥å­¦ä¹ ï¼Œäºæ­¤ä»–ä»¬çš„åŒºåˆ«æ˜¯ï¼š
  - Monte Carlo uses **an entire episode of experience before learning**
  - Temporal Difference uses **only a step$(S_t,A_t,R_{t+1},S_{t+1})$ to learn**

#### 2.2.1è’™ç‰¹å¡æ´›(Monte Carlo)

Monte Carlo waits until the end of the episode, calculates $G_t$(expected return) and uses it as **a target for updating $V(S_t)$.**So it requires a **complete entire episode of interaction before updating our value function.**

åœ¨ç©å®Œä¸€å±€åæ‰æ€»ç»“å­¦ä¹ ã€‚

- å…¬å¼ï¼š
  $$
  V(S_t)\leftarrow V(S_t)+\alpha[G_t-V(S_t)]
  $$

  - $V(S_t)$: value of state t

    - å·¦ä¾§çš„é‚£ä¸ªæ˜¯ï¼šæ–°çš„çŠ¶æ€ä»·å€¼
    - å³ä¾§çš„2ä¸ªæ˜¯ï¼šä¹‹å‰ä¼°è®¡çš„estimateçš„çŠ¶æ€ä»·å€¼

  - $G_t$: sum of the total rewards at timestep

- æµç¨‹ï¼š

  1. åœ¨ä¸€ä¸ªepisodeå…³å¡ç»“æŸåï¼Œæˆ‘ä»¬å¾—åˆ°è¿™ä¸€å±€ä¸€ç³»åˆ—çš„state,action,rewardå’Œnew state
  2. agentå°†æ‰€æœ‰rewardç›¸åŠ ï¼Œå¾—åˆ°$G_t$
  3. åˆ©ç”¨ä¸Šé¢çš„å…¬å¼æ›´æ–°ä»·å€¼å‡½æ•°$V(S_t)$
  4. ç”¨æ–°å­¦åˆ°çš„çŸ¥è¯†å¼€å§‹æ–°ä¸€è½®çš„æ¸¸æˆã€‚

#### 2.2.2æ—¶é—´å·®åˆ†(Temporal Difference)

Temporal difference, on the other hand, waits for only one interaction (one step)$S_{t+1}$ to form a TD target and update $V(S_t)$ using $R_{t+1}$ and $\gamma V(S_{t+1})$ã€‚

æ¯èµ°ä¸€æ­¥å­¦ä¹ ä¸€æ¬¡ã€‚

- å…¬å¼ï¼š
  $$
  V(S_t) \leftarrow V(S_t) + \alpha[\underbrace{\underbrace{R_{t+1}+\gamma V(S_{t+1})}_{TD\ Target}-V(S_t)}_{TD\ Error}]
  $$

  - $V(S_t)$: value of state 
    - å·¦ä¾§çš„é‚£ä¸ªæ˜¯ï¼šæ–°çš„çŠ¶æ€ä»·å€¼
    - å³ä¾§çš„2ä¸ªæ˜¯ï¼šä¹‹å‰ä¼°è®¡çš„estimateçš„çŠ¶æ€ä»·å€¼
  - $V(S_{t+1})$: value of next state
  - $\gamma$: discount
  - $R_{t+1}$: reward

- æµç¨‹

  1. æ¯ä¸€æ­¥ç»“æŸåï¼Œæˆ‘ä»¬ä¼šå¾—åˆ°state,action,rewardå’Œnew state

  2. å°†$V(S_{t+1})$å’Œ$R_{t+1}$ç”¨äºestimate $G_t$,å¹¶åˆ©ç”¨ä¸Šå¼æ›´æ–°æ–°çš„ä»·å€¼å‡½æ•°$V(S_t)$

     

### 2.3 [Q-Learning](https://huggingface.co/blog/deep-rl-q-part2#what-is-q-learning)æ˜¯ä»€ä¹ˆ

- Q-Learning is **the algorithm we use to train our Q-Function**, an action-value function that determines the value of being at a particular state and taking a specific action at that state.

- Q-Learningæ˜¯ä¸€ä¸ªoff-policy, value-basedå‡½æ•°å¹¶ä½¿ç”¨Temporal Differenceæ–¹æ³•æ¥è®­ç»ƒå®ƒçš„action-value function.

- Q-Learningæ˜¯ä¸€ä¸ªç”¨æ¥è®­ç»ƒQ-functionçš„ç®—æ³•ã€‚

  ![](https://github.com/Fernweh-yang/Reading-Notes/blob/main/%E7%AC%94%E8%AE%B0%E9%85%8D%E5%A5%97%E5%9B%BE%E7%89%87/Reinforcement%20Learning/Q-function.jpg?raw=true)

  - Q-functionæ˜¯ä¸€ä¸ªaction-value functionã€‚å®ƒdetermineäº†åœ¨ä¸€ä¸ªparticular stateå’Œé‡‡å–specific actionæ—¶çš„valueã€‚
    - If we have an optimal Q-function, we have an optimal policy since we know for each state what is the best action to take.
  - Q-tableè®°å½•äº†æ‰€æœ‰çš„state-action pair values ,æ˜¯Q-functionçš„memoryã€‚
    - åœ¨ç»™äºˆäº†ä¸€ä¸ªactionå’ŒstateåQ-functionä¼šæœç´¢è¿™ä¸ªQ-tableå¹¶è¾“å‡ºä¸€ä¸ªå€¼ã€‚
    - ä¸€å¼€å§‹Q-tableé€šå¸¸ä¼šå…¨éƒ½åˆå§‹ä¸º0ï¼Œåœ¨exploreç¯å¢ƒæ—¶æ›´æ–°Q-table

### 2.4 [Q-Learningç®—æ³•](https://huggingface.co/blog/deep-rl-q-part2#the-q-learning-algorithm)

![](https://github.com/Fernweh-yang/Reading-Notes/blob/main/%E7%AC%94%E8%AE%B0%E9%85%8D%E5%A5%97%E5%9B%BE%E7%89%87/Reinforcement%20Learning/Q-learning-Algorithm.jpg?raw=true)

1. ç¬¬ä¸€æ­¥ï¼šåˆå§‹åŒ–Q-Tableçš„æ¯ä¸€ä¸ªstate-action pairä¸º0

2. ç¬¬äºŒæ­¥ï¼šä½¿ç”¨**Epsilon Greedy Strategy**æ¥é€‰æ‹©action

   Epsilon Greedy Strategy is a policy that handles the exploration/exploitation trade-off.

   - å®šä¹‰ä¸€ä¸ªæ¢ç´¢ç‡epsilon $\epsilon=1.0$,å³éšæœºæ‰§è¡ŒæŸä¸€ä¸ªactionçš„æ¦‚ç‡ï¼ˆdo explorationï¼‰ã€‚åˆšå¼€å§‹å­¦ä¹ æ—¶ï¼Œè¿™ä¸ªé€Ÿç‡å¿…é¡»æ˜¯æœ€é«˜å€¼ï¼Œå› ä¸ºæˆ‘ä»¬å¯¹Q-tableçš„å–å€¼ä¸€æ— æ‰€çŸ¥ã€‚è¿™æ„å‘³ç€æˆ‘ä»¬éœ€è¦é€šè¿‡éšæœºé€‰æ‹©æˆ‘ä»¬çš„è¡ŒåŠ¨è¿›è¡Œå¤§é‡æ¢ç´¢ã€‚
   - $1-\epsilon$åˆ™æ˜¯å¼€å‘exploitationçš„æ¦‚ç‡ï¼Œæ ¹æ®å½“å‰çš„Q-tableä¿¡æ¯é€‰æ‹© action with the highest state-action pair value

3. ç¬¬ä¸‰æ­¥ï¼šperformæ‰§è¡Œaction $A_t$  å¾—åˆ°reward $R_{t+1}$å’Œ ä¸‹ä¸€ä¸ªstate$S_{t+1}$

4. ç¬¬å››æ­¥ï¼šæ›´æ–°state-action pair $Q(S_t,A_t)$

   - å› ä¸ºQ-Learningæ˜¯æ—¶é—´å·®åˆ†ç®—æ³•TDï¼Œæ‰€ä»¥æ ¹æ®2.2.2,ç”¨å¦‚ä¸‹å…¬å¼æ›´æ–°
     $$
     Q(S_t,A_t) \leftarrow Q(S_t,A_t) + \alpha[\underbrace{\underbrace{R_{t+1}+\gamma\ \underset{a}{max} Q(S_{t+1},a)}_{TD\ Target}-Q(S_t,A_t)}_{TD\ Error}]\\
     $$

   - $\underset{a}{max} Q(S_{t+1},a)$: è¿™é‡Œæ›´æ–°ç®—æ³•æ—¶ï¼Œæˆ‘ä»¬æ€»æ˜¯é€‰æ‹©å¸¦æ¥highest state-action valueçš„åŠ¨ä½œ, æ‰€ä»¥ç”¨åˆ°çš„æ˜¯**greedy policy**è€Œä¸æ˜¯ç¬¬äºŒæ­¥ä¸­ç”¨åˆ°çš„ epsilon greedy policyã€‚å› ä¸ºepsilon greeedy policyåªæœ‰åœ¨ä¸€ä¸ªéšæœºæ•°å¤§äº$\epsilon$æ—¶æ‰åŸåˆ™æœ€å¤§å€¼ã€‚

### 2.5 å¯¹æ¯”ï¼šoff-policyå’Œon-policy

- off-policy:using **a different policy for acting and updating.**
  - Q-Learningä¸­ç”¨greedy policyæ¥æ›´æ–°ï¼Œç”¨epsilon greedy policyæ¥è¡ŒåŠ¨
- on-policy:using the **same policy for acting and updating.**
  - å¦ä¸€ä¸ªvalue-basedçš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼šsarsa,å°±æ˜¯æ›´æ–°å’Œè¡ŒåŠ¨éƒ½ä½¿ç”¨epsilon greedy policy





## 3. Deep Q-learning

### 3.1 ä¸Q-learningçš„å¯¹æ¯”

- ä¼ ç»Ÿçš„Q-learningåœ¨éœ€è¦å·¨å¤§çš„state spaceé—®é¢˜æ—¶ï¼Œéœ€è¦èŠ±å¤§é‡æ—¶é—´ç”Ÿæˆå’Œæ›´æ–°Q-tableï¼Œæ‰€ä»¥ä¼ ç»Ÿçš„Q-Learningå˜å¾—ineffectiveã€‚ä¸ºäº†è§£å†³å¤§state spaceçš„é—®é¢˜ï¼ŒDeep Q-learningä¸ä¼šä½¿ç”¨Q-tableï¼Œè€Œæ˜¯ç”±ä¸€ä¸ªç¥ç»ç½‘ç»œè·å–çŠ¶æ€å¹¶æ ¹æ®çŠ¶æ€ä¸ºæ¯ä¸€ä¸ªåŠ¨ä½œè¿‘ä¼¼ä¸€ä¸ªQ-valueã€‚
- Deep Q-Learning **uses a deep neural network to approximate the different Q-values for each possible action at a state** (value-function estimation).

![](https://github.com/Fernweh-yang/Reading-Notes/blob/main/%E7%AC%94%E8%AE%B0%E9%85%8D%E5%A5%97%E5%9B%BE%E7%89%87/Reinforcement%20Learning/deep%20q%20and%20q%20learning.jpg?raw=true)



### 3.2 Deep Q-Network(DQN)

![](https://github.com/Fernweh-yang/Reading-Notes/blob/main/%E7%AC%94%E8%AE%B0%E9%85%8D%E5%A5%97%E5%9B%BE%E7%89%87/Reinforcement%20Learning/deep-q-network.jpg?raw=true)

- **Preprocess the input** is an essential step since we want to reduce the complexity of our state.
  - æ¯”å¦‚ä¸Šé¢è¿™ä¸ªåƒç´ æ¸¸æˆï¼Œ
    - å¯ä»¥grayscaleå°†3é€šé“rgbè½¬ä¸º1ä¸ªé€šé“ï¼Œå› ä¸ºé¢œè‰²åœ¨è¿™ä¸ªæ¸¸æˆç¯å¢ƒä¸­ä¸é‡è¦ã€‚
    - è¿˜å¯ä»¥å°†åˆ†è¾¨ç‡ä»160X210å‹ç¼©åˆ°84X84.
    - è¿˜å¯ä»¥cropä¸€éƒ¨åˆ†æ¸¸æˆå±å¹•ï¼Œå› ä¸ºä¸æ˜¯æ‰€æœ‰åœ°æ–¹éƒ½æ˜¯æœ‰ç”¨ä¿¡æ¯

- **temporal limitation**æ—¶é—´é™åˆ¶
  - åªæœ‰1frameå¸§å›¾(ä¸€ä¸ªæ—¶åˆ»)ï¼Œæˆ‘ä»¬ä¸çŸ¥é“å°çƒçš„è¿åŠ¨æ–¹å‘ï¼Œä½†å¦‚æœå°†å¤šå¸§æ”¾åœ¨ä¸€èµ·çœ‹ï¼Œæˆ‘ä»¬å°±å¯ä»¥çŸ¥é“ï¼šå•Šï¼Œå°çƒåœ¨å‘å³ä¸Šæ–¹é£ã€‚
  - æ‰€ä»¥ä¸ºäº†å¾—åˆ°temporal informationæ—¶é—´ä¿¡æ¯ï¼Œwe stack four frames together.
- stacked framesä¼šè¢«3ä¸ªconvolutional layerså¤„ç†
  - These layers **allow us to capture and exploit spatial relationships in images**
  - Because frames are stacked together, **you can exploit some spatial properties across those frames**.

- æœ€åç»è¿‡å…¨è¿æ¥å±‚ï¼Œå¾—åˆ°æŸ1ä¸ªstateä¸‹æ¯ä¸€ä¸ªå¯èƒ½åŠ¨ä½œçš„Q-value

### 3.3 Deep Q-Learning Algorithm

ç›¸æ¯”äº2.4Q-Learningç®—æ³•çš„ç¬¬å››æ­¥ï¼ŒDeep Q-learningä½¿ç”¨ä¸€ä¸ªLoss functionæ¯”è¾ƒ predicted Q-valueå’ŒQ-targetã€‚ç„¶åç”¨gradient descentæ¥æ›´æ–°Q-Networksçš„æƒé‡ï¼Œå¦‚æ­¤åå¤å¾—ä»¥æ›´å¥½çš„è¿‘ä¼¼Q-values

Q-Target: 
$$
y_j=r_j+\gamma max_{a'}\hat{Q}(\phi_{j+1},a';\theta^-)
$$
Q-Loss:
$$
y_j-Q(\phi_j,a_j;\theta)
$$
å¯¹æ¯”æ™®é€šQ-learningçš„ td-target(q-target)å’Œ td-error(q-loss)
$$
Q(S_t,A_t) \leftarrow Q(S_t,A_t) + \alpha[\underbrace{\underbrace{R_{t+1}+\gamma\ \underset{a}{max} Q(S_{t+1},a)}_{TD\ Target}-Q(S_t,A_t)}_{TD\ Error}]\\
$$


Deep Q-Learningç®—æ³•æœ‰2ä¸ªæ­¥éª¤

- **Sampling**: we perform actions and **store the observed experiences tupleså…ƒç»„ in a replay memory**.
- **Training**: Select the **small batch of tuple randomly and learn from it using a gradient descent update step**.

![](https://github.com/Fernweh-yang/Reading-Notes/blob/main/%E7%AC%94%E8%AE%B0%E9%85%8D%E5%A5%97%E5%9B%BE%E7%89%87/Reinforcement%20Learning/deep%20q%20algorithm.jpg?raw=true)

ç”±äºå¼•è¿›äº† Neural Network,ç›¸æ¯”Q-Learningæ›´ä¸ç¨³å®šï¼Œæ‰€ä»¥éœ€è¦åŠ å…¥å¦‚ä¸‹3ç§æªæ–½

1. *Experience Replay*, to make more **efficient use of experiences**.
2. *Fixed Q-Target* **to stabilize the training**.
3. *Double Deep Q-Learning*, to **handle the problem of the overestimationé«˜ä¼° of Q-values**.

![](https://github.com/Fernweh-yang/Reading-Notes/blob/main/%E7%AC%94%E8%AE%B0%E9%85%8D%E5%A5%97%E5%9B%BE%E7%89%87/Reinforcement%20Learning/stable%20deep%20q.jpg?raw=true)

#### 3.3.1 Experience Replay

experience replay:åˆå§‹åŒ–ç¬¬ä¸€è¡Œï¼Œsamplingçš„æœ€å1è¡Œ å’Œ trainingçš„ç¬¬ä¸€è¡Œ 

åœ¨Deep Q-learningä¸­experience replayæœ‰2ä¸ªåŠŸèƒ½

1. **Make more efficient use of the experiences during the training**.

   - Usually, in online reinforcement learning, we interact in the environment, get experiences (state, action, reward, and next state), learn from them (update the neural network) and discard them.
   - But with experience replay, we create a replay bufferç¼“å†² D that saves experience samples **that we can reuse during the training.**=ã€‹This allows us to **learn from individual experiences multiple times**.

2. **Avoid forgetting previous experiences and reduce the correlationç›¸å…³æ€§ between experiences**.

   - The problem we get if we give sequential samples of experiences to our neural network is that it tends to forget **the previous experiences as it overwrites new experiences.** For instance, if we are in the first level and then the second, which is different, our agent can forget how to behave and play in the first level.

   - This prevents **the network from only learning about what it has immediately done.**

   - ä¹Ÿé¿å…äº†action values oscillateéœ‡è¡å’Œdivergeå‘æ•£

     

#### 3.3.2 Fixed Q-Target

- å½“æˆ‘ä»¬è®¡ç®—TD Error(Q-loss)æ—¶,æˆ‘ä»¬è®¡ç®—**TD target (Q-Target)å’Œcurrent Q-value (estimation of Q)**çš„åŒºåˆ«ã€‚

  ä½†æˆ‘ä»¬ä¸çŸ¥é“çœŸæ­£çš„TD Targetæ˜¯ä»€ä¹ˆï¼Œæ‰€ä»¥æˆ‘ä»¬ä¹Ÿéœ€è¦ç”¨å¦‚ä¸‹å…¬å¼æ›´æ–°TD Targetï¼š
  $$
  y_j=r_j+\gamma max_{a'}\hat{Q}(\phi_{j+1},a';\theta^-)
  $$

- ä½†æˆ‘ä»¬åœ¨æ›´æ–°TD Targetå’ŒQ valueæ—¶ï¼Œç”¨çš„æ˜¯åŒä¸€ç»„parameterã€‚è¿™å°±å¯¼è‡´åœ¨è®­ç»ƒçš„æ¯ä¸€æ­¥ï¼Œæˆ‘ä»¬çš„Q-Valueåœ¨shifçš„åŒæ—¶ï¼ŒQ-targetä¹Ÿåœ¨shiftã€‚å°±åƒæ˜¯åœ¨è¿½é€ä¸€ä¸ªä¼šåŠ¨çš„ç›®æ ‡ã€‚

  è¿™ä¼šå¯¼è‡´significant oscillationéœ‡è¡ in training.

- ä¸ºäº†è§£å†³ä¸Šè¿°é—®é¢˜ï¼Œå°±éœ€è¦fixed Q-target:

  - Use a **separate network with a fixed parameter** for estimating the TD Target
  - **Copy the parameters from our Deep Q-Network at every C step** to update the target network.

#### 3.3.3 Double DQN

Double DQNs, or Double Learning ç”±[Hado van Hasselt](https://papers.nips.cc/paper/3964-double-q-learning)æå‡ºï¼Œè§£å†³äº†**overestimationé«˜ä¼° of Q-values**çš„é—®é¢˜ã€‚

- æˆ‘ä»¬åœ¨æ›´æ–°TD targetæ—¶ï¼Œè¦å¦‚ä½•ç¡®è®¤æˆ‘ä»¬é€‰æ‹©äº†**the best action for the next state is the action with the highest Q-value**ï¼Ÿ
  - The accuracy of Q values depends on what action we tried **and** what neighboring states we explored.
  - å› ä¸ºä¸€å¼€å§‹æˆ‘ä»¬æ²¡æœ‰è¶³å¤Ÿå¾—åˆ†exploreç¯å¢ƒï¼Œæ‰€ä»¥æˆ‘ä»¬é€‰æ‹©æœ€å¤§Q Valueæ—¶ï¼Œå¯èƒ½æ˜¯ä¸å‡†ç¡®çš„(noisy)ï¼Œä»è€Œå¯¼è‡´false positivesã€‚
  - å¦‚æœnon-optimal actions are regularly **given a higher Q value than the optimal best action, the learning will be complicated.**
- ä¸ºäº†è§£å†³ä¸Šè¿°é—®é¢˜ï¼Œéœ€è¦ä½¿ç”¨2ä¸ªç¥ç»ç½‘ç»œæ¥è®© `åŠ¨ä½œactionçš„é€‰æ‹©` å’Œ `target Q valueçš„ç”Ÿæˆ` è§£è€¦decouple:
  - Use our **DQN network** to select the best action to take for the next state (the action with the highest Q value).
  - Use our **Target network** to calculate the target Q value of taking that action at the next state.



## 4. Unity MLAgents

- [Unity ML-Agents toolkit](https://github.com/Unity-Technologies/ml-agents) is a plugin based on the game engine Unity that allows us to use the **Unity Game Engine as an environment builder to train agents.**

  [Unity ML-Agents Toolkit with hugging face](https://github.com/huggingface/ml-agents) å¯ä»¥è®©æˆ‘ä»¬ä¸ç”¨ä¸‹è½½unityç›´æ¥ä½¿ç”¨mlagent

### 4.1 Four components

Unity ML-Agents has four essential components.

1. **Learning environment**:

    **contains the Unity scene (the environment) and the environment elements** (game characters)**.**

2. **Python API**

   contains the **low-level Python interface** for **interacting and manipulating the environment**. Itâ€™s the API we use to launch the training.

3. **communicator**

   connects the environment (C#) with the Python API (Python).

4. **Python trainers**

   the RL algorithms made with PyTorch (PPO, SACâ€¦).

### 4.2 Inside the Learning Component

There are three important elements

1. Agent:

   the actor of the scene

2. Brain:

   the policy we optimize to train the agent

3. Academy:

   - This element **orchestratesç­–åˆ’ agents and their decision-making process.** 

   - å¯ä»¥æŠŠacademyæƒ³è±¡æˆä¸€ä¸ªmaestroéŸ³ä¹å¤§å¸ˆï¼Œè´Ÿè´£å¤„ç†æ¥è‡ªpython apiçš„è¯·æ±‚ã€‚æ¯”å¦‚python apiè¯´"we need some observations", academyå°±è®©agentså»æœé›†ä¸€äº›observationsã€‚

   - The Academy **will be the one that will send the order to our Agents and ensure that agents are in sync**åŒæ­¥:

     - Collect Observations

     - Select your action using your policy

     - Take the Action

     - Reset if you reached the max step or if youâ€™re done.

### 4.3 Pyramid Environment

æˆ‘ä»¬è¿™é‡Œç”¨æ¥è®­ç»ƒçš„ç¯å¢ƒ

- The goal in this environment is to train our agent **to get the gold brické‡‘ç – on the top of the Pyramidé‡‘å­—å¡”.** ä¸ºäº†å®ç°è¿™ä¸ªç›®æ ‡ï¼Œæˆ‘ä»¬éœ€è¦ï¼š

  - a button to spawn a pyramid
  - navigate to the Pyramid
  - knock it over
  - move to the gold brick at the top

- reward system:

  - -0.001: for every step

    given this negative reward for every step, will push our agent to go faster

  - +2 for moving to golden brick

  - ä¸ºäº†è®­ç»ƒagentèƒ½seeks that button and then the Pyramid to destroyï¼Œè¿™é‡Œä½¿ç”¨2ç§Rewards

    1. The *extrinsicå¤–åœ¨ one* given by the environment.
    2. But also an *intrinsicå†…åœ¨ one* called *curiosity*. **This second will push our agent to be curious, or in other terms, to better explore its environment.** å…³äºDeep RLä¸­çš„curiosityè§å‰¯2

- Observation:

  - Instead of normal vision(frame), we use 148 raycastså…‰çº¿æŠ•å°„ that can each detect objects (switch, bricks, golden brick, and walls.)
  - a **boolean variable indicatingè¡¨æ˜ the switch state** (did we turn on or not the switch to spawn the Pyramid) 
  -  a vector that **contains agentâ€™ speed.**

- Action space:

  - Forward Motion: Up/Down
  - Rotation: Rotate left / Rotate right

## 5. Policy Gradient with Pytorch



# äºŒã€Stable-baseline3å®ç°ä¸€ã€çš„ä»£ç 

## 1. Foundation:

### 1.1 gym çš„åŸºæœ¬ç”¨æ³•ï¼š

```python
import gym

# First, we create our environment called LunarLander-v2
env = gym.make("LunarLander-v2")

# Then we reset this environment
observation = env.reset()

for _ in range(20):
  # Take a random action
  action = env.action_space.sample()
  print("Action taken:", action)

  # Do this action in the environment and get
  # next_state, reward, done and info
  observation, reward, done, info = env.step(action)
  
  # If the game is done (in our case we land, crashed or timeout)
  if done:
      # Reset the environment
      print("Environment is reset")
      observation = env.reset()
```

### 1.2 Stable-Baselines3çš„åŸºæœ¬ä½¿ç”¨æ­¥éª¤

```python
# Create environment
env = gym.make('LunarLander-v2')

# Instantiate the agent
model = PPO('MlpPolicy', env, verbose=1)
# Train the agent
model.learn(total_timesteps=int(2e5))
```



### 1.3 ä¸€ä¸ªè‡ªåŠ¨é™è½çš„æ™ºèƒ½é£æœº

è¿è¡Œåœ¨colabï¼šhttps://colab.research.google.com/github/huggingface/deep-rl-class/blob/main/unit1/unit1.ipynb

#### 1.3.1 å®‰è£…ç”¨äºcolabè™šæ‹Ÿç•Œé¢æ˜¾ç¤º

  ```python
  !sudo apt-get update
  !apt install python-opengl
  !apt install ffmpeg
  !apt install xvfb
  !pip3 install pyvirtualdisplay
  
  # Virtual display
  from pyvirtualdisplay import Display
  
  virtual_display = Display(visible=0, size=(1400, 900))
  virtual_display.start()
  ```

#### 1.3.2 **å®‰è£…æ·±åº¦å­¦ä¹ åº“**

  ```python
  !pip install importlib-metadata==4.12.0 # To overcome an issue with importlib-metadata https://stackoverflow.com/questions/73929564/entrypoints-object-has-no-attribute-get-digital-ocean
  !pip install gym[box2d]
  !pip install stable-baselines3[extra]
  !pip install huggingface_sb3
  !pip install pyglet==1.5.1
  !pip install ale-py==0.7.4 # To overcome an issue with gym (https://github.com/DLR-RM/stable-baselines3/issues/875)
  
  !pip install pickle5
  ```

#### 1.3.3 **å¯¼å…¥åŒ…**

  ```python
  import gym
  
  from huggingface_sb3 import load_from_hub, package_to_hub, push_to_hub
  from huggingface_hub import notebook_login # To log to our Hugging Face account to be able to upload models to the Hub.
  
  from stable_baselines3 import PPO
  from stable_baselines3 import DQN	# æ‰©å±•éƒ¨åˆ†ï¼šå°è¯•ä½¿ç”¨å¦ä¸ªç®—æ³•DQN
  from stable_baselines3.common.evaluation import evaluate_policy
  from stable_baselines3.common.env_util import make_vec_env
  ```

#### 1.3.4 **å»ºç«‹ç¯å¢ƒ**

- ç¯å¢ƒæ–‡æ¡£ï¼šhttps://www.gymlibrary.dev/environments/box2d/lunar_lander/

  - Action Space:
    - do nothing
    - fire left orientation engine
    - fire main engine
    - fire right orientation engine.
  - Observation Space
    - the coordinates of the lander in `x` & `y`
    - its linear velocities in `x` & `y`
    - its angle
    - its angular velocity
    - two booleans that represent whether each leg is in contact with the ground or not.
  - Rewards
    - Moving from the top of the screen to the landing pad and zero speed is about 100~140 points.
    - Firing main engine is -0.3 each frame
    - Each leg ground contact is +10 points
    - Episode finishes if the lander crashes (additional - 100 points) or come to rest (+100 points)

-  å»ºç«‹å¹¶æŸ¥çœ‹ç¯å¢ƒ

```python
# ä½¿ç”¨gymåˆ›å»ºç¯å¢ƒ
env = gym.make("LunarLander-v2")
env.reset()	#å°†ç¯å¢ƒé‡ç½®
print("_____OBSERVATION SPACE_____ \n")
print("Observation Space Shape", env.observation_space.shape)
print("Sample observation", env.observation_space.sample()) # Get a random observation
print("\n _____ACTION SPACE_____ \n")
print("Action Space Shape", env.action_space.n)
print("Action Space Sample", env.action_space.sample()) # Take a random action


# ä½¿ç”¨stable baseline3å‘é‡åŒ–ç¯å¢ƒ
# We create a vectorized environment (method for stacking multiple independent environments into a single environment) of 16 environments, this way, we'll have more diverse experiences during the training.
env = make_vec_env('LunarLander-v2', n_envs=16)
```



#### 1.3.5 **å»ºç«‹RLç®—æ³•æ¨¡å‹**

- ä½¿ç”¨ [Stable Baselines3 (SB3)](https://stable-baselines3.readthedocs.io/en/master/)ï¼ŒSB3 is a set of **reliable implementations of reinforcement learning algorithms in PyTorch**.

##### 1.[PPOç®—æ³•](https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html#example%5D)

```python
# We use MultiLayerPerceptron (MLPPolicy) because the input is a vector,
# if we had frames as input we would use CnnPolicy
model = PPO(
  policy = 'MlpPolicy',
  env = env,
  n_steps = 1024,
  batch_size = 64,
  n_epochs = 4,
  gamma = 0.999,
  gae_lambda = 0.98,
  ent_coef = 0.01,
  verbose=1)
```

##### 2.DQNç®—æ³•

```python
model = DQN(
    policy='MlpPolicy',
    env = env,
    verbose=1,
    batch_size= 64  
)
```



#### 1.3.6 **è®­ç»ƒæ¨¡å‹**

1. PPO:

   ```python
   # Train it for 500,000 timesteps
   model.learn(total_timesteps=500000)
   # Save the model
   model_name = "ppo-LunarLander-v2"
   model.save(model_name)
   ```

   

2. DQN:

   ```python
   # Train it for 500,000 timesteps
   model.learn(total_timesteps=500000)
   # Save the model
   model_name = "dqn-LunarLander-v2"
   model.save(model_name)
   ```

   

#### 1.3.7 **è¯„ä¼°è®­ç»ƒå®Œçš„æ™ºèƒ½ä½“**

```python
eval_env = gym.make("LunarLander-v2")
mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=10, deterministic=True)
print(f"mean_reward={mean_reward:.2f} +/- {std_reward}")
```

#### 1.3.8 **å‘å¸ƒæ¨¡å‹åˆ°Huggingfaceä¸Š**

- [Huggingfaceåº“](https://github.com/huggingface/huggingface_sb3/tree/main#hugging-face--x-stable-baselines3-v20)åŠŸèƒ½ï¼š

  - You can **showcase our work** ğŸ”¥
  - You can **visualize your agent playing** ğŸ‘€
  - You can **share with the community an agent that others can use** ğŸ’¾
  - You can **access a leaderboard ğŸ† to see how well your agent is performing compared to your classmates** ğŸ‘‰

- è¿æ¥ç”µè„‘åˆ°Huggingface

  - Create a new token (https://huggingface.co/settings/tokens) **with write role**

    è¾“å…¥å¦‚ä¸‹å‘½ä»¤åä¼šè¦æ±‚è¾“å…¥token

  ```python
  # å¦‚æœç”¨çš„colabæˆ–è€…jupyter
  notebook_login()
  !git config --global credential.helper store
  # å¦‚æœç”¨çš„æœ¬åœ°pythonç¯å¢ƒ
  huggingface-cli login
  ```

- ä½¿ç”¨package_to_hub()å‡½æ•°æ¥ä¸Šä¼ ä»£ç 

  ```python
  import gym
  from stable_baselines3.common.vec_env import DummyVecEnv
  from stable_baselines3.common.env_util import make_vec_env
  
  from huggingface_sb3 import package_to_hub
  
  ## TODO: Define a repo_id
  ## repo_id is the id of the model repository from the Hugging Face Hub (repo_id = {organization}/{repo_name} for instance ThomasSimonini/ppo-LunarLander-v2
  repo_id = "TUMxudashuai/ppo-LunarLander-v2"
  
  # TODO: Define the name of the environment
  env_id =  "LunarLander-v2"
  
  # Create the evaluation env
  eval_env = DummyVecEnv([lambda: gym.make(env_id)])
  
  
  # TODO: Define the model architecture we used
  model_architecture = "PPO"
  
  ## TODO: Define the commit message
  commit_message = "Upload PPO LunarLander-v2 trained agent"
  
  # method save, evaluate, generate a model card and record a replay video of your agent before pushing the repo to the hub
  package_to_hub(model=model, # Our trained model
                 model_name=model_name, # The name of our trained model 
                 model_architecture=model_architecture, # The model architecture we used: in our case PPO
                 env_id=env_id, # Name of the environment
                 eval_env=eval_env, # Evaluation Environment
                 repo_id=repo_id, # id of the model repository from the Hugging Face Hub (repo_id = {organization}/{repo_name} for instance ThomasSimonini/ppo-LunarLander-v2
                 commit_message=commit_message)
  
  # Note: if after running the package_to_hub function and it gives an issue of rebasing, please run the following code
  # cd <path_to_repo> && git add . && git commit -m "Add message" && git pull 
  # And don't forget to do a "git push" at the end to push the change to the hub.
  ```

  - model here: https://huggingface.co/TUMxudashuai/ppo-LunarLander-v2
    - see a video preview of your agent at the right.
    - click "Files and versions" to see all the files in the repository.
    - click "Use in stable-baselines3" to get a code snippet that shows how to load the model.
    - a model card (`README.md` file) which gives a description of the model

#### æ‰©å±•ï¼š

1. Try different hyperparameters of `PPO`. You can see them at https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html#parameters.
2. Check the [Stable-Baselines3 documentation](https://stable-baselines3.readthedocs.io/en/master/modules/dqn.html) and try another models such as DQN.
3. Try to **change the environment**, why not using CartPole-v1, MountainCar-v0 or CarRacing-v0? Check how they works [using the gym documentation](https://www.gymlibrary.dev/) and have fun

### 1.4 å€’ç«‹æ‘†cartpole

#### 1.4.1 ç¯å¢ƒ

```python
!sudo apt-get update
!apt install python-opengl
!apt install ffmpeg
!apt install xvfb
!pip3 install pyvirtualdisplay

# Virtual display
from pyvirtualdisplay import Display

virtual_display = Display(visible=0, size=(1400, 900))
virtual_display.start()

!pip install importlib-metadata==4.13.0
!pip install gym[classic_control]
!pip install stable-baselines3[extra]
!pip install huggingface_sb3
!pip install pyglet==1.5.1
!pip install ale-py==0.7.4 # To overcome an issue with gym (https://github.com/DLR-RM/stable-baselines3/issues/875)

!pip install pickle5
```

#### 1.4.2 åŠ è½½åº“

```python
import gym

from huggingface_sb3 import load_from_hub, package_to_hub, push_to_hub
from huggingface_hub import notebook_login # To log to our Hugging Face account to be able to upload models to the Hub.

from stable_baselines3 import PPO
from stable_baselines3 import DQN	# æ‰©å±•éƒ¨åˆ†ï¼šå°è¯•ä½¿ç”¨å¦ä¸ªç®—æ³•DQN
from stable_baselines3.common.evaluation import evaluate_policy
from stable_baselines3.common.env_util import make_vec_env
```

#### 1.4.3 åˆ›å»ºç¯å¢ƒ

```python
env = gym.make('CartPole-v1')
env.reset()
print("_____OBSERVATION SPACE_____ \n")
print("Observation Space Shape", env.observation_space.shape)
print("Sample observation", env.observation_space.sample()) # Get a random observation
print("\n _____ACTION SPACE_____ \n")
print("Action Space Shape", env.action_space.n)
print("Action Space Sample", env.action_space.sample()) # Take a random action
env = make_vec_env('CartPole-v1', n_envs=16)
```

#### 1.4.4 åˆ›å»ºæ¨¡å‹

```python
model = PPO(
  policy = 'MlpPolicy',
  env = env,
  n_steps = 1024,
  batch_size = 64,
  n_epochs = 4,
  gamma = 0.999,
  gae_lambda = 0.98,
  ent_coef = 0.01,
  verbose=1)
```



#### 1.4.5 è®­ç»ƒæ¨¡å‹

```
# Train it for 500,000 timesteps
model.learn(total_timesteps=500000)
model_name = "ppo-CartPole-v1"
model.save(model_name)
```



## 2. Q-Learning

ä»£ç åœ°å€ï¼šhttps://colab.research.google.com/github/huggingface/deep-rl-class/blob/main/unit2/unit2.ipynb

### 2.1 Frozen Lake

#### 2.1.1 å®‰è£…ç¯å¢ƒ

```python
!sudo apt-get update%%capture
!pip install pyglet==1.5.1 
!apt install python-opengl
!apt install ffmpeg
!apt install xvfb
!pip3 install pyvirtualdisplay

# Virtual display
from pyvirtualdisplay import Display

virtual_display = Display(visible=0, size=(1400, 900))
virtual_display.start()

%%capture
!pip install gym==0.24 # We install the newest gym version for the Taxi-v3 "rgb_array version"
!pip install pygame
!pip install numpy

!pip install huggingface_hub
!pip install pickle5
!pip install pyyaml==6.0 # avoid key error metadata
!pip install imageio imageio_ffmpeg
```

#### 2.1.2 å¯¼å…¥åŒ…

```python
import numpy as np
import gym
import random
import imageio
import os 

import pickle5 as pickle
from tqdm.notebook import tqdm
```

#### 2.1.3 å»ºç«‹ç¯å¢ƒ

- ç¯å¢ƒä¿¡æ¯ï¼šhttps://www.gymlibrary.dev/environments/toy_text/frozen_lake/
  - ç›®æ ‡æ˜¯ï¼š train our Q-Learning agent **to navigate from the starting state (S) to the goal state (G) by walking only on frozen tiles (F) and avoid holes (H)**.
  - æœ‰ä¸¤ä¸ªå°ºå¯¸çš„åœ°å›¾ï¼š
    - `map_name="4x4"`: a 4x4 grid version
      - æœ‰16ç§å¯èƒ½çš„observations
    - `map_name="8x8"`: a 8x8 grid version
      - æœ‰64ç§å¯èƒ½çš„observations
  - ç¯å¢ƒæœ‰2ç§æ¨¡å¼ï¼š
    - `is_slippery=False`: The agent always move in the intendedæ‰“ç®—çš„ direction due to the non-slippery nature of the frozen lake.
    - `is_slippery=True`: The agent may not always move in the intended direction due to the slippery nature of the frozen lake (stochasticéšæœºçš„).
  - æœ‰å››ç§å¯èƒ½çš„è¡ŒåŠ¨ï¼Œaction space
    - 0: GO LEFT
    - 1: GO DOWN
    - 2: GO RIGHT
    - 3: GO UP
  - Reward function
    - Reach goal: +1
    - Reach hole: 0
    - Reach frozen: 0

- ä»£ç 

  ```python
  env = gym.make("FrozenLake-v1", map_name="4x4", is_slippery=False)
  env.reset()
  print("_____OBSERVATION SPACE_____ \n")
  print("Observation Space", env.observation_space)
  print("Sample observation", env.observation_space.sample()) # Get a random observation
  print("\n _____ACTION SPACE_____ \n")
  print("Action Space Shape", env.action_space.n)
  print("Action Space Sample", env.action_space.sample()) # Take a random action
  ```

#### 2.1.4 åˆ›å»ºå¹¶åˆå§‹åŒ–Q-table

ç”±ä¸€ã€2.4çš„ç¬¬ä¸€æ­¥å¯çŸ¥

Q-tableæ˜¯è¡Œrowä¸ºstateï¼Œåˆ—columnä¸ºactionçš„ä¸€ä¸ªè¡¨æ ¼

```python
# openAI gym æä¾›å¦‚ä¸‹ä¸¤ä¸ªå‚æ•°å‘ŠçŸ¥çŠ¶æ€ç©ºé—´å’ŒåŠ¨ä½œç©ºé—´çš„ç»´åº¦
state_space = env.observation_space.n
print("There are ", state_space, " possible states")
action_space = env.action_space.n
print("There are ", action_space, " possible actions")

# å°†Q-tableåˆå§‹åŒ–ä¸º0
def initialize_q_table(state_space, action_space):
  Qtable = np.zeros((state_space, action_space))
  return Qtable
Qtable_frozenlake = initialize_q_table(state_space, action_space)
```



#### 2.1.5 å®šä¹‰epsilon-greedy policy

ç”±ä¸€ã€2.4çš„ç¬¬äºŒæ­¥,ç¬¬ä¸‰æ­¥å¯çŸ¥

```python
def epsilon_greedy_policy(Qtable, state, epsilon):
  # Randomly generate a number between 0 and 1
  random_int = random.uniform(0,1)
  # if random_int > greater than epsilon --> exploitation
  if random_int > epsilon:
    # Take the action with the highest value given a state
    # np.maxï¼šæ¥å—ä¸€ä¸ªå‚æ•°ï¼Œè¿”å›æ•°ç»„ä¸­çš„æœ€å¤§å€¼ï¼›
    # np.argmaxï¼šæ¥å—ä¸€ä¸ªå‚æ•°ï¼Œè¿”å›æ•°ç»„ä¸­æœ€å¤§å€¼å¯¹åº”çš„ç´¢å¼•ï¼›
    action = np.argmax(Qtable[state])
  # else --> exploration
  else:
    # ä¸‹é¢è¿™ä¸ªå‡½æ•°å¯ä»¥Sample a random action from the entire action space 
    action = env.action_space.sample()
  
  return action
```

#### 2.1.6 å®šä¹‰greedy policy

ç”±ä¸€ã€2.4çš„ç¬¬å››æ­¥å¯çŸ¥

```python
def greedy_policy(Qtable, state):
  # Exploitation: take the action with the highest state, action value
  action = np.argmax(Qtable[state])
  
  return action
```

#### 2.1.7 å®šä¹‰hyperparameters

å®šä¹‰2.4ä¸­å…¬å¼ä»¥åŠè®­ç»ƒçš„å‚æ•°

```python
# Training parameters
n_training_episodes = 10000  # Total training episodes
learning_rate = 0.7          # Learning rate

# Evaluation parameters
n_eval_episodes = 100        # Total number of test episodes

# Environment parameters
env_id = "FrozenLake-v1"     # Name of the environment
max_steps = 99               # Max steps per episode
gamma = 0.95                 # Discounting rate
eval_seed = []               # The evaluation seed of the environment

# Exploration parameters
max_epsilon = 1.0             # Exploration probability at start
min_epsilon = 0.05            # Minimum exploration probability 
decay_rate = 0.0005            # Exponential decay rate for exploration prob
```



#### 2.1.8 å»ºç«‹è®­ç»ƒå¾ªç¯

```python
def train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable):
  for episode in tqdm(range(n_training_episodes)):
    # Reduce epsilon (because we need less and less exploration)
    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)
    # Reset the environment
    state = env.reset()
    step = 0
    done = False

    # repeat
    for step in range(max_steps):
      # Choose the action At using epsilon greedy policy
      action = epsilon_greedy_policy(Qtable, state, epsilon)

      # Take action At and observe Rt+1 and St+1
      # Take the action (a) and observe the outcome state(s') and reward (r)
      new_state, reward, done, info = env.step(action)

      # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]
      Qtable[state][action] = Qtable[state][action] + learning_rate * (reward + gamma * np.max(Qtable[new_state]) - Qtable[state][action])   

      # If done, finish the episode
      if done:
        break
      
      # Our state is the new state
      state = new_state
  return Qtable
```



#### 2.1.9 è®­ç»ƒæ¨¡å‹

```python
# è®­ç»ƒæ¨¡å‹
Qtable_frozenlake = train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable_frozenlake)

# æŸ¥çœ‹è®­ç»ƒå®Œåçš„Q-table
Qtable_frozenlake
```



#### 2.1.10 è¯„ä¼°æ¨¡å‹

```python
def evaluate_agent(env, max_steps, n_eval_episodes, Q, seed):
  """
  Evaluate the agent for ``n_eval_episodes`` episodes and returns average reward and std of reward.
  :param env: The evaluation environment
  :param n_eval_episodes: Number of episode to evaluate the agent
  :param Q: The Q-table
  :param seed: The evaluation seed array (for taxi-v3)
  """
  episode_rewards = []
  for episode in tqdm(range(n_eval_episodes)):
    if seed:
      state = env.reset(seed=seed[episode])
    else:
      state = env.reset()
    step = 0
    done = False
    total_rewards_ep = 0
    
    for step in range(max_steps):
      # Take the action (index) that have the maximum expected future reward given that state
      action = np.argmax(Q[state][:])
      new_state, reward, done, info = env.step(action)
      total_rewards_ep += reward
        
      if done:
        break
      state = new_state
    episode_rewards.append(total_rewards_ep)
  mean_reward = np.mean(episode_rewards)
  std_reward = np.std(episode_rewards)

  return mean_reward, std_reward

mean_reward, std_reward = evaluate_agent(env, max_steps, n_eval_episodes, Qtable_frozenlake, eval_seed)
print(f"Mean_reward={mean_reward:.2f} +/- {std_reward:.2f}")
```

#### 2.1.11 å‘å¸ƒæ¨¡å‹åˆ°Huggingface

1. ä¸ä¿®æ”¹ä¸‹é¢çš„ä»£ç ï¼Œç”¨äºä¸Šä¼ 

```python
%%capture
from huggingface_hub import HfApi, HfFolder, Repository
from huggingface_hub.repocard import metadata_eval_result, metadata_save

from pathlib import Path
import datetime
import json

#-------------------------------------------------------------------------------
def record_video(env, Qtable, out_directory, fps=1):
  images = []  
  done = False
  state = env.reset(seed=random.randint(0,500))
  img = env.render(mode='rgb_array')
  images.append(img)
  while not done:
    # Take the action (index) that have the maximum expected future reward given that state
    action = np.argmax(Qtable[state][:])
    state, reward, done, info = env.step(action) # We directly put next_state = state for recording logic
    img = env.render(mode='rgb_array')
    images.append(img)
  imageio.mimsave(out_directory, [np.array(img) for i, img in enumerate(images)], fps=fps)

#-------------------------------------------------------------------------------
def push_to_hub(repo_id, 
                model,
                env,
                video_fps=1,
                local_repo_path="hub",
                commit_message="Push Q-Learning agent to Hub",
                token= None
                ):
  _, repo_name = repo_id.split("/")

  eval_env = env
  
  # Step 1: Clone or create the repo
  # Create the repo (or clone its content if it's nonempty)
  api = HfApi()
  
  repo_url = api.create_repo(
        repo_id=repo_id,
        token=token,
        private=False,
        exist_ok=True,)
  
  # Git pull
  repo_local_path = Path(local_repo_path) / repo_name
  repo = Repository(repo_local_path, clone_from=repo_url, use_auth_token=True)
  repo.git_pull()
  
  repo.lfs_track(["*.mp4"])

  # Step 1: Save the model
  if env.spec.kwargs.get("map_name"):
    model["map_name"] = env.spec.kwargs.get("map_name")
    if env.spec.kwargs.get("is_slippery", "") == False:
      model["slippery"] = False

  print(model)
  
    
  # Pickle the model
  with open(Path(repo_local_path)/'q-learning.pkl', 'wb') as f:
    pickle.dump(model, f)
  
  # Step 2: Evaluate the model and build JSON
  mean_reward, std_reward = evaluate_agent(eval_env, model["max_steps"], model["n_eval_episodes"], model["qtable"], model["eval_seed"])

  # First get datetime
  eval_datetime = datetime.datetime.now()
  eval_form_datetime = eval_datetime.isoformat()

  evaluate_data = {
        "env_id": model["env_id"], 
        "mean_reward": mean_reward,
        "n_eval_episodes": model["n_eval_episodes"],
        "eval_datetime": eval_form_datetime,
  }
  # Write a JSON file
  with open(Path(repo_local_path) / "results.json", "w") as outfile:
      json.dump(evaluate_data, outfile)

  # Step 3: Create the model card
  # Env id
  env_name = model["env_id"]
  if env.spec.kwargs.get("map_name"):
    env_name += "-" + env.spec.kwargs.get("map_name")

  if env.spec.kwargs.get("is_slippery", "") == False:
    env_name += "-" + "no_slippery"

  metadata = {}
  metadata["tags"] = [
        env_name,
        "q-learning",
        "reinforcement-learning",
        "custom-implementation"
    ]

  # Add metrics
  eval = metadata_eval_result(
      model_pretty_name=repo_name,
      task_pretty_name="reinforcement-learning",
      task_id="reinforcement-learning",
      metrics_pretty_name="mean_reward",
      metrics_id="mean_reward",
      metrics_value=f"{mean_reward:.2f} +/- {std_reward:.2f}",
      dataset_pretty_name=env_name,
      dataset_id=env_name,
    )

  # Merges both dictionaries
  metadata = {**metadata, **eval}

  model_card = f"""
  # **Q-Learning** Agent playing **{env_id}**
  This is a trained model of a **Q-Learning** agent playing **{env_id}** .
  """

  model_card += """
  ## Usage
  """

  model_card += f"""model = load_from_hub(repo_id="{repo_id}", filename="q-learning.pkl")

  # Don't forget to check if you need to add additional attributes (is_slippery=False etc)
  env = gym.make(model["env_id"])

  evaluate_agent(env, model["max_steps"], model["n_eval_episodes"], model["qtable"], model["eval_seed"])
  """

  model_card +=""" """

  readme_path = repo_local_path / "README.md"
  readme = ""
  if readme_path.exists():
      with readme_path.open("r", encoding="utf8") as f:
        readme = f.read()
  else:
    readme = model_card

  with readme_path.open("w", encoding="utf-8") as f:
    f.write(readme)

  # Save our metrics to Readme metadata
  metadata_save(readme_path, metadata)

  # Step 4: Record a video
  video_path =  repo_local_path / "replay.mp4"
  record_video(env, model["qtable"], video_path, video_fps)

  # Push everything to hub
  print(f"Pushing repo {repo_name} to the Hugging Face Hub")
  repo.push_to_hub(commit_message=commit_message)

  print(f"Your model is pushed to the hub. You can view your model here: {repo_url}")
```

2. ä¸Šä¼ åˆ°è‡ªå·±çš„è´¦å·

   https://huggingface.co/settings/tokens

```python
# è¾“å…¥è´¦å·topken
from huggingface_hub import notebook_login
notebook_login()

# è®°å½•æˆ‘ä»¬è¦ä¼ çš„æ¨¡å‹ä»¥åŠç”¨åˆ°çš„è¶…å‚
model = {
    "env_id": env_id,
    "max_steps": max_steps,
    "n_training_episodes": n_training_episodes,
    "n_eval_episodes": n_eval_episodes,
    "eval_seed": eval_seed,

    "learning_rate": learning_rate,
    "gamma": gamma,

    "max_epsilon": max_epsilon,
    "min_epsilon": min_epsilon,
    "decay_rate": decay_rate,

    "qtable": Qtable_frozenlake
}

# push
username = "TUMxudashuai" # FILL THIS
repo_name = "q-FrozenLake-v1-4x4-noSlippery"
push_to_hub(
    repo_id=f"{username}/{repo_name}",
    model=model,
    env=env)
```

### 2.2 Taxi-v3

#### 2.2.1 å®‰è£…ç¯å¢ƒ

åŒ2.1.1

#### 2.2.2 å¯¼å…¥åŒ…

åŒ2.1.2

#### 2.2.3 å»ºç«‹ç¯å¢ƒ

- ç¯å¢ƒä¿¡æ¯ï¼šhttps://www.gymlibrary.dev/environments/toy_text/taxi/

```
env = gym.make("Taxi-v3")
```

#### 2.2.4 åˆ›å»ºå¹¶åˆå§‹åŒ–Q-table

```
state_space = env.observation_space.n
print("There are ", state_space, " possible states")
action_space = env.action_space.n
print("There are ", action_space, " possible actions")

Qtable_taxi = initialize_q_table(state_space, action_space)
print(Qtable_taxi)
print("Q-table shape: ", Qtable_taxi .shape)
```



#### 2.2.5 å®šä¹‰epsilon-greedy policy

åŒ2.1.5

#### 2.2.6 å®šä¹‰greedy policy

åŒ2.1.6

#### 2.2.7 å®šä¹‰hyperparameters

```python
# Training parameters
n_training_episodes = 25000   # Total training episodes
learning_rate = 0.7           # Learning rate

# Evaluation parameters
n_eval_episodes = 100        # Total number of test episodes

# DO NOT MODIFY EVAL_SEED
eval_seed = [16,54,165,177,191,191,120,80,149,178,48,38,6,125,174,73,50,172,100,148,146,6,25,40,68,148,49,167,9,97,164,176,61,7,54,55,
 161,131,184,51,170,12,120,113,95,126,51,98,36,135,54,82,45,95,89,59,95,124,9,113,58,85,51,134,121,169,105,21,30,11,50,65,12,43,82,145,152,97,106,55,31,85,38,
 112,102,168,123,97,21,83,158,26,80,63,5,81,32,11,28,148] # Evaluation seed, this ensures that all classmates agents are trained on the same taxi starting position
                                                          # Each seed has a specific starting state

# Environment parameters
env_id = "Taxi-v3"           # Name of the environment
max_steps = 99               # Max steps per episode
gamma = 0.95                 # Discounting rate

# Exploration parameters
max_epsilon = 1.0             # Exploration probability at start
min_epsilon = 0.05           # Minimum exploration probability 
decay_rate = 0.005            # Exponential decay rate for exploration prob
```

#### 2.2.8 å»ºç«‹è®­ç»ƒå¾ªç¯

åŒ2.1.8

#### 2.2.9 è®­ç»ƒæ¨¡å‹

```python
Qtable_taxi = train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable_taxi)
```



#### 2.2.10 å‘å¸ƒæ¨¡å‹åˆ°Huggingface

å‰ç½®æ“ä½œåŒ2.1.10

ç™»å½•åï¼š

```python
# è®°å½•è‡ªå·±çš„æ¨¡å‹(q-table)å’Œç”¨åˆ°çš„è¶…å‚
model = {
    "env_id": env_id,
    "max_steps": max_steps,
    "n_training_episodes": n_training_episodes,
    "n_eval_episodes": n_eval_episodes,
    "eval_seed": eval_seed,

    "learning_rate": learning_rate,
    "gamma": gamma,

    "max_epsilon": max_epsilon,
    "min_epsilon": min_epsilon,
    "decay_rate": decay_rate,

    "qtable": Qtable_taxi
}

# ä¸Šä¼ åˆ°è‡ªå·±çš„åº“
username = "TUMxudashuai" # FILL THIS
repo_name = "q-Taxi-v3"
push_to_hub(
    repo_id=f"{username}/{repo_name}",
    model=model,
    env=env)
```

#### 2.2.11 ä»Huggingfaceä¸Šä¸‹è½½æ¨¡å‹

```python
from urllib.error import HTTPError

from huggingface_hub import hf_hub_download

#---------------- ä¸‹è½½å‡½æ•°å®šä¹‰ ----------------
def load_from_hub(repo_id: str, filename: str) -> str:
    """
    Download a model from Hugging Face Hub.
    :param repo_id: id of the model repository from the Hugging Face Hub
    :param filename: name of the model zip file from the repository
    """
    try:
        from huggingface_hub import cached_download, hf_hub_url
    except ImportError:
        raise ImportError(
            "You need to install huggingface_hub to use `load_from_hub`. "
            "See https://pypi.org/project/huggingface-hub/ for installation."
        )

    # Get the model from the Hub, download and cache the model on your local disk
    pickle_model = hf_hub_download(
        repo_id=repo_id,
        filename=filename
    )

    with open(pickle_model, 'rb') as f:
      downloaded_model_file = pickle.load(f)
    
    return downloaded_model_file

#---------------- ä¸‹è½½taxi-v3æ¨¡å‹ï¼Œå¹¶ä½¿ç”¨----------------
model = load_from_hub(repo_id="TUMxudashuai/q-Taxi-v3", filename="q-learning.pkl")
print(model)
env = gym.make(model["env_id"])
evaluate_agent(env, model["max_steps"], model["n_eval_episodes"], model["qtable"], model["eval_seed"])

#---------------- ä¸‹è½½Frozenlakeæ¨¡å‹ï¼Œå¹¶ä½¿ç”¨ ----------------
model = load_from_hub(repo_id="TUMxudashuai/q-FrozenLake-v1-4x4-noSlippery", filename="q-learning.pkl")
env = gym.make(model["env_id"], is_slippery=False)
evaluate_agent(env, model["max_steps"], model["n_eval_episodes"], model["qtable"], model["eval_seed"])
```



## 3. Deep Q-Learning

https://colab.research.google.com/github/huggingface/deep-rl-class/blob/main/unit3/unit3.ipynb

### 3.1 Atari' Space Invaders

#### 3.1.1 setup a virtual display

```python
%%capture
pip install pyglet==1.5.1 
sudo apt install python-opengl
sudo apt install ffmpeg
sudo apt install xvfb
pip3 install pyvirtualdisplay

# Additional dependencies for RL Baselines3 Zoo
apt-get install swig cmake freeglut3-dev 

# Virtual display
from pyvirtualdisplay import Display

virtual_display = Display(visible=0, size=(1400, 900))
virtual_display.start()
```



#### 3.1.2 ä¸‹è½½[RL-Baseline3 Zoo](https://github.com/DLR-RM/rl-baselines3-zoo)

RL Baselines3 Zoo: A Training Framework for Stable Baselines3 Reinforcement Learning Agents

```python
!git clone https://github.com/DLR-RM/rl-baselines3-zoo
%cd /content/rl-baselines3-zoo/
!pip install -r requirements.txt
!pip install huggingface_sb3
```

- å¦‚æœé‡åˆ°AutoROMçš„é—®é¢˜ï¼Œå¯¼è‡´æ— æ³•ä¸‹è½½stable-baseline3[extra]

  - ç¬¬ä¸€æ­¥ï¼š

    ```
    pip install autorom
    AutoROM --install-dir /path/to/install	#å®‰è£…åœ°å€
    AutoROM --accept-license
    ```

  - ç¬¬äºŒæ­¥ï¼š

    ```
    pip install sb3-contrib
    pip install optuna
    ```

  - ç¬¬ä¸‰æ­¥ï¼š

    ä¿®æ”¹/home/xuy1fe/.conda/envs/RL-learning/lib/python3.8/site-packages/stable_baselines3/common/atari_wrappers.pyçš„ç¬¬36è¡Œ

    ```
    åŸæ¥ï¼šnoops = self.unwrapped.np_random.randint(1, self.noop_max + 1)
    æ”¹ä¸ºï¼šnoops = self.unwrapped.np_random.integers(1, self.noop_max + 1)
    ```

#### 3.1.3 è®¾ç½®hyperparameters

ä½¿ç”¨RL-Baseline3 Zooæ¡†æ¶åšRLæ—¶ï¼Œåœ¨[rl-baselines3-zoo/hyperparams/dqn.yml](https://github.com/DLR-RM/rl-baselines3-zoo/blob/master/hyperparams/dqn.yml)å¤„å®šä¹‰è¶…å‚ã€‚

```yml
atari:
  env_wrapper:
    - stable_baselines3.common.atari_wrappers.AtariWrapper
  frame_stack: 4
  policy: 'CnnPolicy'
  n_timesteps: !!float 1e6
  buffer_size: 100000
  learning_rate: !!float 1e-4
  batch_size: 32
  learning_starts: 100000
  target_update_interval: 1000
  train_freq: 4
  gradient_steps: 1
  exploration_fraction: 0.1
  exploration_final_eps: 0.01
  # If True, you need to deactivate handle_timeout_termination
  # in the replay_buffer_kwargs
  optimize_memory_usage: False
```

- å„ä¸ªå‚æ•°çš„æ„ä¹‰è§[stable-baseline3](https://stable-baselines3.readthedocs.io/en/master/modules/dqn.html#parameters) 
  - æˆ‘ä»¬å°†4ä¸ªframe stackèµ·æ¥
  - æˆ‘ä»¬ä½¿ç”¨CnnPolicyæ¥è®­ç»ƒ
  - è®­ç»ƒ1million steps

#### 3.1.4 train

ä½¿ç”¨RL-Baseline3 Zooæ¡†æ¶åšRLæ—¶ï¼Œä½¿ç”¨[rl-baselines3-zoo/train.py](https://github.com/DLR-RM/rl-baselines3-zoo/blob/master/train.py)æ¥è®­ç»ƒ

```
python train.py --algo dqn  --env SpaceInvadersNoFrameskip-v4 -f logs/
```

#### 3.1.5 Evaluate

ä½¿ç”¨RL-Baseline3 Zooæ¡†æ¶åšRLæ—¶ï¼Œä½¿ç”¨[rl-baselines3-zoo/enjoy.py](https://github.com/DLR-RM/rl-baselines3-zoo/blob/master/enjoy.py)æ¥è¯„ä¼°

```
python enjoy.py  --algo dqn  --env SpaceInvadersNoFrameskip-v4  --no-render  --n-timesteps 5000  --folder logs/
```

#### 3.1.6 ä¸Šä¼ åˆ°Huggingface

Token: https://huggingface.co/settings/tokens

1. ç™»é™†è´¦å·

```shell
# å¦‚æœç›´æ¥ç”¨ç»ˆç«¯
huggingface-cli login

# å¦‚æœç”¨jupyter
from huggingface_hub import notebook_login # To log to our Hugging Face account to be able to upload models to the Hub.
notebook_login()
!git config --global credential.helper store
```

2. ä¸Šä¼ 

```
python -m rl_zoo3.push_to_hub  --algo dqn  --env SpaceInvadersNoFrameskip-v4  --repo-name dqn-SpaceInvadersNoFrameskip-v4  -orga TUMxudashuai  -f logs/
```



#### 3.1.7 ä»Huggingfaceä¸‹è½½æ¨¡å‹

```shell
# Download model and save it into the logs/ folder
python -m rl_zoo3.load_from_hub --algo dqn --env BeamRiderNoFrameskip-v4 -orga sb3 -f rl_trained/
# è¯„ä¼°ä¸‹è½½çš„æ¨¡å‹
python enjoy.py --algo dqn --env BeamRiderNoFrameskip-v4 -n 5000  -f rl_trained/
```



## 4. Unity MLAgents

https://colab.research.google.com/github/huggingface/deep-rl-class/blob/main/unit4/unit4.ipynb

cudaç‰ˆæœ¬æœ‰é—®é¢˜ï¼Œæœªæ‰¾åˆ°å¦‚ä½•åœ¨æœ¬åœ°è·‘çš„æ–¹æ³•

### 4.1 ä¸‹è½½ml-agents

```shell
git clone https://github.com/huggingface/ml-agents/
cd ml-agents
pip3 install -e ./ml-agents-envs
pip3 install -e ./ml-agents
```

### 4.2 ä¸‹è½½è®­ç»ƒç¯å¢ƒpyramids

```shell
# å­˜æ”¾ç¯å¢ƒçš„åœ°å€
mkdir ./trained-envs-executables/linux -p
# ç”¨wgetä»google driveï¼šhttps://drive.google.com/uc?export=download&id=1UiFNdKlsH0NTu32xV-giYUEVKV4-vc7H ä¸‹è½½ç¯å¢ƒ
# ä¹Ÿå¯ä»¥ç›´æ¥ä¸‹è½½ç„¶åæ”¾åˆ°ä¸Šé¢é‚£ä¸ªæ–‡ä»¶å¤¹
!wget --load-cookies /tmp/cookies.txt "https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1UiFNdKlsH0NTu32xV-giYUEVKV4-vc7H' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\1\n/p')&id=1UiFNdKlsH0NTu32xV-giYUEVKV4-vc7H" -O ./trained-envs-executables/linux/Pyramids.zip && rm -rf /tmp/cookies.txt
# è§£å‹ä¸‹è½½çš„æ–‡ä»¶
unzip -d ./trained-envs-executables/linux/ ./trained-envs-executables/linux/Pyramids.zip
# ç»™æ–‡ä»¶æƒé™
chmod -R 755 ./trained-envs-executables/linux/Pyramids/Pyramids
```

### 4.3 ä¿®æ”¹hyerparameters

ML-Agentsè®­ç»ƒç”¨çš„è¶…å‚ï¼Œä¿å­˜åœ¨ml-agents/config/çš„å¯¹åº”yamlæ–‡ä»¶ä¸­ã€‚ML-Agentå„ä¸ªè¶…å‚çš„å®šä¹‰è§[å®˜ç½‘](https://github.com/Unity-Technologies/ml-agents/blob/main/docs/Training-Configuration-File.md)

è¿™é‡Œæˆ‘ä»¬ä¿®æ”¹ml-agents/config/ppo/PyramidsRND.yamlæ–‡ä»¶

```
max_steps: 3000000
æ”¹æˆ
max_steps: 500000
```

### 4.4 è®­ç»ƒagent

è®¾ç½®4ä¸ªå‚æ•°ï¼š

1. `mlagents-learn <config>`: the path where the hyperparameter config file is.
2. `--env`: where the environment executable is.
3. `--run_id`: the name you want to give to your training run id.
4. `--no-graphics`: to not launch the visualization during the training.

```shell
mlagents-learn ./config/ppo/PyramidsRND.yaml --env=./trained-envs-executables/linux/Pyramids/Pyramids --run-id="Pyramids Training" --no-graphics
```



### 4.5 ä¸Šä¼ åˆ°hugging face

- ç™»é™†

    ```shell
    # if no jupyter:huggingface-cli login
    from huggingface_hub import notebook_login
    notebook_login()
    ```
    
- ä¸Šä¼ 

  ä½¿ç”¨`mlagents-push-to-hf`ä¸Šä¼ ï¼Œè®¾ç½®å¦‚ä¸‹å‚æ•°ï¼š
  
  - `--run-id`: the name of the training run id.
  - `--local-dir`: where the agent was saved, itâ€™s results/, so in my case results/First Training.
  - `--repo-id`: the name of the Hugging Face repo you want to create or update. Itâ€™s always / If the repo does not exist **it will be created automatically**
  - `--commit-message`: since HF repos are git repository you need to define a commit message.
  
  ```shell
  !mlagents-push-to-hf --run-id="Pyramids Training" --local-dir="./results/Pyramids Training" --repo-id="TUMxudashuai/testpyramidsrnd" --commit-message="First Pyramids"
  ```
  
  
### 4.6 Visual agent online

https://huggingface.co/spaces/unity/ML-Agents-Pyramids

åœ¨ä¸Šè¿°ç½‘å€ï¼Œé€‰æ‹©4.5ä¸­ä¸Šä¼ çš„æ¨¡å‹



## 5. Policy Gradient with PyTorch

https://colab.research.google.com/github/huggingface/deep-rl-class/blob/main/unit5/unit5.ipynb



# å‰¯1ã€ Optuna: hyperparameter optimization

[Optuna](https://github.com/optuna/optuna) is an automatic hyperparameter optimization software framework, particularly designed for machine learning.

## 0. è‡ªåŠ¨è¶…å‚æœç´¢ èµ„æ–™æ±‡æ€»

1. [æ·±åº¦å­¦ä¹ æ¨¡å‹è¶…å‚æ•°æœç´¢å®ç”¨æŒ‡å—](https://zhuanlan.zhihu.com/p/45353509)

   ä¸»è¦æœ‰4ç§æœç´¢ç­–ç•¥

   1. **Babysitting (æˆ–Grad student Descent)**äººå·¥æœç´¢
   2. **ç½‘æ ¼æœç´¢ Grid Search**
   3. **éšæœºæœç´¢ Random Search**
   4. **è´å¶æ–¯ä¼˜åŒ– Bayesian Optimization**

## 1. ä¸‹è½½ä¾èµ–

```shell
!pip install stable-baselines3
# Optional: install SB3 contrib to have access to additional algorithms
!pip install sb3-contrib
# Optuna will be used in the last part when doing hyperparameter tuning
!pip install optuna
```

## 2. å¯¼å…¥åº“å’Œç®—æ³•

```python
import gym
import numpy as np
from stable_baselines3 import PPO, A2C, SAC, TD3, DQN
# Algorithms from the contrib repo
# https://github.com/Stable-Baselines-Team/stable-baselines3-contrib
from sb3_contrib import QRDQN, TQC
from stable_baselines3.common.env_util import make_vec_env	# Parallel environments
from stable_baselines3.common.evaluation import evaluate_policy
```

## 3. The Importance of tuned Hyperparameters

- When compared with Supervised Learning, Deep Reinforcement Learning is far more sensitive to the choice of hyper-parameters such as learning rate, number of neurons, number of layers, optimizer ... etc.

- é™¤äº†è¶…å‚ï¼Œç®—æ³•çš„é€‰æ‹©ä¹Ÿæ˜¯å¾ˆé‡è¦çš„ã€‚

- ä¸‹é¢æ˜¯ç”¨ä¸åŒç®—æ³•çš„é€‰æ‹© å’Œ è°ƒæ•´å‚æ•° å¯¹å­¦ä¹ [Pendulum](https://www.gymlibrary.dev/environments/classic_control/pendulum/)çš„å½±å“æ¯”è¾ƒï¼š

  é¦–å…ˆå¯¼å…¥ç¯å¢ƒ

  ```python
  env_id = "Pendulum-v1"
  # Env used only for evaluation
  eval_envs = make_vec_env(env_id, n_envs=10)
  # 4000 training timesteps
  budget_pendulum = 4000
  ```

### 3.1 PPO

1. è®­ç»ƒ4000æ­¥(20 episodes)

    ```python
    ppo_model = PPO("MlpPolicy", env_id, seed=0, verbose=0).learn(budget_pendulum)
    mean_reward, std_reward = evaluate_policy(ppo_model, eval_envs, n_eval_episodes=100, deterministic=True)
    
    print(f"PPO Mean episode reward: {mean_reward:.2f} +/- {std_reward:.2f}")
    ```

    å¾—åˆ°ï¼šPPO Mean episode reward: -1146.01 +/- 253.07

    å¯è§æ•ˆæœä¸å¥½ï¼Œå°è¯•æ­¥éª¤2ï¼š

2. è®­ç»ƒçš„æ›´é•¿40000æ­¥

    ```python
    new_budget = 10 * budget_pendulum
    ppo_model = PPO("MlpPolicy", env_id, seed=0, verbose=0).learn(new_budget)
    mean_reward, std_reward = evaluate_policy(ppo_model, eval_envs, n_eval_episodes=100, deterministic=True)
    
    print(f"PPO Mean episode reward: {mean_reward:.2f} +/- {std_reward:.2f}")
    ```

    å¾—åˆ°ï¼šPPO Mean episode reward: -1164.48 +/- 205.87

    å¯è§æ•ˆæœä¸€èˆ¬ï¼Œå°è¯•æ­¥éª¤3ï¼š

3.  tune Hyperparameters

    ```python
    tuned_params = {
        "gamma": 0.9,
        "use_sde": True,
        "sde_sample_freq": 4,
        "learning_rate": 1e-3,
    }
    
    # budget = 10 * budget_pendulum
    ppo_tuned_model = PPO("MlpPolicy", env_id, seed=1, verbose=1, **tuned_params).learn(50_000, log_interval=5)
    
    mean_reward, std_reward = evaluate_policy(ppo_tuned_model, eval_envs, n_eval_episodes=100, deterministic=True)
    
    print(f"Tuned PPO Mean episode reward: {mean_reward:.2f} +/- {std_reward:.2f}")
    ```

    å¾—åˆ°ï¼šTuned PPO Mean episode reward: -173.78 +/- 99.40

    å¯è§æ•ˆæœå¥½äº†å¾ˆå¤š

### 3.2 [A2C](https://stable-baselines3.readthedocs.io/en/master/modules/a2c.html)

```python
a2c_model = A2C("MlpPolicy",env_id,verbose=1)
mean_reward, std_reward = evaluate_policy(a2c_model, eval_envs, n_eval_episodes=100, deterministic=True)

print(f"A2C Mean episode reward: {mean_reward:.2f} +/- {std_reward:.2f}")
```

å¾—åˆ°ï¼šA2C Mean episode reward: -1210.47 +/- 331.78



## 4. Grad Student Descent

è¿™ç§æ–¹æ³•æ˜¯**100ï¼…æ‰‹åŠ¨**ï¼Œæ˜¯ç ”ç©¶äººå‘˜ï¼Œå­¦ç”Ÿå’Œä¸šä½™çˆ±å¥½è€…æœ€å¹¿æ³›é‡‡ç”¨çš„æ–¹æ³•ã€‚

- ç”¨A2Cç®—æ³•å­¦ä¹ CartPole-v1ä»»åŠ¡ï¼š

  ```python
  budget = 20_000
  eval_envs_cartpole = make_vec_env("CartPole-v1", n_envs=10)
  model = A2C("MlpPolicy", "CartPole-v1", seed=8, verbose=1).learn(budget)
  mean_reward, std_reward = evaluate_policy(model, eval_envs_cartpole, n_eval_episodes=50, deterministic=True)
  
  print(f"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}")
  ```

  å¾—åˆ°mean_reward:140.40 +/- 54.13

  ç„¶åæˆ‘ä»¬å°è¯•æ‰‹åŠ¨è°ƒå‚ï¼Œæ‰“è´¥ä¸Šé¢è¿™ä¸ªåˆ†æ•°ï¼Œæœ€ä¼˜è§£å¾—åˆ†æ˜¯500ï¼š

- Grad Student Descent

  ```python
  import torch.nn as nn
  policy_kwargs = dict(
      net_arch=[
        dict(vf=[64, 64], pi=[64, 64]), # network architectures for actor/critic
      ],
      activation_fn=nn.Tanh,
  )
  
  hyperparams = dict(
      n_steps=5, # number of steps to collect data before updating policy
      learning_rate=7e-4,
      gamma=0.99, # discount factor
      max_grad_norm=0.5, # The maximum value for the gradient clipping
      ent_coef=0.0, # Entropy coefficient for the loss calculation
  )
  
  model = A2C("MlpPolicy", "CartPole-v1", seed=8, verbose=1, **hyperparams).learn(budget)
  
  mean_reward, std_reward = evaluate_policy(model, eval_envs_cartpole, n_eval_episodes=50, deterministic=True)
  
  print(f"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}")
  ```

  å¾—åˆ°mean_reward:132.30 +/- 34.88

  æ˜¾ç„¶æ‰‹åŠ¨è°ƒå¾ˆéº»çƒ¦

## 5. Automatic Hyperparameter Tuning

 Create a script that allows to search for the best hyperparameters automatically.

### 5.1 å¯¼å…¥optunaåº“

```python
import optuna
from optuna.pruners import MedianPruner
from optuna.samplers import TPESampler
from optuna.visualization import plot_optimization_history, plot_param_importances
```

### 5.2 Config

è®¾ç½®optunaçš„å‚æ•°

```python
N_TRIALS = 100  # Maximum number of trials
N_JOBS = 1 # Number of jobs to run in parallel
N_STARTUP_TRIALS = 5  # Stop random sampling after N_STARTUP_TRIALS
N_EVALUATIONS = 2  # Number of evaluations during the training
N_TIMESTEPS = int(2e4)  # Training budget
EVAL_FREQ = int(N_TIMESTEPS / N_EVALUATIONS)
N_EVAL_ENVS = 5
N_EVAL_EPISODES = 10
TIMEOUT = int(60 * 15)  # 15 minutes

ENV_ID = "CartPole-v1"

DEFAULT_HYPERPARAMS = {
    "policy": "MlpPolicy",
    "env": ENV_ID,
}
```

### 5.3 Define search space

å®šä¹‰optunaæƒ³è¦æ‰¾é‚£äº›è¶…å‚ï¼Œä»¥åŠä»–ä»¬çš„èŒƒå›´

```python
from typing import Any, Dict
import torch
import torch.nn as nn
# å˜é‡ååé¢çš„å†’å·æ˜¯ï¼šç±»å‹æ³¨è§£ï¼Œ3.6ä»¥ååŠ å…¥çš„ï¼Œå†’å·å³è¾¹æ˜¯ç±»å‹ï¼Œä»…ä»…æ˜¯æ³¨é‡Šï¼Œæœ‰äº›é¸¡è‚‹
def sample_a2c_params(trial: optuna.Trial) -> Dict[str, Any]:
    """
    Sampler for A2C hyperparameters.

    :param trial: Optuna trial object
    :return: The sampled hyperparameters for the given trial.
    """
    # Discount factor between 0.9 and 0.9999
    gamma = 1.0 - trial.suggest_float("gamma", 0.0001, 0.1, log=True)
    max_grad_norm = trial.suggest_float("max_grad_norm", 0.3, 5.0, log=True)
    # 8, 16, 32, ... 1024
    n_steps = 2 ** trial.suggest_int("exponent_n_steps", 3, 10)

    ### YOUR CODE HERE
    # TODO:
    # - define the learning rate search space [1e-5, 1] (log) -> `suggest_float`
    # - define the network architecture search space ["tiny", "small"] -> `suggest_categorical`
    # - define the activation function search space ["tanh", "relu"]
    learning_rate =trial.suggest_float("lr",1e-5, 1, log=True)
    net_arch = trial.suggest_categorical("net_arch",["tiny","small"])
    activation_fn = trial.suggest_categorical("activation_fn",["tanh","relu"])

    ### END OF YOUR CODE

    # Display true values
    trial.set_user_attr("gamma_", gamma)
    trial.set_user_attr("n_steps", n_steps)

    net_arch = [
        {"pi": [64], "vf": [64]}
        if net_arch == "tiny"
        else {"pi": [64, 64], "vf": [64, 64]}
    ]

    activation_fn = {"tanh": nn.Tanh, "relu": nn.ReLU}[activation_fn]

    return {
        "n_steps": n_steps,
        "gamma": gamma,
        "learning_rate": learning_rate,
        "max_grad_norm": max_grad_norm,
        "policy_kwargs": {
            "net_arch": net_arch,
            "activation_fn": activation_fn,
        },
    }
```

### 5.4 Define the Callback function

define a custom callback to report the results of periodic evaluations to Optuna:

```python
from stable_baselines3.common.callbacks import EvalCallback

class TrialEvalCallback(EvalCallback):
    """
    Callback used for evaluating and reporting a trial.
    
    :param eval_env: Evaluation environement
    :param trial: Optuna trial object
    :param n_eval_episodes: Number of evaluation episodes
    :param eval_freq:   Evaluate the agent every ``eval_freq`` call of the callback.
    :param deterministic: Whether the evaluation should
        use a stochastic or deterministic policy.
    :param verbose:
    """

    def __init__(
        self,
        eval_env: gym.Env,
        trial: optuna.Trial,
        n_eval_episodes: int = 5,
        eval_freq: int = 10000,
        deterministic: bool = True,
        verbose: int = 0,
    ):
		# super() å‡½æ•°æ˜¯ç”¨äºè°ƒç”¨çˆ¶ç±»(è¶…ç±»)çš„ä¸€ä¸ªæ–¹æ³•ã€‚
		# super() æ˜¯ç”¨æ¥è§£å†³å¤šé‡ç»§æ‰¿é—®é¢˜çš„ï¼Œç›´æ¥ç”¨ç±»åè°ƒç”¨çˆ¶ç±»æ–¹æ³•åœ¨ä½¿ç”¨å•ç»§æ‰¿çš„æ—¶å€™æ²¡é—®é¢˜ï¼Œä½†æ˜¯å¦‚æœä½¿ç”¨å¤šç»§æ‰¿ï¼Œä¼šæ¶‰åŠåˆ°æŸ¥æ‰¾é¡ºåºï¼ˆMROï¼‰ã€é‡å¤è°ƒç”¨ï¼ˆé’»çŸ³ç»§æ‰¿ï¼‰ç­‰ç§ç§é—®é¢˜ã€‚
        super().__init__(
            eval_env=eval_env,
            n_eval_episodes=n_eval_episodes,
            eval_freq=eval_freq,
            deterministic=deterministic,
            verbose=verbose,
        )
        self.trial = trial
        self.eval_idx = 0
        self.is_pruned = False

    def _on_step(self) -> bool:
        if self.eval_freq > 0 and self.n_calls % self.eval_freq == 0:
            # Evaluate policy (done in the parent class)
            # è¯„ä»·ä½“ç³»åœ¨stable_baselines3.common.callbacksçš„EvalCallbackä¸­å·²å®šä¹‰ï¼Œè¦æ‰¾mean rewardæœ€é«˜çš„
            # ä¹Ÿæ˜¯5.6ä¸­objective valueçš„å€¼
            super()._on_step()
            self.eval_idx += 1
            # Send report to Optuna
            self.trial.report(self.last_mean_reward, self.eval_idx)
            # æ ¹æ®trialçš„pruning algorithmæ¥åˆ¤æ–­æ˜¯å¦éœ€è¦åœæ­¢å½“å‰å‚æ•°çš„è®­ç»ƒ
            if self.trial.should_prune():
                self.is_pruned = True
                return False
        return True
```

### 5.5 Define the objective function

define the objective function that is in charge of sampling hyperparameters, creating the model and then returning the result to Optuna

```python
def objective(trial: optuna.Trial) -> float:
    """
    Objective function using by Optuna to evaluate
    one configuration (i.e., one set of hyperparameters).

    Given a trial object, it will sample hyperparameters,
    evaluate it and report the result (mean episodic reward after training)

    :param trial: Optuna trial object
    :return: Mean episodic reward after training
    """

    kwargs = DEFAULT_HYPERPARAMS.copy()
    ### YOUR CODE HERE
    # TODO: 
    # 1. Sample hyperparameters and update the keyword arguments
    # ç”¨5.3çš„å–æ ·å‡½æ•°ï¼Œåœ¨æœç´¢èŒƒå›´å†…æ’åˆ—ç»„åˆå‚æ•°ï¼Œå¹¶è¿”å›
    # ä½¿ç”¨å­—å…¸çš„updateå‡½æ•°ï¼Œå°†è¿”å›å€¼æ›´æ–°åˆ°kwargsä¸­å»
    kwargs.update(sample_a2c_params(trial))
    # åˆ›å»ºRL model
    model = A2C(**kwargs)

    # 2. Create envs used for evaluation using `make_vec_env`, `ENV_ID` and `N_EVAL_ENVS`
    eval_envs = make_vec_env(ENV_ID,N_EVAL_ENVS)

    # 3. Create the `TrialEvalCallback` callback defined above that will periodically evaluate
    # and report the performance using `N_EVAL_EPISODES` every `EVAL_FREQ`
    # TrialEvalCallback signature:
    # TrialEvalCallback(eval_env, trial, n_eval_episodes, eval_freq, deterministic, verbose)
    # ä½¿ç”¨5.4çš„å›è°ƒå‡½æ•°ï¼Œè¿”å›æ¯ä¸ªå‚æ•°ç»„åˆçš„æ•ˆæœ
    eval_callback = TrialEvalCallback(eval_envs,
                                      trial, 
                                      N_EVAL_EPISODES, 
                                      EVAL_FREQ,
                                      deterministic=True)

    ### END OF YOUR CODE

    nan_encountered = False
    try:
        # Train the model
        model.learn(N_TIMESTEPS, callback=eval_callback)
    except AssertionError as e:
        # Sometimes, random hyperparams can generate NaN
        print(e)
        nan_encountered = True
    finally:
        # Free memory
        model.env.close()
        eval_envs.close()

    # Tell the optimizer that the trial failed
    if nan_encountered:
        return float("nan")

    if eval_callback.is_pruned:
        raise optuna.exceptions.TrialPruned()

    return eval_callback.last_mean_reward
```

### 5.6 optimization loop

è°ƒç”¨5.3ï¼Œ5.4ï¼Œ5.5çš„ä»£ç ï¼Œä¸æ–­å¾ªç¯æ‰¾å‡ºæœ€ä¼˜çš„å‚æ•°è®¾ç½®

```python
import torch as th

# Set pytorch num threads to 1 for faster training
th.set_num_threads(1)
# Select the sampler, can be random, TPESampler, CMAES, ...
sampler = TPESampler(n_startup_trials=N_STARTUP_TRIALS)
# Do not prune before 1/3 of the max budget is used
# åœ¨1/3 budgeté¢„ç®—åï¼Œä¼šprune the least promising trials
# æ¯ä¸ªtrialsä»£è¡¨ä¸€ç»„å‚æ•°è®¾ç½®ç»„åˆ
pruner = MedianPruner(
    n_startup_trials=N_STARTUP_TRIALS, n_warmup_steps=N_EVALUATIONS // 3
)
# Create the study and start the hyperparameter optimization
study = optuna.create_study(sampler=sampler, pruner=pruner, direction="maximize")

try:
    study.optimize(objective, n_trials=N_TRIALS, n_jobs=N_JOBS, timeout=TIMEOUT)
except KeyboardInterrupt:
    pass

print("Number of finished trials: ", len(study.trials))

print("Best trial:")
trial = study.best_trial

print(f"  Value: {trial.value}")

print("  Params: ")
for key, value in trial.params.items():
    print(f"    {key}: {value}")

print("  User attrs:")
for key, value in trial.user_attrs.items():
    print(f"    {key}: {value}")

# Write report
study.trials_dataframe().to_csv("study_results_a2c_cartpole.csv")

fig1 = plot_optimization_history(study)
fig2 = plot_param_importances(study)

fig1.show()
fig2.show()
```

æœ€åå¾—åˆ°

```python
Number of finished trials:  60
Best trial:
  Value: 500.0
  Params: 
    gamma: 0.007193710709881038
    max_grad_norm: 1.971438140124285
    exponent_n_steps: 8
    lr: 0.007340303426309211
    net_arch: tiny
    activation_fn: relu
  User attrs:
    gamma_: 0.9928062892901189
    n_steps: 256
```



# å‰¯2ã€ Curiosity-Driven Learning

[Curiosity-Driven Learning through Next State Prediction](https://medium.com/data-from-the-trenches/curiosity-driven-learning-through-next-state-prediction-f7f4e2f592fa)

[Random Network Distillation: a new take on Curiosity-Driven Learning](https://medium.com/data-from-the-trenches/curiosity-driven-learning-through-random-network-distillation-488ffd8e5938)

## 1. Two Major Problems in Modern RL

### 1.1 [sparse rewards](https://zhuanlan.zhihu.com/p/558034131)ç¨€ç–å¥–åŠ±  problem

å¯¹åº”äºdense rewardsç¨ å¯†å¥–åŠ±

- most rewards do not contain information, and hence are set to zero.å®Œæˆç›®æ ‡äº‹ä»¶çš„æ¬¡æ•°å¤ªå°‘æˆ–è€…å®Œæˆç›®æ ‡çš„æ­¥æ•°å¤ªé•¿ï¼Œå¯¼è‡´å¥–åŠ±ç©ºé—´ä¸­çš„è´Ÿå¥–åŠ±æ ·æœ¬è¿œè¿œå¤šäºæ­£å¥–åŠ±æ ·æœ¬æ•°ã€‚

- é—®é¢˜çš„åŸå› ï¼š

  RLæ˜¯åŸºäºreward hypothesisçš„ç®—æ³•ï¼Œä¹Ÿå°±æ˜¯è¯´RLçš„ç›®æ ‡æ˜¯æœ€å¤§åŒ–cumulative rewardsã€‚å› æ­¤å¦‚æœRewardsä¸€ç›´éƒ½æ˜¯0ï¼Œagentä¹Ÿå°±ä¸çŸ¥é“ä»–ä»¬çš„actionæ˜¯å¦appropriateæ°å½“ï¼Œæœ€åagentè¿›æ­¥ç¼“æ…¢ã€‚

- ä¾‹å­ï¼š

  1. åœ¨å›´æ£‹ä¸­ï¼Œä»å¼€å§‹ä¸‹æ£‹åˆ°æ£‹å±€ç»“æŸæ‰èƒ½åˆ¤æ–­èƒœè´Ÿï¼Œæ­¤æ—¶æ™ºèƒ½ä½“æ‰èƒ½è·å¾—å¥–åŠ±ï¼Œæ£‹å±€ä¸­é—´è¿‡ç¨‹ä¸­çš„å¥–åŠ±å¾ˆéš¾è¯„ä»·ï¼›
  2. åœ¨å¯¼èˆªä»»åŠ¡ä¸­ï¼Œæ™ºèƒ½ä½“åªæœ‰åœ¨è§„å®šçš„æ—¶é—´æ­¥å†…åˆ°è¾¾æŒ‡å®šä½ç½®æ‰èƒ½å¾—åˆ°å¥–åŠ±ï¼Œä¸­é—´è¿‡ç¨‹çš„æ¯ä¸€æ­¥éƒ½æ˜¯æ— å¥–åŠ±çš„ï¼›
  3. åœ¨æœºæ¢°è‡‚æŠ“å–ä»»åŠ¡ä¸­ï¼Œæœºæ¢°è‡‚é€šè¿‡å®Œæˆä¸€ç³»åˆ—å¤æ‚çš„ä½å§¿æ§åˆ¶æˆåŠŸæŠ“å–ç›®æ ‡åæ‰èƒ½è·å¾—å¥–åŠ±ï¼Œä¸­é—´ä»»ä½•ä¸€æ­¥çš„å¤±è´¥éƒ½å¯¼è‡´æ— æ³•è·å¾—å¥–åŠ±ã€‚

  åœ¨è¿™äº›ä»»åŠ¡è¿‡ç¨‹ä¸­ï¼Œå¦‚æœagentæ²¡æœ‰å¾—åˆ°æœ‰æ•ˆçš„åé¦ˆ(dense rewards), å®ƒä¼šèŠ±è´¹å¤§é‡æ—¶é—´å­¦ä¹ æœ€ä¼˜ç­–ç•¥å¹¶ä¸”åˆ°å¤„ä¹±è¯•å´æ‰¾ä¸åˆ°ç›®æ ‡ã€‚

### 1.2  extrinsic reward function is handmadeæ‰‹å·¥çš„

- åœ¨æ¯ä¸€ä¸ªç¯å¢ƒä¸­ï¼Œäººä»¬å¿…é¡»æ‰‹åŠ¨Implementå®æ–½ reward functionã€‚ä½†æˆ‘ä»¬å¦‚ä½•scaleåº¦é‡ä¸€ä¸ªå¤§è€Œå¤æ‚çš„ç¯å¢ƒå‘¢ï¼Ÿ

## 2. What is Curiosity

- ä¸ºäº†è§£å†³ä¸Šé¢è¿™ä¸¤ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬éœ€è¦å¼€å‘ä¸€ä¸ªreward functionï¼Œå®ƒå¯¹äºagentæ˜¯intrinsicå†…åœ¨çš„(generated by the agent itself). The agent will act as a self-learner since it will be the student, but also its own feedback master.

  è¿™ä¸ªintrinsic reward mechanismæœºåˆ¶ï¼Œä¹Ÿè¢«ç§°ä¸º **curiosity**, å› ä¸ºå®ƒçš„rewardä¼šä¸æ–­è®©agentå»explore novel/unfamiliar states.

   In order to achieve that, our agent will receive a high reward when exploring new trajectories.

## 3. ä¸¤ç§è®¡ç®—curiosityçš„æ–¹æ³•

### 3.1 Curiosity-Driven Learning through Next State Prediction

[Curiosity-Driven Learning through Next State Prediction](https://medium.com/data-from-the-trenches/curiosity-driven-learning-through-next-state-prediction-f7f4e2f592fa)æ˜¯ä¸€ä¸ªç»å…¸æ–¹æ³•ã€‚
$$
IR=||predicted(s_{t+1})-s_{t+1} ||
$$

- Intrinsic reward(IR): prediction error in predicting $s_{t+1}$ given $s_t$ and $a_t$
- æˆ‘ä»¬ä¼šå¾—åˆ°
  - small IR in familiar states: å› ä¸º easy to predict next state
  - big IR in unfamiliar states: å› ä¸º hard to predict next state in unknown trajectories
- Using curiosity will push our agent to favor transitions with high prediction error (which will be higher in areas where the agent has spent less time, or in areas with complex dynamics) **and consequently better explore our environment.**

### 3.2 Random Network Distillation

ML-Agentsä½¿ç”¨äº†ä¸€ä¸ªæ›´å…ˆè¿›çš„æ–¹æ³•ï¼š[Random Network Distillation: a new take on Curiosity-Driven Learning](https://medium.com/data-from-the-trenches/curiosity-driven-learning-through-random-network-distillation-488ffd8e5938)



